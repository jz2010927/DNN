{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np, matplotlib.pyplot as plt, multiprocessing as mp\n",
    "from numpy import random\n",
    "import torch, cv2, time, random, os, threading, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = {\n",
    "    'boolean': torch.bool, 'ui8': torch.uint8, 'i8': torch.int8, 'i16': torch.int16, 'i32': torch.int32, 'i64': torch.int64, \n",
    "    'f16': torch.float16, 'f32': torch.float32, 'f64': torch.float64, 'f64Complex': torch.complex64, 'f128Complex': torch.complex128\n",
    "}\n",
    "DEVICE = {\n",
    "    'auto': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), \n",
    "    'cpu': torch.device('cpu'), \n",
    "    'cuda0': torch.device('cuda:0')\n",
    "}\n",
    "\n",
    "DEVICE_CHOICE = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, layers=(10, 20, 10), aFunc=('cos', 'PRelu', 'PRelu', 'sin')):\n",
    "        self.layerNum = len(layers)\n",
    "        assert self.layerNum >= 1\n",
    "        torch.manual_seed(0)\n",
    "        self.backProp = {\n",
    "            'PRelu': True,  # 如果False, 则 == Leaky Relu\n",
    "            'Norm': True\n",
    "        }\n",
    "        self.paramLimits = {\n",
    "            'weights': (-10000000, 10000000), \n",
    "            'biases': (-10000000, 10000000), \n",
    "            'relu param': (-100, 100), \n",
    "            'BN gamma': (-1000, 1000), \n",
    "            'BN beta': (-1000, 1000), \n",
    "            'dW': (-1, 1), \n",
    "            'dB': (-10, 10), \n",
    "            'dReluP': (-10, 10), \n",
    "            'dGamma': (-10, 10), \n",
    "            'dBeta': (-10, 10)\n",
    "        }\n",
    "        self.layerShapes = layers\n",
    "        self.aFuncChosen = aFunc\n",
    "        self.defaultLr = 0.00001\n",
    "        self.lr = {\n",
    "            'weight': self.defaultLr * 250,\n",
    "            'bias': self.defaultLr * 5000,\n",
    "            'relu param': self.defaultLr * 10, \n",
    "            'BN gamma': self.defaultLr, \n",
    "            'BN beta': self.defaultLr\n",
    "        }\n",
    "        self.inputs = None\n",
    "        self.targetY = None\n",
    "        self.weights = [None] * self.layerNum\n",
    "        self.biases = [None] * self.layerNum\n",
    "        self.reluParam = [0.01] * self.layerNum\n",
    "        self.layers = {\n",
    "            'Z': [None] * self.layerNum, \n",
    "            'N': [None] * self.layerNum, \n",
    "            'A': [None] * self.layerNum\n",
    "        }\n",
    "        self.BN = {\n",
    "            'epsilon': 1e-5, \n",
    "            'gamma': [1] * self.layerNum, \n",
    "            'beta': [0] * self.layerNum, \n",
    "            'cache': [None] * self.layerNum\n",
    "        }\n",
    "        self.activFunc = {\n",
    "            'PRelu': lambda x, i: torch.max(x, x * self.reluParam[i]), \n",
    "            'sigmoid': lambda x: 1/(1+torch.exp(-x)), \n",
    "            'softmax': lambda x: torch.exp(x - torch.max(x)) / torch.sum(torch.exp(x - torch.max(x))), \n",
    "            'tanh': lambda x: torch.tanh(x), \n",
    "            'sin': lambda x: torch.sin(x), \n",
    "            'cos': lambda x: torch.cos(x), \n",
    "            'linear': lambda x: x\n",
    "        }\n",
    "        self.activFuncDer = {\n",
    "            'PRelu': self._PReluDer,  \n",
    "            'sigmoid': lambda x: self.activFunc['sigmoid'](x) * (1 - self.activFunc['sigmoid'](x)), \n",
    "            'softmax': lambda x, a: self.activFunc['softmax'](x) * (a - self.activFunc['softmax'](x)), \n",
    "            'tanh': lambda x: 1 - torch.tanh(x) ** 2, \n",
    "            'sin': lambda x: torch.cos(x), \n",
    "            'cos': lambda x: -torch.sin(x), \n",
    "            'linear': lambda x: 1\n",
    "        }\n",
    "        self.lossFunc = {\n",
    "            'mse': lambda predictY, targetY: (targetY - predictY) ** 2, \n",
    "            'bce': lambda predictY, targetY: targetY * torch.log(predictY) + (1 - targetY) * torch.log(1 - predictY)\n",
    "        }\n",
    "        self.lossFuncDer = {\n",
    "            'mse': lambda predictY, targetY: 2 * (targetY - predictY), \n",
    "            'bce': lambda predictY, targetY: Y / predictY + (targetY - 1) / (1 - predictY)\n",
    "        }\n",
    "    \n",
    "    def _dA_dReluP(self, x, i):\n",
    "        data = x.clone()\n",
    "        data[data > 0] = 0\n",
    "        data[data <= 0] = torch.mean(data[data <= 0])\n",
    "        return data\n",
    "    \n",
    "    def _PReluDer(self, x, i): \n",
    "        data = x.clone()\n",
    "        data[data > 0] = 1\n",
    "        data[data <= 0] = self.reluParam[i]\n",
    "        return data\n",
    "    \n",
    "    def setBackProp(self, **kwargs):\n",
    "        '''\n",
    "        Keys: PRelu, Norm\n",
    "        Values: True, False\n",
    "        '''\n",
    "        for k, v in kwargs.items():\n",
    "            try:\n",
    "                self.backProp[k] = v\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    def batchNorm(self, x, layerIter):\n",
    "        mean = torch.mean(x)\n",
    "        variance = torch.mean((x - mean) ** 2)\n",
    "        # normalize\n",
    "        fenzi = (x - mean) * 1.0\n",
    "        fenmu = torch.sqrt(variance + self.BN['epsilon'])\n",
    "        # * 1.0 是转换成float\n",
    "        xNorm = fenzi / fenmu\n",
    "        cache = {\n",
    "            'BNmean': mean, \n",
    "            'BNvariance': variance, \n",
    "            'BNfenzi': fenzi, \n",
    "            'BNfenmu': fenmu\n",
    "        }\n",
    "        self.BN['cache'][layerIter] = cache\n",
    "        return self.BN['gamma'][layerIter] * xNorm + self.BN['beta'][layerIter]\n",
    "    \n",
    "    def setToLimit(self, data, limits):\n",
    "        x = data.clone()\n",
    "        x[x<limits[0]] = limits[0]\n",
    "        x[x>limits[1]] = limits[1]\n",
    "        return x\n",
    "    \n",
    "    # 生成 w、b, 以x的形状是 m x 1\n",
    "    def genParam(self, x):\n",
    "        self.inputs = x\n",
    "        column = x.shape[0]\n",
    "        for i, r in enumerate(self.layerShapes):\n",
    "#             print('row: ', row, '\\ncolumn: ', column)\n",
    "            self.weights[i] = torch.ones(r, column, dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "            self.biases[i] = torch.zeros(r, 1, dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "            column = r\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        layerNameFirst = 'N' if self.backProp['Norm'] else 'Z'\n",
    "#         print('inputs.shape: ', inputs.shape)\n",
    "        for i in range(self.layerNum):\n",
    "#             print(f'w[{i}].shape: ', self.weights[i].shape)\n",
    "#             print(f'b[{i}].shape: ', self.biases[i].shape)\n",
    "            self.layers['Z'][i] = self.weights[i] @ inputs + self.biases[i]\n",
    "            if self.backProp['Norm']:\n",
    "                self.layers['N'][i] = self.batchNorm(self.layers['Z'][i], i)\n",
    "#                 print('BN forwarded')\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                self.layers['A'][i] = self.activFunc[self.aFuncChosen[i]](self.layers[layerNameFirst][i], i)\n",
    "            else:\n",
    "                self.layers['A'][i] = self.activFunc[self.aFuncChosen[i]](self.layers[layerNameFirst][i])\n",
    "            inputs = self.layers['A'][i]\n",
    "#             print(f'layer[{i}].shape: ', self.layers['A'][i].shape)\n",
    "\n",
    "    # 预测\n",
    "    def predict(self, x):\n",
    "        self.forward(x)\n",
    "        predictY = self.layers['A'][-1]\n",
    "        print('output: ', predictY.squeeze())\n",
    "        return predictY\n",
    "    \n",
    "    # 反向传播函数\n",
    "    # input = x, Z = W @ input + b, N = batchNormalize(Z), Y_preditc = activateFunc(N), L = lossFunc(Y_predict)\n",
    "    # 根据链式法则 dL / dW = (dL / dY_predict) * (dY_predict / dN) * (dN / dZ) * (dZ / dW)\n",
    "    # dL / dY_predict = lossFunc_Der, dY_predict / dN = activateFunc_Der, dN/dZ = gamma/sqrt(variance+epsilon), dZ/dW = input\n",
    "    # ==> dL / dW = lossFunc_Der(Y_predict) * activateFunc_Der(Z) * gamma/sqrt(variance+epsilon) * input\n",
    "    # 同理可证 dL / db = lossFunc_Der(Y_predict) * activateFunc_Der(Z) * gamma/sqrt(variance+epsilon) * 1\n",
    "    def backprop(self):\n",
    "        '''\n",
    "        尚未完成\n",
    "        '''\n",
    "        layerNameFirst = 'N' if self.backProp['Norm'] else 'Z'\n",
    "            \n",
    "        dW = [None] * self.layerNum\n",
    "        dB = [None] * self.layerNum\n",
    "        dReluP = [None] * self.layerNum\n",
    "        dGamma = [None] * self.layerNum\n",
    "        dBeta = [None] * self.layerNum\n",
    "        \n",
    "        dL_Div_dYtrain = self.lossFuncDer['mse'](self.layers['A'][-1], self.targetY)\n",
    "        dActivation = [None] * self.layerNum\n",
    "        for i in reversed(range(self.layerNum)):\n",
    "#             print('i: ', i)\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                dActivation[i] = self.activFuncDer[self.aFuncChosen[i]](self.layers[layerNameFirst][i], i)\n",
    "            else:\n",
    "                dActivation[i] = self.activFuncDer[self.aFuncChosen[i]](self.layers[layerNameFirst][i])\n",
    "#             print(f'weight[{i}] shape: {self.weights[i].shape} \\nbias[{i}] shape: {self.biases[i].shape} \\n')\n",
    "            if i == self.layerNum - 1:\n",
    "                dB[i] = dL_Div_dYtrain * dActivation[i]\n",
    "                if self.backProp['PRelu']:\n",
    "                    dReluP[i] = torch.mean(dL_Div_dYtrain * self.layers[layerNameFirst][i])\n",
    "                if self.backProp['Norm']:\n",
    "                    dB[i] *= self.BN['gamma'][i] / self.BN['cache'][i]['BNfenmu']\n",
    "                    dBeta[i] = dL_Div_dYtrain * dActivation[i]\n",
    "                    dGamma[i] = dBeta[i] * (self.BN['cache'][i]['BNfenzi'] / self.BN['cache'][i]['BNfenmu'])\n",
    "                dW[i] = dB[i] @ torch.transpose(self.layers['A'][i-1], 0, 1)\n",
    "            else:\n",
    "                dB[i] = (torch.transpose(self.weights[i+1], 0, 1) @ dB[i+1]) * dActivation[i]\n",
    "                dW[i] = dB[i] @ torch.transpose(self.layers['A'][i-1], 0, 1)\n",
    "                if self.backProp['PRelu']:\n",
    "                    dReluP[i] = torch.mean(torch.transpose(self.weights[i], 0, 1) @ dB[i] * self.layers[layerNameFirst][i-1])\n",
    "                if self.backProp['Norm']:\n",
    "                    dBeta[i] = torch.transpose(self.weights[i+1], 0, 1) @ (dBeta[i+1] * (self.BN['gamma'][i+1] / self.BN['cache'][i+1]['BNfenmu']))\n",
    "                    dGamma[i] = dBeta[i] * (self.BN['cache'][i]['BNfenzi'] / self.BN['cache'][i]['BNfenmu'])\n",
    "#             print(f'dReluP[{i}]: {dReluP[i]}')\n",
    "            dB_Limited = self.setToLimit(dB[i], self.paramLimits['dB'])\n",
    "            dW_Limited = self.setToLimit(dW[i], self.paramLimits['dW'])\n",
    "        \n",
    "            self.biases[i] += dB_Limited * self.lr['bias']\n",
    "            self.weights[i] += dW_Limited * self.lr['weight']\n",
    "            \n",
    "#             self.biases[i] = self.setToLimit(self.biases[i], self.paramLimits['biases'])\n",
    "#             self.weights[i] = self.setToLimit(self.weights[i], self.paramLimits['weights'])\n",
    "            \n",
    "            if self.aFuncChosen[i] == 'PRelu' and self.backProp['PRelu']:\n",
    "                dReluP_Limited = self.setToLimit(dReluP[i], self.paramLimits['dReluP']).item()\n",
    "                self.reluParam[i] += dReluP[i] * self.lr['relu param']\n",
    "#                 print('PRelu back proped')\n",
    "            if self.backProp['Norm']:\n",
    "                dBeta_Limited = torch.mean(self.setToLimit(dBeta[i], self.paramLimits['dBeta'])).item()\n",
    "                dGamma_Limited = torch.mean(self.setToLimit(dGamma[i], self.paramLimits['dGamma'])).item()\n",
    "                self.BN['beta'][i] += dBeta_Limited * self.lr['BN gamma']\n",
    "                self.BN['gamma'][i] += dGamma_Limited * self.lr['BN beta']\n",
    "#                 print('BN back proped')\n",
    "#             print(self.reluParam)\n",
    "            \n",
    "    def train(self, inputs, targetY, nanInvestigate=40, epoch = 1000):\n",
    "        self.targetY = targetY\n",
    "        for e in range(epoch):\n",
    "            self.forward(self.inputs)\n",
    "            if epoch % 100 == 0 and epoch != 0:\n",
    "                loss = torch.norm(self.layers['A'][-1] - self.targetY)\n",
    "                print(e, f': loss = {loss}')\n",
    "                if loss < 5.0:\n",
    "                    for k,v in self.lr.items():\n",
    "                        self.lr[k] /= 2\n",
    "                if loss < 2.0:\n",
    "                    for k,v in self.lr.items():\n",
    "                        self.lr[k] /= 5\n",
    "                if loss < 1.0 or torch.isnan(loss):\n",
    "                    return\n",
    "#                 print(f'{i} Loss: ', torch.mean(self.lossFunc['mse'](self.layers['A'][-1], self.targetY)).item())\n",
    "#                 print(epoch/100, ': \\n', torch.transpose(self.layers['A'][-1], 0, 1))\n",
    "            self.backprop()\n",
    "            if e > nanInvestigate and nanInvestigate > 0:\n",
    "                self.saveParams('d:\\\\nanInvest_'+str(e)+'.pt', True)\n",
    "                \n",
    "    def printShape(self):\n",
    "        for i in range(self.layerNum):\n",
    "            print(f'weight[{i}] shape: ', self.weights[i].shape)\n",
    "            print(f'bias[{i}] shape: ', self.biases[i].shape)\n",
    "            print(f'Relu Params[{i}]: ', self.reluParam[i])\n",
    "            print(f'BN gamma[{i}]: ', self.BN['gamma'][i])\n",
    "            print(f'BN beta[{i}]: ', self.BN['beta'][i])\n",
    "#             print(f'Z layer shape: ', self.layers['Z'][i].shape)\n",
    "#             print(f'N layer shape: ', self.layers['N'][i].shape)\n",
    "#             print(f'A layer shape: ', self.layers['A'][i].shape)\n",
    "    \n",
    "    def saveParams(self, PATH, layers=False, reluParam=False, BN=False):\n",
    "        params = {\n",
    "            'weight': self.weights, \n",
    "            'bias': self.biases\n",
    "        }\n",
    "        if layers:\n",
    "            params['layers'] = self.layers\n",
    "        if reluParam:\n",
    "            params['reluParam'] = self.reluParam\n",
    "        if BN:\n",
    "            params['BN'] = self.BN\n",
    "        torch.save(params, PATH)\n",
    "        \n",
    "    def readParams(self, PATH, layers=False, reluParam=False, BN=False):\n",
    "        params = torch.load(PATH)\n",
    "        self.weights = params['weight']\n",
    "        self.biases = params['bias']\n",
    "        if layers:\n",
    "            self.layers = params['layers']\n",
    "        if reluParam:\n",
    "            self.reluParam = params['reluParam']\n",
    "        if BN:\n",
    "            self.BN = params['BN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight[0] shape:  torch.Size([5, 400])\n",
      "bias[0] shape:  torch.Size([5, 1])\n",
      "Relu Params[0]:  0.01\n",
      "BN gamma[0]:  1\n",
      "BN beta[0]:  0\n",
      "weight[1] shape:  torch.Size([5, 5])\n",
      "bias[1] shape:  torch.Size([5, 1])\n",
      "Relu Params[1]:  0.01\n",
      "BN gamma[1]:  1\n",
      "BN beta[1]:  0\n",
      "weight[2] shape:  torch.Size([400, 5])\n",
      "bias[2] shape:  torch.Size([400, 1])\n",
      "Relu Params[2]:  0.01\n",
      "BN gamma[2]:  1\n",
      "BN beta[2]:  0\n",
      "0 : loss = 62.37351830057729\n",
      "1 : loss = 47.38406365293229\n",
      "2 : loss = 44.07994685948557\n",
      "3 : loss = 43.34796148067238\n",
      "4 : loss = 43.082165989254136\n",
      "5 : loss = 42.95356338668195\n",
      "6 : loss = 42.88382469680335\n",
      "7 : loss = 42.84068912013821\n",
      "8 : loss = 42.81156003703165\n",
      "9 : loss = 42.7906275508252\n",
      "10 : loss = 42.77486977655901\n",
      "11 : loss = 42.762561422055484\n",
      "12 : loss = 42.75265638783953\n",
      "13 : loss = 42.7444839082591\n",
      "14 : loss = 42.737602710524456\n",
      "15 : loss = 42.73170399694605\n",
      "16 : loss = 42.72656914146947\n",
      "17 : loss = 42.722041243001875\n",
      "18 : loss = 42.71800174208654\n",
      "19 : loss = 42.714361815823025\n",
      "20 : loss = 42.71105198475234\n",
      "21 : loss = 42.70801728050032\n",
      "22 : loss = 42.705214819449125\n",
      "23 : loss = 42.70260897625118\n",
      "24 : loss = 42.700171295099004\n",
      "25 : loss = 42.69787889733606\n",
      "26 : loss = 42.69571270985024\n",
      "27 : loss = 42.69365646963184\n",
      "28 : loss = 42.69169722836317\n",
      "29 : loss = 42.689823541469686\n",
      "30 : loss = 42.688026065968444\n",
      "31 : loss = 42.686296881251266\n",
      "32 : loss = 42.68462893151455\n",
      "33 : loss = 42.68301613594296\n",
      "34 : loss = 42.681453271555405\n",
      "35 : loss = 42.67993538438589\n",
      "36 : loss = 42.67845833173701\n",
      "37 : loss = 42.67701818323066\n",
      "38 : loss = 42.67561160476957\n",
      "39 : loss = 42.67423557043512\n",
      "40 : loss = 42.6728875834659\n",
      "41 : loss = 42.67156536444867\n",
      "42 : loss = 42.67026694824679\n",
      "43 : loss = 42.66899052592248\n",
      "44 : loss = 42.66773449668628\n",
      "45 : loss = 42.66649740457904\n",
      "46 : loss = 42.66527794687998\n",
      "47 : loss = 42.66407490804785\n",
      "48 : loss = 42.662887282252385\n",
      "49 : loss = 42.66171419367037\n",
      "50 : loss = 42.660554726585126\n",
      "51 : loss = 42.659408118753745\n",
      "52 : loss = 42.65827362044366\n",
      "53 : loss = 42.65715052249177\n",
      "54 : loss = 42.65603820543864\n",
      "55 : loss = 42.65493613705316\n",
      "56 : loss = 42.653843816459045\n",
      "57 : loss = 42.652760793189024\n",
      "58 : loss = 42.65168658877831\n",
      "59 : loss = 42.65062080172835\n",
      "60 : loss = 42.64956306392114\n",
      "61 : loss = 42.64851293831954\n",
      "62 : loss = 42.64747011555677\n",
      "63 : loss = 42.64643428219925\n",
      "64 : loss = 42.64540514550175\n",
      "65 : loss = 42.64438245266374\n",
      "66 : loss = 42.6433659661526\n",
      "67 : loss = 42.64235546535417\n",
      "68 : loss = 42.641350745449216\n",
      "69 : loss = 42.64035158845544\n",
      "70 : loss = 42.6393577867532\n",
      "71 : loss = 42.63836912260107\n",
      "72 : loss = 42.63738539262439\n",
      "73 : loss = 42.63640643555711\n",
      "74 : loss = 42.63543210281382\n",
      "75 : loss = 42.6344622473701\n",
      "76 : loss = 42.633496678855764\n",
      "77 : loss = 42.63253523941054\n",
      "78 : loss = 42.63157780418673\n",
      "79 : loss = 42.63062427983021\n",
      "80 : loss = 42.62967458605332\n",
      "81 : loss = 42.62872863054917\n",
      "82 : loss = 42.62778631669854\n",
      "83 : loss = 42.62684755831582\n",
      "84 : loss = 42.625912263793595\n",
      "85 : loss = 42.62498033497459\n",
      "86 : loss = 42.62405168317925\n",
      "87 : loss = 42.62312624895901\n",
      "88 : loss = 42.62220395975355\n",
      "89 : loss = 42.621284753882414\n",
      "90 : loss = 42.62036859244512\n",
      "91 : loss = 42.619455457793364\n",
      "92 : loss = 42.61854527542691\n",
      "93 : loss = 42.61763798097002\n",
      "94 : loss = 42.61673357292742\n",
      "95 : loss = 42.615832043393986\n",
      "96 : loss = 42.61493346138063\n",
      "97 : loss = 42.61403784233484\n",
      "98 : loss = 42.613145098435886\n",
      "99 : loss = 42.612255144874354\n",
      "100 : loss = 42.61136790335672\n",
      "101 : loss = 42.61048329493448\n",
      "102 : loss = 42.609601241042874\n",
      "103 : loss = 42.608721666661566\n",
      "104 : loss = 42.60784450011509\n",
      "105 : loss = 42.6069696757917\n",
      "106 : loss = 42.606097135070144\n",
      "107 : loss = 42.605226817687615\n",
      "108 : loss = 42.60435866903196\n",
      "109 : loss = 42.603492651154\n",
      "110 : loss = 42.60262871276186\n",
      "111 : loss = 42.601766798074806\n",
      "112 : loss = 42.600906856968365\n",
      "113 : loss = 42.600048853271346\n",
      "114 : loss = 42.599192743427786\n",
      "115 : loss = 42.59833848372279\n",
      "116 : loss = 42.597486026841615\n",
      "117 : loss = 42.5966353347835\n",
      "118 : loss = 42.59578636777196\n",
      "119 : loss = 42.59493908309561\n",
      "120 : loss = 42.59409344160693\n",
      "121 : loss = 42.593249414714656\n",
      "122 : loss = 42.59240696941505\n",
      "123 : loss = 42.59156607577663\n",
      "124 : loss = 42.590726697095235\n",
      "125 : loss = 42.589888811819634\n",
      "126 : loss = 42.5890523855105\n",
      "127 : loss = 42.58821739502561\n",
      "128 : loss = 42.5873838117519\n",
      "129 : loss = 42.58655160369671\n",
      "130 : loss = 42.58572075339645\n",
      "131 : loss = 42.58489125454439\n",
      "132 : loss = 42.584063101509905\n",
      "133 : loss = 42.58323626356556\n",
      "134 : loss = 42.58241072514165\n",
      "135 : loss = 42.58158645951073\n",
      "136 : loss = 42.58076346483856\n",
      "137 : loss = 42.579941712426205\n",
      "138 : loss = 42.57912117452157\n",
      "139 : loss = 42.57830182428045\n",
      "140 : loss = 42.577483635729436\n",
      "141 : loss = 42.576666583730635\n",
      "142 : loss = 42.57585064394805\n",
      "143 : loss = 42.57503579281561\n",
      "144 : loss = 42.57422200750666\n",
      "145 : loss = 42.57340926590491\n",
      "146 : loss = 42.572597546576674\n",
      "147 : loss = 42.57178682874451\n",
      "148 : loss = 42.57097709226193\n",
      "149 : loss = 42.570168317589356\n",
      "150 : loss = 42.569360485771135\n",
      "151 : loss = 42.56855357841358\n",
      "152 : loss = 42.567747577663965\n",
      "153 : loss = 42.56694246619052\n",
      "154 : loss = 42.566138227163215\n",
      "155 : loss = 42.56533484423547\n",
      "156 : loss = 42.564532301526576\n",
      "157 : loss = 42.56373058360496\n",
      "158 : loss = 42.56292967547206\n",
      "159 : loss = 42.56212956254702\n",
      "160 : loss = 42.56133023065185\n",
      "161 : loss = 42.56053166599742\n",
      "162 : loss = 42.55973385516987\n",
      "163 : loss = 42.55893678511763\n",
      "164 : loss = 42.55814044313904\n",
      "165 : loss = 42.557344816870355\n",
      "166 : loss = 42.55654989427436\n",
      "167 : loss = 42.55575566362937\n",
      "168 : loss = 42.55496211351864\n",
      "169 : loss = 42.55416923282032\n",
      "170 : loss = 42.55337701069767\n",
      "171 : loss = 42.55258543658976\n",
      "172 : loss = 42.55179450794884\n",
      "173 : loss = 42.5510042400394\n",
      "174 : loss = 42.55021462764759\n",
      "175 : loss = 42.54942565996967\n",
      "176 : loss = 42.548637326472225\n",
      "177 : loss = 42.54784961688358\n",
      "178 : loss = 42.54706252118564\n",
      "179 : loss = 42.546276029605885\n",
      "180 : loss = 42.54549013260978\n",
      "181 : loss = 42.54470482089341\n",
      "182 : loss = 42.5439200853764\n",
      "183 : loss = 42.54313591719511\n",
      "184 : loss = 42.542352320881704\n",
      "185 : loss = 42.5415692942503\n",
      "186 : loss = 42.540786828445434\n",
      "187 : loss = 42.540004914819626\n",
      "188 : loss = 42.53922354492727\n",
      "189 : loss = 42.53844271051863\n",
      "190 : loss = 42.537662442757885\n",
      "191 : loss = 42.53688276714654\n",
      "192 : loss = 42.53610368518081\n",
      "193 : loss = 42.53532519488372\n",
      "194 : loss = 42.534547298259454\n",
      "195 : loss = 42.53376998521322\n",
      "196 : loss = 42.53299324588399\n",
      "197 : loss = 42.53221707063794\n",
      "198 : loss = 42.531441450061926\n",
      "199 : loss = 42.53066637495729\n",
      "200 : loss = 42.52989183633386\n",
      "201 : loss = 42.52911782540409\n",
      "202 : loss = 42.528344333577465\n",
      "203 : loss = 42.52757135245504\n",
      "204 : loss = 42.52679887382417\n",
      "205 : loss = 42.52602688965344\n",
      "206 : loss = 42.52525539208769\n",
      "207 : loss = 42.52448437351633\n",
      "208 : loss = 42.523713826494216\n",
      "209 : loss = 42.522943743666545\n",
      "210 : loss = 42.52217411783664\n",
      "211 : loss = 42.5214049419617\n",
      "212 : loss = 42.520636209148755\n",
      "213 : loss = 42.51986791265078\n",
      "214 : loss = 42.51910004586282\n",
      "215 : loss = 42.51833261770239\n",
      "216 : loss = 42.51756562990835\n",
      "217 : loss = 42.5167990756634\n",
      "218 : loss = 42.5160329482926\n",
      "219 : loss = 42.51526724125965\n",
      "220 : loss = 42.5145019481634\n",
      "221 : loss = 42.51373706273437\n",
      "222 : loss = 42.51297257883149\n",
      "223 : loss = 42.512208490438795\n",
      "224 : loss = 42.51144479166239\n",
      "225 : loss = 42.51068147672732\n",
      "226 : loss = 42.50991853997468\n",
      "227 : loss = 42.50915597585871\n",
      "228 : loss = 42.50839377894406\n",
      "229 : loss = 42.50763194390305\n",
      "230 : loss = 42.50687048816385\n",
      "231 : loss = 42.50610940643797\n",
      "232 : loss = 42.505348693152555\n",
      "233 : loss = 42.50458834284435\n",
      "234 : loss = 42.50382835015715\n",
      "235 : loss = 42.50306870983918\n",
      "236 : loss = 42.50230941674065\n",
      "237 : loss = 42.50155046581133\n",
      "238 : loss = 42.50079185209816\n",
      "239 : loss = 42.50003357074304\n",
      "240 : loss = 42.49927561698055\n",
      "241 : loss = 42.498517986135816\n",
      "242 : loss = 42.497760673622416\n",
      "243 : loss = 42.4970036749403\n",
      "244 : loss = 42.496246985673864\n",
      "245 : loss = 42.49549060148996\n",
      "246 : loss = 42.494734518136056\n",
      "247 : loss = 42.49397873143837\n",
      "248 : loss = 42.49322323730013\n",
      "249 : loss = 42.492468031699794\n",
      "250 : loss = 42.491713110689396\n",
      "251 : loss = 42.490958470392904\n",
      "252 : loss = 42.490204107004594\n",
      "253 : loss = 42.489450016787515\n",
      "254 : loss = 42.48869619607196\n",
      "255 : loss = 42.48794264125399\n",
      "256 : loss = 42.48718934879401\n",
      "257 : loss = 42.48643631521534\n",
      "258 : loss = 42.48568353710287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 : loss = 42.484931011101715\n",
      "260 : loss = 42.48417873391592\n",
      "261 : loss = 42.4834267023072\n",
      "262 : loss = 42.482674913093724\n",
      "263 : loss = 42.48192336314888\n",
      "264 : loss = 42.48117204940012\n",
      "265 : loss = 42.48042096882783\n",
      "266 : loss = 42.47967011846418\n",
      "267 : loss = 42.478919495392084\n",
      "268 : loss = 42.47816909674413\n",
      "269 : loss = 42.477418919701506\n",
      "270 : loss = 42.47666896149304\n",
      "271 : loss = 42.4759192193942\n",
      "272 : loss = 42.47516969072614\n",
      "273 : loss = 42.47442037285475\n",
      "274 : loss = 42.47367126318975\n",
      "275 : loss = 42.472922359183805\n",
      "276 : loss = 42.472173658331656\n",
      "277 : loss = 42.47142515816926\n",
      "278 : loss = 42.47067685627297\n",
      "279 : loss = 42.46992875025873\n",
      "280 : loss = 42.46918083778126\n",
      "281 : loss = 42.468433116533305\n",
      "282 : loss = 42.467685584244904\n",
      "283 : loss = 42.46693823868259\n",
      "284 : loss = 42.46619107764875\n",
      "285 : loss = 42.46544409898085\n",
      "286 : loss = 42.46469730055082\n",
      "287 : loss = 42.46395068026431\n",
      "288 : loss = 42.463204236060086\n",
      "289 : loss = 42.46245796590938\n",
      "290 : loss = 42.461711867815275\n",
      "291 : loss = 42.46096593981204\n",
      "292 : loss = 42.46022017996463\n",
      "293 : loss = 42.459474586367996\n",
      "294 : loss = 42.45872915714659\n",
      "295 : loss = 42.45798389045378\n",
      "296 : loss = 42.457238784471315\n",
      "297 : loss = 42.45649383740877\n",
      "298 : loss = 42.45574904750304\n",
      "299 : loss = 42.455004413017846\n",
      "300 : loss = 42.45425993224319\n",
      "301 : loss = 42.45351560349493\n",
      "302 : loss = 42.45277142511425\n",
      "303 : loss = 42.452027395467226\n",
      "304 : loss = 42.45128351294434\n",
      "305 : loss = 42.45053977596008\n",
      "306 : loss = 42.44979618295244\n",
      "307 : loss = 42.449052732382555\n",
      "308 : loss = 42.44830942273424\n",
      "309 : loss = 42.44756625251359\n",
      "310 : loss = 42.4468232202486\n",
      "311 : loss = 42.44608032448876\n",
      "312 : loss = 42.445337563804635\n",
      "313 : loss = 42.44459493678757\n",
      "314 : loss = 42.443852442049234\n",
      "315 : loss = 42.44311007822133\n",
      "316 : loss = 42.44236784395519\n",
      "317 : loss = 42.44162573792146\n",
      "318 : loss = 42.440883758809775\n",
      "319 : loss = 42.44014190532838\n",
      "320 : loss = 42.43940017620385\n",
      "321 : loss = 42.438658570180735\n",
      "322 : loss = 42.437917086021294\n",
      "323 : loss = 42.437175722505145\n",
      "324 : loss = 42.43643447842901\n",
      "325 : loss = 42.43569335260637\n",
      "326 : loss = 42.43495234386724\n",
      "327 : loss = 42.43421145105782\n",
      "328 : loss = 42.43347067304029\n",
      "329 : loss = 42.43273000869248\n",
      "330 : loss = 42.43198945690765\n",
      "331 : loss = 42.431249016594194\n",
      "332 : loss = 42.43050868667542\n",
      "333 : loss = 42.42976846608927\n",
      "334 : loss = 42.42902835378813\n",
      "335 : loss = 42.428288348738484\n",
      "336 : loss = 42.42754844992081\n",
      "337 : loss = 42.42680865632928\n",
      "338 : loss = 42.426068966971506\n",
      "339 : loss = 42.425329380868384\n",
      "340 : loss = 42.42458989705385\n",
      "341 : loss = 42.42385051457465\n",
      "342 : loss = 42.423111232490164\n",
      "343 : loss = 42.42237204987218\n",
      "344 : loss = 42.42163296580469\n",
      "345 : loss = 42.42089397938371\n",
      "346 : loss = 42.42015508971709\n",
      "347 : loss = 42.419416295924286\n",
      "348 : loss = 42.41867759713622\n",
      "349 : loss = 42.41793899249508\n",
      "350 : loss = 42.41720048115414\n",
      "351 : loss = 42.41646206227758\n",
      "352 : loss = 42.415723735040324\n",
      "353 : loss = 42.41498549862785\n",
      "354 : loss = 42.41424735223607\n",
      "355 : loss = 42.413509295071115\n",
      "356 : loss = 42.41277132634921\n",
      "357 : loss = 42.412033445296494\n",
      "358 : loss = 42.411295651148876\n",
      "359 : loss = 42.41055794315191\n",
      "360 : loss = 42.40982032056057\n",
      "361 : loss = 42.40908278263921\n",
      "362 : loss = 42.40834532866134\n",
      "363 : loss = 42.407607957909526\n",
      "364 : loss = 42.40687066967521\n",
      "365 : loss = 42.406133463258634\n",
      "366 : loss = 42.40539633796866\n",
      "367 : loss = 42.40465929312266\n",
      "368 : loss = 42.40392232804639\n",
      "369 : loss = 42.40318544207384\n",
      "370 : loss = 42.40244863454714\n",
      "371 : loss = 42.40171190481643\n",
      "372 : loss = 42.400975252239725\n",
      "373 : loss = 42.40023867618281\n",
      "374 : loss = 42.39950217601913\n",
      "375 : loss = 42.39876575112967\n",
      "376 : loss = 42.39802940090286\n",
      "377 : loss = 42.397293124734404\n",
      "378 : loss = 42.39655692202727\n",
      "379 : loss = 42.39582079219151\n",
      "380 : loss = 42.39508473464418\n",
      "381 : loss = 42.394348748809236\n",
      "382 : loss = 42.39361283411745\n",
      "383 : loss = 42.39287699000627\n",
      "384 : loss = 42.39214121591979\n",
      "385 : loss = 42.391405511308584\n",
      "386 : loss = 42.390669875629634\n",
      "387 : loss = 42.38993430834629\n",
      "388 : loss = 42.389198808928114\n",
      "389 : loss = 42.388463376850794\n",
      "390 : loss = 42.38772801159612\n",
      "391 : loss = 42.38699271265181\n",
      "392 : loss = 42.38625747951152\n",
      "393 : loss = 42.38552231167466\n",
      "394 : loss = 42.38478720864642\n",
      "395 : loss = 42.3840521699376\n",
      "396 : loss = 42.38331719506454\n",
      "397 : loss = 42.38258228354914\n",
      "398 : loss = 42.38184743491864\n",
      "399 : loss = 42.38111264870566\n",
      "400 : loss = 42.38037792444806\n",
      "401 : loss = 42.37964326168889\n",
      "402 : loss = 42.37890865997635\n",
      "403 : loss = 42.37817411886363\n",
      "404 : loss = 42.37743963790897\n",
      "405 : loss = 42.376705216675454\n",
      "406 : loss = 42.375970854731044\n",
      "407 : loss = 42.37523655164847\n",
      "408 : loss = 42.37450230700519\n",
      "409 : loss = 42.373768120383296\n",
      "410 : loss = 42.37303399136947\n",
      "411 : loss = 42.372299919554905\n",
      "412 : loss = 42.37156590453529\n",
      "413 : loss = 42.37083194591069\n",
      "414 : loss = 42.37009804328552\n",
      "415 : loss = 42.3693641962685\n",
      "416 : loss = 42.36863040447256\n",
      "417 : loss = 42.36789666751481\n",
      "418 : loss = 42.367162985016485\n",
      "419 : loss = 42.36642935660287\n",
      "420 : loss = 42.36569578190329\n",
      "421 : loss = 42.36496226055098\n",
      "422 : loss = 42.36422879218314\n",
      "423 : loss = 42.36349537644078\n",
      "424 : loss = 42.36276201296872\n",
      "425 : loss = 42.362028701415575\n",
      "426 : loss = 42.361295441433626\n",
      "427 : loss = 42.36056223267881\n",
      "428 : loss = 42.3598290748107\n",
      "429 : loss = 42.359095967492436\n",
      "430 : loss = 42.358362910406164\n",
      "431 : loss = 42.357629903305494\n",
      "432 : loss = 42.3568969458639\n",
      "433 : loss = 42.356164037758255\n",
      "434 : loss = 42.355431178668745\n",
      "435 : loss = 42.35469836827889\n",
      "436 : loss = 42.35396560627546\n",
      "437 : loss = 42.35323289234843\n",
      "438 : loss = 42.35250022619097\n",
      "439 : loss = 42.35176760749938\n",
      "440 : loss = 42.35103503597305\n",
      "441 : loss = 42.35030251131444\n",
      "442 : loss = 42.34957003322902\n",
      "443 : loss = 42.34883760142525\n",
      "444 : loss = 42.348105215614545\n",
      "445 : loss = 42.347372875511205\n",
      "446 : loss = 42.34664058083242\n",
      "447 : loss = 42.34590833129822\n",
      "448 : loss = 42.345176126631436\n",
      "449 : loss = 42.344443966557655\n",
      "450 : loss = 42.34371185080521\n",
      "451 : loss = 42.342979779105136\n",
      "452 : loss = 42.34224775119113\n",
      "453 : loss = 42.34151576679953\n",
      "454 : loss = 42.34078382566928\n",
      "455 : loss = 42.340051927541865\n",
      "456 : loss = 42.33932007216138\n",
      "457 : loss = 42.33858825927434\n",
      "458 : loss = 42.33785648862981\n",
      "459 : loss = 42.337124759979275\n",
      "460 : loss = 42.33639307307665\n",
      "461 : loss = 42.33566142767823\n",
      "462 : loss = 42.334929823542694\n",
      "463 : loss = 42.33419826043103\n",
      "464 : loss = 42.33346673810655\n",
      "465 : loss = 42.33273525633485\n",
      "466 : loss = 42.33200381488375\n",
      "467 : loss = 42.33127241352333\n",
      "468 : loss = 42.33054105202586\n",
      "469 : loss = 42.32980973016577\n",
      "470 : loss = 42.32907844780582\n",
      "471 : loss = 42.328347204734996\n",
      "472 : loss = 42.32761600073403\n",
      "473 : loss = 42.32688483558576\n",
      "474 : loss = 42.32615370907506\n",
      "475 : loss = 42.32542262098884\n",
      "476 : loss = 42.324691571115984\n",
      "477 : loss = 42.323960559247396\n",
      "478 : loss = 42.32322958517592\n",
      "479 : loss = 42.322498648696325\n",
      "480 : loss = 42.32176774960531\n",
      "481 : loss = 42.321036887701446\n",
      "482 : loss = 42.320306062785185\n",
      "483 : loss = 42.319575274658824\n",
      "484 : loss = 42.318844523126494\n",
      "485 : loss = 42.31811380799411\n",
      "486 : loss = 42.317383129069384\n",
      "487 : loss = 42.31665248616181\n",
      "488 : loss = 42.315921879082595\n",
      "489 : loss = 42.315191307644696\n",
      "490 : loss = 42.31446077166278\n",
      "491 : loss = 42.313730270953165\n",
      "492 : loss = 42.31299980533387\n",
      "493 : loss = 42.31226937462456\n",
      "494 : loss = 42.31153897864651\n",
      "495 : loss = 42.31080861722264\n",
      "496 : loss = 42.31007829017744\n",
      "497 : loss = 42.30934799733697\n",
      "498 : loss = 42.308617738528895\n",
      "499 : loss = 42.307887513582365\n",
      "500 : loss = 42.307157322328095\n",
      "501 : loss = 42.30642716459828\n",
      "502 : loss = 42.305697040226654\n",
      "503 : loss = 42.304966949048364\n",
      "504 : loss = 42.30423689090007\n",
      "505 : loss = 42.303506865619845\n",
      "506 : loss = 42.30277687304721\n",
      "507 : loss = 42.30204691302308\n",
      "508 : loss = 42.30131698538977\n",
      "509 : loss = 42.30058708999099\n",
      "510 : loss = 42.29985722667181\n",
      "511 : loss = 42.29912739527865\n",
      "512 : loss = 42.29839759565929\n",
      "513 : loss = 42.29766782766279\n",
      "514 : loss = 42.29693809113955\n",
      "515 : loss = 42.29620838594128\n",
      "516 : loss = 42.29547871192092\n",
      "517 : loss = 42.294749068932724\n",
      "518 : loss = 42.29401945683218\n",
      "519 : loss = 42.29328987547605\n",
      "520 : loss = 42.29256032472226\n",
      "521 : loss = 42.29183080443002\n",
      "522 : loss = 42.29110131445969\n",
      "523 : loss = 42.29037185467285\n",
      "524 : loss = 42.289642424932254\n",
      "525 : loss = 42.2889130251018\n",
      "526 : loss = 42.28818365504657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527 : loss = 42.28745431463276\n",
      "528 : loss = 42.28672500372771\n",
      "529 : loss = 42.28599572219987\n",
      "530 : loss = 42.2852664699188\n",
      "531 : loss = 42.28453724675516\n",
      "532 : loss = 42.28380805258067\n",
      "533 : loss = 42.283078887268154\n",
      "534 : loss = 42.28234975069146\n",
      "535 : loss = 42.28162064272552\n",
      "536 : loss = 42.280891563246286\n",
      "537 : loss = 42.28016251213074\n",
      "538 : loss = 42.27943348925688\n",
      "539 : loss = 42.278704494503735\n",
      "540 : loss = 42.27797552775128\n",
      "541 : loss = 42.27724658888054\n",
      "542 : loss = 42.27651767777347\n",
      "543 : loss = 42.275788794313\n",
      "544 : loss = 42.275059938383045\n",
      "545 : loss = 42.274331109868434\n",
      "546 : loss = 42.27360230865495\n",
      "547 : loss = 42.272873534629305\n",
      "548 : loss = 42.27214478767913\n",
      "549 : loss = 42.27141606769296\n",
      "550 : loss = 42.27068737456024\n",
      "551 : loss = 42.2699587081713\n",
      "552 : loss = 42.26923006841735\n",
      "553 : loss = 42.268501455190496\n",
      "554 : loss = 42.26777286838368\n",
      "555 : loss = 42.26704430789072\n",
      "556 : loss = 42.26631577360628\n",
      "557 : loss = 42.26558726542587\n",
      "558 : loss = 42.26485878324581\n",
      "559 : loss = 42.26413032696327\n",
      "560 : loss = 42.263401896476225\n",
      "561 : loss = 42.26267349168345\n",
      "562 : loss = 42.26194511248453\n",
      "563 : loss = 42.26121675877985\n",
      "564 : loss = 42.260488430470566\n",
      "565 : loss = 42.25976012745861\n",
      "566 : loss = 42.25903184964669\n",
      "567 : loss = 42.258303596938276\n",
      "568 : loss = 42.25757536923759\n",
      "569 : loss = 42.2568471664496\n",
      "570 : loss = 42.25611898848002\n",
      "571 : loss = 42.25539083523529\n",
      "572 : loss = 42.25466270662257\n",
      "573 : loss = 42.25393460254977\n",
      "574 : loss = 42.253206522925474\n",
      "575 : loss = 42.252478467658996\n",
      "576 : loss = 42.25175043666033\n",
      "577 : loss = 42.25102242984018\n",
      "578 : loss = 42.25029444710992\n",
      "579 : loss = 42.249566488381625\n",
      "580 : loss = 42.248838553568014\n",
      "581 : loss = 42.248110642582496\n",
      "582 : loss = 42.24738275533912\n",
      "583 : loss = 42.246654891752605\n",
      "584 : loss = 42.245927051738306\n",
      "585 : loss = 42.24519923521223\n",
      "586 : loss = 42.24447144209102\n",
      "587 : loss = 42.24374367229193\n",
      "588 : loss = 42.24301592573286\n",
      "589 : loss = 42.24228820233231\n",
      "590 : loss = 42.24156050200941\n",
      "591 : loss = 42.240832824683885\n",
      "592 : loss = 42.240105170276045\n",
      "593 : loss = 42.23937753870683\n",
      "594 : loss = 42.238649929897754\n",
      "595 : loss = 42.237922343770904\n",
      "596 : loss = 42.23719478024897\n",
      "597 : loss = 42.236467239255184\n",
      "598 : loss = 42.23573972071337\n",
      "599 : loss = 42.235012224547916\n",
      "600 : loss = 42.23428475068375\n",
      "601 : loss = 42.233557299046375\n",
      "602 : loss = 42.23282986956181\n",
      "603 : loss = 42.232102462156675\n",
      "604 : loss = 42.23137507675806\n",
      "605 : loss = 42.230647713293635\n",
      "606 : loss = 42.22992037169157\n",
      "607 : loss = 42.229193051880586\n",
      "608 : loss = 42.22846575378989\n",
      "609 : loss = 42.227738477349234\n",
      "610 : loss = 42.22701122248886\n",
      "611 : loss = 42.22628398913953\n",
      "612 : loss = 42.22555677723249\n",
      "613 : loss = 42.22482958669949\n",
      "614 : loss = 42.22410241747277\n",
      "615 : loss = 42.22337526948507\n",
      "616 : loss = 42.22264814266959\n",
      "617 : loss = 42.221921036960026\n",
      "618 : loss = 42.22119395229054\n",
      "619 : loss = 42.220466888595766\n",
      "620 : loss = 42.21973984581082\n",
      "621 : loss = 42.21901282387124\n",
      "622 : loss = 42.21828582271306\n",
      "623 : loss = 42.217558842272744\n",
      "624 : loss = 42.21683188248723\n",
      "625 : loss = 42.216104943293885\n",
      "626 : loss = 42.21537802463052\n",
      "627 : loss = 42.21465112643538\n",
      "628 : loss = 42.21392424864715\n",
      "629 : loss = 42.21319739120495\n",
      "630 : loss = 42.212470554048316\n",
      "631 : loss = 42.21174373711722\n",
      "632 : loss = 42.211016940352046\n",
      "633 : loss = 42.210290163693585\n",
      "634 : loss = 42.20956340708305\n",
      "635 : loss = 42.20883667046208\n",
      "636 : loss = 42.20810995377268\n",
      "637 : loss = 42.207383256957286\n",
      "638 : loss = 42.20665657995872\n",
      "639 : loss = 42.2059299227202\n",
      "640 : loss = 42.205203285185355\n",
      "641 : loss = 42.204476667298174\n",
      "642 : loss = 42.20375006900304\n",
      "643 : loss = 42.203023490244725\n",
      "644 : loss = 42.202296930968366\n",
      "645 : loss = 42.201570391119496\n",
      "646 : loss = 42.20084387064399\n",
      "647 : loss = 42.200117369488126\n",
      "648 : loss = 42.19939088759852\n",
      "649 : loss = 42.19866442492216\n",
      "650 : loss = 42.19793798140639\n",
      "651 : loss = 42.197211556998944\n",
      "652 : loss = 42.19648515164784\n",
      "653 : loss = 42.19575876530151\n",
      "654 : loss = 42.19503239790871\n",
      "655 : loss = 42.19430604941854\n",
      "656 : loss = 42.19357971978044\n",
      "657 : loss = 42.19285340894419\n",
      "658 : loss = 42.19212711685991\n",
      "659 : loss = 42.19140084347805\n",
      "660 : loss = 42.1906745887494\n",
      "661 : loss = 42.189948352625066\n",
      "662 : loss = 42.18922213505648\n",
      "663 : loss = 42.188495935995405\n",
      "664 : loss = 42.18776975539392\n",
      "665 : loss = 42.18704359320441\n",
      "666 : loss = 42.18631744937959\n",
      "667 : loss = 42.185591323872494\n",
      "668 : loss = 42.18486521663643\n",
      "669 : loss = 42.18413912762506\n",
      "670 : loss = 42.1834130567923\n",
      "671 : loss = 42.18268700409242\n",
      "672 : loss = 42.18196096947996\n",
      "673 : loss = 42.181234952909755\n",
      "674 : loss = 42.180508954336936\n",
      "675 : loss = 42.17978297371694\n",
      "676 : loss = 42.179057011005476\n",
      "677 : loss = 42.17833106615856\n",
      "678 : loss = 42.17760513913249\n",
      "679 : loss = 42.17687922988383\n",
      "680 : loss = 42.176153338369424\n",
      "681 : loss = 42.17542746454642\n",
      "682 : loss = 42.17470160837224\n",
      "683 : loss = 42.17397576980454\n",
      "684 : loss = 42.17324994880129\n",
      "685 : loss = 42.172524145320715\n",
      "686 : loss = 42.17179835932131\n",
      "687 : loss = 42.171072590761824\n",
      "688 : loss = 42.170346839601294\n",
      "689 : loss = 42.16962110579899\n",
      "690 : loss = 42.16889538931446\n",
      "691 : loss = 42.1681696901075\n",
      "692 : loss = 42.167444008138155\n",
      "693 : loss = 42.16671834336676\n",
      "694 : loss = 42.16599269575384\n",
      "695 : loss = 42.16526706526023\n",
      "696 : loss = 42.16454145184698\n",
      "697 : loss = 42.16381585547538\n",
      "698 : loss = 42.16309027610698\n",
      "699 : loss = 42.16236471370355\n",
      "700 : loss = 42.16163916822714\n",
      "701 : loss = 42.16091363964\n",
      "702 : loss = 42.16018812790462\n",
      "703 : loss = 42.15946263298375\n",
      "704 : loss = 42.15873715484033\n",
      "705 : loss = 42.15801169343757\n",
      "706 : loss = 42.1572862487389\n",
      "707 : loss = 42.15656082070795\n",
      "708 : loss = 42.155835409308615\n",
      "709 : loss = 42.15511001450498\n",
      "710 : loss = 42.15438463626137\n",
      "711 : loss = 42.15365927454233\n",
      "712 : loss = 42.1529339293126\n",
      "713 : loss = 42.15220860053718\n",
      "714 : loss = 42.15148328818124\n",
      "715 : loss = 42.15075799221019\n",
      "716 : loss = 42.15003271258964\n",
      "717 : loss = 42.14930744928543\n",
      "718 : loss = 42.148582202263576\n",
      "719 : loss = 42.147856971490334\n",
      "720 : loss = 42.14713175693214\n",
      "721 : loss = 42.14640655855565\n",
      "722 : loss = 42.1456813763277\n",
      "723 : loss = 42.144956210215355\n",
      "724 : loss = 42.144231060185874\n",
      "725 : loss = 42.143505926206686\n",
      "726 : loss = 42.14278080824546\n",
      "727 : loss = 42.14205570627002\n",
      "728 : loss = 42.141330620248404\n",
      "729 : loss = 42.14060555014883\n",
      "730 : loss = 42.13988049593973\n",
      "731 : loss = 42.139155457589695\n",
      "732 : loss = 42.13843043506752\n",
      "733 : loss = 42.137705428342194\n",
      "734 : loss = 42.13698043738287\n",
      "735 : loss = 42.13625546215889\n",
      "736 : loss = 42.1355305026398\n",
      "737 : loss = 42.1348055587953\n",
      "738 : loss = 42.134080630595285\n",
      "739 : loss = 42.13335571800981\n",
      "740 : loss = 42.13263082100914\n",
      "741 : loss = 42.13190593956367\n",
      "742 : loss = 42.13118107364401\n",
      "743 : loss = 42.130456223220925\n",
      "744 : loss = 42.129731388265355\n",
      "745 : loss = 42.129006568748395\n",
      "746 : loss = 42.12828176464134\n",
      "747 : loss = 42.12755697591563\n",
      "748 : loss = 42.126832202542886\n",
      "749 : loss = 42.126107444494856\n",
      "750 : loss = 42.12538270174352\n",
      "751 : loss = 42.12465797426097\n",
      "752 : loss = 42.12393326201946\n",
      "753 : loss = 42.12320856499144\n",
      "754 : loss = 42.12248388314947\n",
      "755 : loss = 42.12175921646633\n",
      "756 : loss = 42.12103456491491\n",
      "757 : loss = 42.12030992846825\n",
      "758 : loss = 42.11958530709959\n",
      "759 : loss = 42.11886070078229\n",
      "760 : loss = 42.11813610948988\n",
      "761 : loss = 42.11741153319603\n",
      "762 : loss = 42.11668697187454\n",
      "763 : loss = 42.11596242549942\n",
      "764 : loss = 42.11523789404477\n",
      "765 : loss = 42.11451337748486\n",
      "766 : loss = 42.11378887579412\n",
      "767 : loss = 42.1130643889471\n",
      "768 : loss = 42.112339916918515\n",
      "769 : loss = 42.11161545968319\n",
      "770 : loss = 42.110891017216154\n",
      "771 : loss = 42.110166589492515\n",
      "772 : loss = 42.10944217648755\n",
      "773 : loss = 42.10871777817667\n",
      "774 : loss = 42.107993394535434\n",
      "775 : loss = 42.10726902553952\n",
      "776 : loss = 42.10654467116476\n",
      "777 : loss = 42.105820331387115\n",
      "778 : loss = 42.10509600618267\n",
      "779 : loss = 42.10437169552767\n",
      "780 : loss = 42.103647399398454\n",
      "781 : loss = 42.10292311777153\n",
      "782 : loss = 42.10219885062352\n",
      "783 : loss = 42.10147459793117\n",
      "784 : loss = 42.100750359671366\n",
      "785 : loss = 42.100026135821125\n",
      "786 : loss = 42.09930192635758\n",
      "787 : loss = 42.09857773125799\n",
      "788 : loss = 42.097853550499764\n",
      "789 : loss = 42.09712938406039\n",
      "790 : loss = 42.0964052319175\n",
      "791 : loss = 42.09568109404889\n",
      "792 : loss = 42.09495697043243\n",
      "793 : loss = 42.09423286104611\n",
      "794 : loss = 42.09350876586806\n",
      "795 : loss = 42.09278468487653\n",
      "796 : loss = 42.092060618049885\n",
      "797 : loss = 42.091336565366596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798 : loss = 42.090612526805266\n",
      "799 : loss = 42.08988850234461\n",
      "800 : loss = 42.08916449196345\n",
      "801 : loss = 42.088440495640754\n",
      "802 : loss = 42.08771651335555\n",
      "803 : loss = 42.08699254508703\n",
      "804 : loss = 42.08626859081447\n",
      "805 : loss = 42.08554465051727\n",
      "806 : loss = 42.08482072417494\n",
      "807 : loss = 42.08409681176709\n",
      "808 : loss = 42.08337291327345\n",
      "809 : loss = 42.082649028673856\n",
      "810 : loss = 42.08192515794825\n",
      "811 : loss = 42.08120130107668\n",
      "812 : loss = 42.080477458039304\n",
      "813 : loss = 42.0797536288164\n",
      "814 : loss = 42.07902981338832\n",
      "815 : loss = 42.078306011735535\n",
      "816 : loss = 42.07758222383863\n",
      "817 : loss = 42.07685844967829\n",
      "818 : loss = 42.07613468923528\n",
      "819 : loss = 42.07541094249049\n",
      "820 : loss = 42.074687209424916\n",
      "821 : loss = 42.073963490019636\n",
      "822 : loss = 42.07323978425583\n",
      "823 : loss = 42.07251609211478\n",
      "824 : loss = 42.07179241357788\n",
      "825 : loss = 42.0710687486266\n",
      "826 : loss = 42.07034509724253\n",
      "827 : loss = 42.06962145940732\n",
      "828 : loss = 42.06889783510277\n",
      "829 : loss = 42.06817422431072\n",
      "830 : loss = 42.06745062701313\n",
      "831 : loss = 42.066727043192074\n",
      "832 : loss = 42.06600347282968\n",
      "833 : loss = 42.065279915908185\n",
      "834 : loss = 42.06455637240993\n",
      "835 : loss = 42.063832842317346\n",
      "836 : loss = 42.06310932561293\n",
      "837 : loss = 42.062385822279296\n",
      "838 : loss = 42.06166233229913\n",
      "839 : loss = 42.06093885565522\n",
      "840 : loss = 42.06021539233044\n",
      "841 : loss = 42.05949194230776\n",
      "842 : loss = 42.0587685055702\n",
      "843 : loss = 42.05804508210092\n",
      "844 : loss = 42.05732167188312\n",
      "845 : loss = 42.056598274900125\n",
      "846 : loss = 42.05587489113532\n",
      "847 : loss = 42.05515152057218\n",
      "848 : loss = 42.05442816319427\n",
      "849 : loss = 42.053704818985224\n",
      "850 : loss = 42.05298148792878\n",
      "851 : loss = 42.05225817000874\n",
      "852 : loss = 42.051534865209014\n",
      "853 : loss = 42.05081157351354\n",
      "854 : loss = 42.050088294906416\n",
      "855 : loss = 42.04936502937174\n",
      "856 : loss = 42.04864177689376\n",
      "857 : loss = 42.047918537456745\n",
      "858 : loss = 42.04719531104508\n",
      "859 : loss = 42.04647209764321\n",
      "860 : loss = 42.04574889723567\n",
      "861 : loss = 42.04502570980707\n",
      "862 : loss = 42.0443025353421\n",
      "863 : loss = 42.04357937382551\n",
      "864 : loss = 42.04285622524213\n",
      "865 : loss = 42.04213308957689\n",
      "866 : loss = 42.04140996681477\n",
      "867 : loss = 42.04068685694083\n",
      "868 : loss = 42.03996375994022\n",
      "869 : loss = 42.03924067579813\n",
      "870 : loss = 42.038517604499866\n",
      "871 : loss = 42.03779454603077\n",
      "872 : loss = 42.037071500376264\n",
      "873 : loss = 42.03634846752187\n",
      "874 : loss = 42.03562544745314\n",
      "875 : loss = 42.034902440155726\n",
      "876 : loss = 42.034179445615344\n",
      "877 : loss = 42.03345646381778\n",
      "878 : loss = 42.03273349474888\n",
      "879 : loss = 42.03201053839457\n",
      "880 : loss = 42.031287594740846\n",
      "881 : loss = 42.03056466377375\n",
      "882 : loss = 42.029841745479445\n",
      "883 : loss = 42.029118839844095\n",
      "884 : loss = 42.02839594685397\n",
      "885 : loss = 42.0276730664954\n",
      "886 : loss = 42.02695019875479\n",
      "887 : loss = 42.026227343618594\n",
      "888 : loss = 42.02550450107334\n",
      "889 : loss = 42.02478167110562\n",
      "890 : loss = 42.0240588537021\n",
      "891 : loss = 42.023336048849494\n",
      "892 : loss = 42.02261325653459\n",
      "893 : loss = 42.02189047674424\n",
      "894 : loss = 42.021167709465345\n",
      "895 : loss = 42.02044495468488\n",
      "896 : loss = 42.0197222123899\n",
      "897 : loss = 42.0189994825675\n",
      "898 : loss = 42.01827676520483\n",
      "899 : loss = 42.01755406028913\n",
      "900 : loss = 42.01683136780767\n",
      "901 : loss = 42.016108687747796\n",
      "902 : loss = 42.01538602009691\n",
      "903 : loss = 42.01466336484249\n",
      "904 : loss = 42.01394072197204\n",
      "905 : loss = 42.01321809147316\n",
      "906 : loss = 42.012495473333495\n",
      "907 : loss = 42.01177286754073\n",
      "908 : loss = 42.01105027408263\n",
      "909 : loss = 42.010327692947016\n",
      "910 : loss = 42.00960512412175\n",
      "911 : loss = 42.008882567594775\n",
      "912 : loss = 42.00816002335407\n",
      "913 : loss = 42.00743749138768\n",
      "914 : loss = 42.006714971683714\n",
      "915 : loss = 42.00599246423032\n",
      "916 : loss = 42.0052699690157\n",
      "917 : loss = 42.00454748602814\n",
      "918 : loss = 42.00382501525594\n",
      "919 : loss = 42.003102556687494\n",
      "920 : loss = 42.00238011031122\n",
      "921 : loss = 42.00165767611561\n",
      "922 : loss = 42.00093525408919\n",
      "923 : loss = 42.000212844220556\n",
      "924 : loss = 41.999490446498356\n",
      "925 : loss = 41.99876806091127\n",
      "926 : loss = 41.99804568744805\n",
      "927 : loss = 41.9973233260975\n",
      "928 : loss = 41.99660097684848\n",
      "929 : loss = 41.99587863968988\n",
      "930 : loss = 41.99515631461065\n",
      "931 : loss = 41.994434001599814\n",
      "932 : loss = 41.993711700646394\n",
      "933 : loss = 41.99298941173952\n",
      "934 : loss = 41.99226713486834\n",
      "935 : loss = 41.99154487002207\n",
      "936 : loss = 41.990822617189934\n",
      "937 : loss = 41.99010037636127\n",
      "938 : loss = 41.9893781475254\n",
      "939 : loss = 41.98865593067174\n",
      "940 : loss = 41.98793372578974\n",
      "941 : loss = 41.98721153286887\n",
      "942 : loss = 41.986489351898705\n",
      "943 : loss = 41.98576718286882\n",
      "944 : loss = 41.985045025768855\n",
      "945 : loss = 41.9843228805885\n",
      "946 : loss = 41.983600747317475\n",
      "947 : loss = 41.98287862594557\n",
      "948 : loss = 41.982156516462595\n",
      "949 : loss = 41.98143441885842\n",
      "950 : loss = 41.98071233312298\n",
      "951 : loss = 41.97999025924621\n",
      "952 : loss = 41.979268197218126\n",
      "953 : loss = 41.97854614702878\n",
      "954 : loss = 41.977824108668266\n",
      "955 : loss = 41.977102082126706\n",
      "956 : loss = 41.9763800673943\n",
      "957 : loss = 41.97565806446128\n",
      "958 : loss = 41.97493607331789\n",
      "959 : loss = 41.97421409395446\n",
      "960 : loss = 41.97349212636134\n",
      "961 : loss = 41.97277017052894\n",
      "962 : loss = 41.97204822644769\n",
      "963 : loss = 41.971326294108074\n",
      "964 : loss = 41.97060437350061\n",
      "965 : loss = 41.969882464615885\n",
      "966 : loss = 41.96916056744449\n",
      "967 : loss = 41.96843868197708\n",
      "968 : loss = 41.96771680820434\n",
      "969 : loss = 41.96699494611701\n",
      "970 : loss = 41.966273095705866\n",
      "971 : loss = 41.96555125696171\n",
      "972 : loss = 41.9648294298754\n",
      "973 : loss = 41.964107614437836\n",
      "974 : loss = 41.96338581063994\n",
      "975 : loss = 41.96266401847269\n",
      "976 : loss = 41.96194223792709\n",
      "977 : loss = 41.961220468994206\n",
      "978 : loss = 41.96049871166511\n",
      "979 : loss = 41.95977696593094\n",
      "980 : loss = 41.95905523178286\n",
      "981 : loss = 41.95833350921208\n",
      "982 : loss = 41.95761179820984\n",
      "983 : loss = 41.95689009876741\n",
      "984 : loss = 41.95616841087613\n",
      "985 : loss = 41.955446734527335\n",
      "986 : loss = 41.95472506971244\n",
      "987 : loss = 41.95400341642285\n",
      "988 : loss = 41.95328177465005\n",
      "989 : loss = 41.95256014438554\n",
      "990 : loss = 41.95183852562085\n",
      "991 : loss = 41.95111691834757\n",
      "992 : loss = 41.95039532255731\n",
      "993 : loss = 41.94967373824171\n",
      "994 : loss = 41.94895216539246\n",
      "995 : loss = 41.94823060400128\n",
      "996 : loss = 41.94750905405991\n",
      "997 : loss = 41.94678751556016\n",
      "998 : loss = 41.94606598849385\n",
      "999 : loss = 41.94534447285282\n",
      "output:  tensor([-0.1218,  0.1978,  0.3496,  1.7690,  0.5936,  0.6531,  0.1617,  1.3281,\n",
      "        -0.0159, -0.3894,  0.6605,  1.0043, -0.1773,  0.2005,  1.2077,  0.3723,\n",
      "        -2.1267,  0.2134, -0.5094, -1.7617,  0.8405, -0.1563, -1.2261, -0.4667,\n",
      "         0.6479,  0.2560,  0.9529,  0.9239, -0.3563, -0.9936, -1.0288,  0.1491,\n",
      "        -0.0175, -0.7099,  0.2046,  0.2677, -1.0292,  0.2746,  1.0162, -0.2734,\n",
      "         0.9371, -0.8144,  1.4576,  0.6393,  0.3648,  0.2767, -0.3059,  0.3309,\n",
      "        -0.0585, -1.2827,  0.6838, -2.1082,  1.4588,  0.3966, -0.1516, -1.5511,\n",
      "         0.6507,  0.0430, -0.6939, -2.0080, -0.4254, -1.5205, -1.3604,  0.2206,\n",
      "         0.3114,  1.0879, -0.9288,  1.1338,  0.5107,  0.3078,  0.0674,  0.4300,\n",
      "         0.4366, -1.9324, -1.2488, -1.7150, -1.9170,  0.1621,  1.3887,  1.0126,\n",
      "         0.3673,  0.3468, -1.2634,  1.4571,  1.4520, -1.8960, -0.4478, -1.9096,\n",
      "         0.3491,  1.0501,  1.4279,  1.4369,  2.1246,  0.8441, -0.6153,  0.2515,\n",
      "         0.0528,  0.6872, -0.8953, -0.3940,  0.3360,  0.0798, -0.1185,  1.2529,\n",
      "         1.1893, -1.3273,  0.0210, -1.0459,  0.1173,  1.3034,  0.4090,  0.2104,\n",
      "         0.5170, -2.1780,  0.4164,  0.8797,  1.4062, -1.0585,  0.4162, -0.5756,\n",
      "         1.4411, -1.0070,  1.4472, -0.0367,  0.9019,  0.2684, -0.8109,  0.3183,\n",
      "         0.5942,  0.0732,  1.4034,  1.2237,  0.6068, -2.1938, -0.2895,  0.2635,\n",
      "        -0.8652,  0.5778,  0.0958,  0.4432, -2.5688,  0.2889,  1.3833,  0.5285,\n",
      "         0.5629, -1.7031, -1.6413,  0.6709,  0.6903, -0.1711,  0.3781, -2.5626,\n",
      "         0.2653,  0.5880,  0.2029,  0.1613, -1.6676,  0.5075,  0.2707, -0.6351,\n",
      "        -1.8202, -0.0923,  0.3184,  0.3357,  1.3429,  1.0107,  0.8633,  1.0183,\n",
      "         0.2487,  1.4109,  0.8909, -1.9740,  0.0554,  0.3267,  0.4469,  0.7272,\n",
      "         0.3061, -1.6622, -1.8933,  0.9483,  0.0210, -0.2574, -0.0058, -0.1507,\n",
      "        -0.0555,  1.4014, -2.0457,  0.0964, -0.8766,  0.3128,  0.6232, -1.0978,\n",
      "         0.4204, -0.5836,  0.8138,  0.2426,  0.7577,  1.2975,  1.1793, -0.1955,\n",
      "         0.3647,  0.1214, -1.7296,  0.2090,  0.4191,  0.4797,  0.4609,  0.9064,\n",
      "         1.3730,  0.2214, -0.8192, -0.3217,  0.0101,  1.2189,  1.0031, -0.4124,\n",
      "         0.4493, -2.5823,  0.6765,  1.0031, -1.3414,  0.3165,  0.2227,  1.4594,\n",
      "         0.4946,  0.5002,  0.4010, -1.7073, -0.6851,  0.2271,  2.2720,  1.4594,\n",
      "         0.8585, -1.3916,  1.4519,  1.3178,  1.4014, -2.0682,  1.3691,  0.2313,\n",
      "         0.2841,  0.7815,  1.4486,  0.4186, -1.5529, -1.2606,  0.0395, -0.9324,\n",
      "        -1.6598,  0.0777, -0.1660, -0.3542,  1.3318, -1.8900,  0.6432,  0.7551,\n",
      "         0.2967, -1.3063,  0.0236,  0.2755, -0.1540,  0.3715, -2.1646,  1.2141,\n",
      "         0.0795,  0.4347,  2.3528,  0.8393, -2.5267,  0.0169, -1.9816,  0.0326,\n",
      "        -1.1390, -0.8623,  0.3233,  0.7342, -0.7902,  0.5825, -0.2817,  0.5850,\n",
      "         0.5351,  0.1218,  0.2440, -2.0950,  0.9868, -2.1436, -0.7265,  0.2750,\n",
      "        -2.0351,  0.7170, -0.0717, -0.4724,  1.3128,  0.4849,  0.1625,  0.2001,\n",
      "         0.7538,  0.3802,  0.4436,  0.3693, -0.5562,  0.2388,  0.3027,  0.9881,\n",
      "        -1.7819, -0.6816,  1.4386, -0.0713,  0.3316,  0.5394,  0.9250, -2.1735,\n",
      "         0.8630, -0.4146, -1.6532,  0.2696,  0.3772, -2.5683,  0.4516,  0.0434,\n",
      "         1.5599,  0.6338,  1.2931,  0.2324,  0.3032,  0.3748, -1.9069,  1.0559,\n",
      "        -1.9286, -0.1552, -1.1853,  1.4550, -0.7901, -0.0883, -1.4252,  0.6946,\n",
      "         0.7708, -0.0918,  0.0104, -0.8352, -0.1935,  0.2225, -0.9869,  0.1238,\n",
      "        -1.7157, -2.0313,  0.3276,  0.8523,  0.2223, -1.7109, -0.2433,  0.4844,\n",
      "        -1.9238,  0.5608,  1.2806, -0.9785,  0.5256, -1.2540, -0.2119,  0.4266,\n",
      "         0.0773, -2.4542,  0.3167, -2.1732,  0.6126, -0.5842,  1.4350,  0.0377,\n",
      "        -0.1912,  1.3041,  0.1935, -1.8480,  1.0836, -0.1939,  1.0373,  0.2502,\n",
      "         1.4096, -1.0768,  0.3001,  1.4488,  0.3362, -2.4585, -0.5501, -0.1803,\n",
      "         0.3212,  0.7812,  1.4377,  0.9308,  1.4573,  0.9345,  0.8381, -1.2367,\n",
      "        -1.2708, -1.4692,  0.3752,  0.0676,  1.4573, -1.5794,  0.7822, -1.8516],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xU1ZXo8d/qpiANUVoG1KGBgfE6klE7cu1RZ/BOHkqMUZCQSB44MU+SiWZivEEhMoJMHFFy0XgzTgYfucloElvFkojG+MyMfpSxSWMTX9GJJtIkEWLAKC3ddK/7R3X185xTp+qcU+dR6/v58JE+VX3OLqlatc/ea68tqooxxpj0qou7AcYYY4KxQG6MMSlngdwYY1LOArkxxqScBXJjjEm5MXFcdPLkyTpz5sw4Lm2MMam1devW3ao6ZeTxUAK5iHwF+CygwHbgU6r6ltvzZ86cSVtbWxiXNsaYmiEiv3I6HnhoRUSagH8AWlT1GKAe+GjQ8xpjjPEnrDHyMUCDiIwBxgM7QzqvMcaYEgIHclXtBL4B/Br4DbBXVX8y8nkislRE2kSkbdeuXUEva4wxpl8YQyuHAGcBs4CpwAQROWfk81R1g6q2qGrLlCmjxuqNMcZUKIyhlVOBl1R1l6r2ABuBvwnhvMYYY3wII2vl18BJIjIe6AJOAUJPScm3d7LuvufZuaeLqY0NLDvtKBbOaQr7MsYYkzqBA7mqbhGR24GfAQeAdmBD0PMOlW/vZMXG7XT19ALQuaeLFRu3A1gwN8bUvFCyVlR1larOVtVjVPXvVHV/GOctWnff8wNBvKirp5d19z0f5mWMMSaVUrFEf+eerrKOG2NMLYlliX65pjY20OkQtKc2NlTl+jY+b4xJslT0yJeddhQNufphxxpy9Sw77ajIr10cn+/c04UyOD6fb++M/NrGGONHKgL5wjlNXLHoWJoaGxCgqbGBKxYdW5VesY3PG2OSLhVDK1AI5nEMZ9j4vDEm6VLRI4+T2zh8tcbnjTGmFAvkJcQ5Pm+MMX6kZmgliCBZJ8XnWdaKMSapMh/Iw1gVGtf4vDHG+JH5oRXLOjHGZF3mA7llnRhjsi7zgdyyTowxWZf5QG5ZJ8aYrMv8ZKdlnRhjsi7zgRws68QYk22ZH1oxxpiss0BujDEpZ4HcGGNSLpRALiKNInK7iDwnIs+KyF+HcV5jjDGlhTXZ+U3gx6r6YREZC4wP6bzGGGNKCBzIReRg4G+BTwKoajfQHfS8xhhj/AmjR/7nwC7gOyLyTmAr8GVVfXPok0RkKbAUYMaMGSFcNnq2V6cxJg3CGCMfA/xP4F9VdQ7wJrB85JNUdYOqtqhqy5QpU0K4bLRsr05jTFqEEch3ADtUdUv/z7dTCOypZlUTjTFpETiQq+pvgVdEpFi85BTgmaDnjZtVTTTGpEVYWStfAm7pz1j5JfCpkM4bm6mNDXQ6BG2rmmiMSZpQ8shVdVv/+Hezqi5U1T+Ecd44WdVEY0xa1ETRrEpY1URjTFpYIPdgVRONMWlgtVaMMSblrEeeULYYyRjjlwXyBCouRirmsRcXIwEWzI0xo9jQSgLZYiRjTDkskCeQLUYyxpTDhlYSYuiYeJ0IvaqjnmOLkUzRkusf57H/fs3xsXNOmsHXFx5b5RaZOFkgT4B8eyfLbnuKnr5C8HYK4rYYyUDhvXLBrds8n3PzE7/mpV1vcMvnbH+XWmFDKwmwetPTA0F8KOn/09TYwBWLjrWJzho3b/0jJYN40WP//RpLrn884haZpLAeeQLs6epxPK7Ay2vPqG5jTCItuf5xXnj1zdJPHKIYzK1nnn3WIzcmBdzGw/38ntXQzz4L5AlwyPhcWcdNbQk6RGJpq9lnQysJsGr+0Sy7/Sl6egfHyXP1wqr5R9sKzxrXvOrHvL6/t/QTPTiVYzbZYoE8AdwqLQK2wrOGnXj5/b6C+JGHTuDQg8a5Dr/Ui4TdNJMwFsgTwqnS4ty1D7mu8LRAnm359k5+98fuks+be8SkgcnMmcs3Oz6nV5V8e6e9ZzLMxsgTzFZ41q7LfvR0yecMDeJQSFN1YxuHZ1togVxE6kWkXUTuDuuctc5tJaet8My2fHsnf9jnnJJaNDKIg/OuVkVWqyfbwuyRfxl4NsTz1Tzbbq42LbvNe9GPgGNu+MI5TVyxyH1pvk16ZlcogVxEpgFnADeEcT5TUPxgNjU22ArPGrEyv52ePu/nXP2R41wfs/dGbQprsvMa4CLgILcniMhSYCnAjBkzQrps9tl2c7Uj397JzU/82vM513zkOHs/mFEC98hF5EzgVVXd6vU8Vd2gqi2q2jJlypSglzUmU4qbiXg5ZHzOVxB3Sze0NMTsCmNoZS6wQEReBn4IvFdEbg7hvMbUjK9t7BiVajrSqvlH+zrXx06cXtZxk36Bh1ZUdQWwAkBE3g18VVXPCXpeM5yt8MyuJdc/zr4SA+Nzj5jk+9+7WIv8B1teoVeVehE+duJ0q1GeYbYgKAVsD8/syrd3liyIVclGEV9feKwF7hoSaiBX1UeAR8I8p/Hew9MCebqVqi8edHJzZX4739/ya4rl7htydVyxqNneNxljPfIUsBWe2TRv/SOej4sEu+Namd8+Kgumq6ePC/u/PCyYZ4ct0U8BW+GZTaU2ilhyYrA03R9secXxeB9W2jZrLJCnwHtmO6druh03yXfi5fd7Pj5uTF3gMW6nvV+L7G4uW2xoJQUefm5XWcdNss2+5B7e6nUPsgBXfqg58HXqRVyD+cQG27QkS6xHngI2Rp4dS65/vGQQh3DGr73yxm1tULZYIE8BGyPPBj+phlBINwyD19DMnhLVFU26WCCvUL69k7lrH2LW8s3MXftQpLWerQpiNpRKNYTKcsa9uNUot6GVbLFAXoHiAp3OPV0ogwt0ogrmVgUx/fxsoHzYQWNDX8Sz7LSjyNWNHkd5s/uAbTSRIaIeM9tRaWlp0ba2tqpfNyxz1z7kWNu5qbGBx5a/N4YWmSRbcv3jvoZUXl57RiTXn7PmJ44bVdj7dVBaSmCIyFZVbRl53LJWKmCTj8avEy+/39fem9d41BgPym083N6vBVkogWFDKxWwyUfjx7z1j/gK4ocdNDbSgGHvV29eJTDSwgJ5BWzy0ZSSb+8suXIT4OBx9Wy5ZF6kbbH3q7cs3GHb0EoFir2nNIypmerLt3f6ylBx2kA5CvZ+9Ta1scFxzitNdyzpmezsaIUH18DeHTBxGpxyKTQvjqaBxlTI78Tm2HrhF5d/oAotMqWMHCOHwh1L0MywKCZQ0z3Z2dEKP/oH6On/1tz7SuFnqE4wty8R44PfIA5w1YffGXFrRluZ326bTTiI4o6l2hOo6QjkD64ZDOJFPV2F41EH1Li/REwqrMxv9x3EzzlpRtWHNUaWtO1VHfjZgnn4m5xXew+BdEx27t1R3vGROlrh6mNgdWPhvx2t/q/t9SViDIUUw5F1v92EvXLTL7eStm7HTTDVnkBNR4984rRCT9jpeClBe9RBv0RClpaFC7XCb544RLNy0y+3KohepW5N5ao9gRq4Ry4i00XkYRF5VkSeFpEvh9GwYU65FHIj/gfkGgrHSwnao3b7svDzJRKyapcGMO6WXP84M5dv9h3Eq5Fm6KXepdyh23ETTLVTPsMYWjkA/G9VfQdwEnCeiPxlCOcd1LwY5l8LE6cDUvjv/Gur06MO8iXioZKiW1lYuJB2K/Pbmbl8s+/xcCikGXZc9v4IW1WaW0lbr1K3pnLVro8UeGhFVX8D/Kb/738UkWeBJuCZoOcepnlxZZOLQYZliteFULNWKp3RzsLChTQrZxilKK4x8ZGKbbCsleoJewLVS6iTnSIyE5gDbHF4bKmItIlI265dVdzZJkiPujhJunFp4edFG+ArPw+crVJpz9qWWsej3GGUorlHTEpUoPz6wmP5P4vfSVNjA32qPPzcLhuWy4jQJjtF5O3AHcAFqvr6yMdVdQOwAQoLgsK6bkmV9qgjTDustGe97LSjHBcu2FLraFTSAy9KSk98qCwUhzLOQgnkIpKjEMRvUdWNYZwzVJUMy0SYu17pjPbCOU20/eq1YbfHHzq+erdvtWJkznU5khjAi6qd22yqJ3AgFxEBbgSeVdX1wZuUEBGmHfrqWTusJs33zuWOrZ0DKWO9qtyxtZOWP5tkH8QQNK/6Ma/v7y39RAcCXP2R4xL97+B2x+fUqTDhizJ1OIwe+Vzg74DtIlKsFPQ1Vb0nhHPHJ+gkqQenJcHfO+xWjtj0cbirF6QOEND+oNI/rLNNP09XzwnDzmU9quD8FrlyU9VeeEcr5M+DPo8hn9wEmH/NqDtHtztBKPw/iOo9ZGsfoh/WSk/RrGobOUYOhUlSv2mP5bj7Qmi7seTTdvRN5uTua0cdF+CliHaXybLZl9zja0d7N0ceOoH7L3x3eA3y0tHaP+leRnvrx8FZ3xp4v3p9YTU25Ni26n0hNHS4fHsny257ip6+wXbn6oR1Z7+zpoJ5WLuKpbtoVhx8TpJW0tsY+Tv/uf87vtKHmup2s6DuUTb1nTzsuGWtlCfIGHjRNdUYRulohTs/D9pX2e/37i/8PkDzYhbOaXIN5Hu6nHcRCmr1pqeHBXGAnj5l9aanayqQR506bIHcS4lJ0kpul5x+R8b1FbrVJQhwZe4G6GEgmFvWin/lVCd0U60a4nx3Abz00+Dn0T7Y+DlovxnO3RT8fGVy+4KI6osjqaJesp+OolkJVUk+uNPv9Jbxz9Ag3Xxt7G1VWS2WJbPKXI050jknzeDltWdEG8QHirtNDCeID/XST+G7Cxifc36vuR034Yh6yb71yAOo5HbJ6bFbet/LJ+ofwG/Zi8PYxblv/y++u+eEgS8NC+bOgvbCqzaEctd50FtZzrpvL/2Ucbnz2dczeqhm3IggE5ZDxuf4g8Pmz4eMz0VyvaSKepcmC+QBVHK75PQ7qw58mrfnxvAh7i9kqkg9HP9JeOEnjpkzAlzUcx2v1XWzac/JtqjDQWoyUf7pcOitXvrffxw4h0vqPjNqnsUp2IZh1fyjWXb7U/QMmVTO1Qur5h8dyfWSLMol+xbIA6hkpaXb79QvWA8j/5GdMmf6jZduLhrTyqbuky0FcYQgQfywg8ZGX6XQZ5ZSSVIHH/y3wXkcH+PqB9XtZ33uumHzLBBdFUTbL7Q6LJBXqqOVhY+s4az6HfyufjJXdJ9N28HzSr5Jy3pjFz+gGz/neK4m2T3wdyucVVBpEE9VAG/5DJzpsPbu3E2+hmnGCFw+5kY2dQ8G8ijrklezeFStskBeiSE9ZQEOZxffnPAd+MAcaC6dE1rWG7t5Mdz5hcHFQUMMnSSt9RTEfHsnF966jUoS9SIdBw8r+2TybDh/VC260YqZViWu+3bZPyyVtanG3z9p35fXAnklqr2HqEMQB6jrD1u1noJY6YRmZOPgYfW8YXC+xKkH7uXcTXDZJNf3jggDQ3O1/v7Jwr68FsgrUe3t3yZOd5z03KmTaarxMcd56x/hhVffLOt3IhlGCTN4A8x6V/C87+M/6dmmJtkd+fsnFcvzo+qYVbGXb4G8EhHWYXF0yqWOk57TJvTx2Om7fQ3nZFElS+xDG0YJa8hkFCnUvQ/jA3/mevj9i67tFIHHjrkb5kTz/klN2dwoOmZV7uVbIK+EU2Atd/u3cr6ti8fvvRi6hgwhdL2WulvAsMxavrmcqiPB6qJEFrSHGDcRVgQrG+Do3E3edwttN5U/bONTasrmRtExq/Lwqy3nqkSQPURh8Nt67yuADn5bd7R6X3PshNHHy9lIOiNOvPz+aIL4N2YXVlWO/BNpEBdYdH00QbzIM1BrIdBHIDVbE0axL69rL/8VWN1YWMHr9Xkvk/XIK1XMDij2rDcuLfzXzzjYvRdX9m3t1GvwOp5BK/Pbfe/a41oXJezx7LLVw6JvV/cuSupdJz5puxFmnBR6e4qL3y4bcxNL6h+inj56qeOuuvcBCarWGcG+vK69fGBY523o9QOwQB5EJeNgHa3Dh0eGKjUm5/ZhlNq4sSonO+XIQycUgng1hkX8cqkTXhUlJj659+LQ27XstKN4884v83EZLD8xhj4W6Y8LX6YRDelUpNLN3d24zGsNE+JQS21EgKh4jYN5/Y6bUmNybj0q7Qv1Ni2J/AbxBXWP8ou3LeH+18+qwrCID3VjC0Mnq/fCJTvjm8s4c73z0FyRW+cigIVzmvhY/YOjaggJwNb/F/r1EmXk8KubkDLdrEceRCWz3V6PlRqTc0lDBKLLYU+AfHunaxC/bMxNfKL+gcED4qsicMRiGDrx48xrXFcJR6XOrZa6W6ckS4b28q8+JtJMt1B65CLyfhF5XkReFJHlYZwzFdz+Ebz+cdwea5hU+oPvFeijymFPgItuf2rYzwvqHuW/x32cl8Z9fKBq5MCfOBrY8plCj3vgz2vJC+Lg2Sal8IUZKq+7RImm2mJiRTGhOkQYmy/XA/8CzAN2AE+KyCZVfSbouROvkjREt985/crS12tePDoFsd9vmcwTEe67GJfZl9xDd3+u+L1jlzFbCsEmohpPpYWxUCdOUue441CfSvg53l7DiMd/MpxrpEUUE6pDhDG0cgLwoqr+EkBEfgicBWQ/kJf5j1NY5TaZljc/xYqxt3EYu5Fy/0FPv3LUF4Eq5LSLR++8DvhiZoL5zOWbC8F7zGBPsWoBPM6JySgd/6lRk56qsE/HMq/3p6y7b2x47x+vu8T+ic5UrPwMS9gTqkOEEcibgKGDPzuAE0c+SUSWAksBZsyYEcJlE8LnP06+vZNH77yOW/khU3O72dk3mWWcx8nv/iILm8t44/Zfa8/GC5mofxwYUvgTeYM1uoGrNo9h4ZzLKn01ifGPa/6RX467FiHq4B3iSso06A+gvU/eRB068P45SPazNncDK14HCGmlp+tCm+nAiM/EuN3s3DeZa+78KFnqjFRLGGPkTh+zUes1VHWDqraoasuUKVNCuGy6bNu8gcvlX5lWt5s6gWl1u7lc/pVtmzeUf7LmxbzRN25UgBsv3Xy2++ZwGhyX7y5AV09kTe+11EkEQbyYQTLwZ0/tBPGiM9ezSyY7vn9WjL0tvOuUGBfetnkDa2TDsM/EGtlQ2WeixoXRI98BTB/y8zRgZwjnzZQvdd/AuLrhM/XjpJcvdd8AlN+Dnlr3+7KOJ17/Ih2lv2cQRgBP+3h2hA5jd1nHK1Ji6PGz3Tczvm744q7Bzkj67yqrKYxA/iRwpIjMAjqBjwIfD+G8mTKp7o2yjpfSkzuYcT17Rx3vrXtbuhYHdLQWVsX238QFjt+Lrq+9HnYFxGXYQ8Iu/OYx9Ji5zkiMAn/mVfUAcD5wH/As0KqqTwc9r/E2boxz+laurys9i4M6WvvzmoPsTiPD0/8siPsTcTqcH281HF7WceMulAVBqnoPcE8Y54pNxLWD9zGOCewfdXwvB9FYwfm06w/uvdcIlluH6u4LYet3HNPgfLNhk2AiTofzY/zpazhw15cY0/vWwLED9W9j/Om1VQQuDLayE6KvHdzRSk57R40bHFBhVc/f8c0KTvk7JnM4u5wfjGC5dSgqLFZV3E5SBPf9Kk35IkyH89Uxal5cCEBDnjcmZVusJYUFcoi+dvCDaxgrB0Ydfp0JtB1c2U41V3SfzTW56+JbGFOuCopXqRYGXf6991QmfuhaS0lLi3I6RlF+mdQQC+QQ/dZtLvVRGnmz4r0S2w6exxv7buQgGT1cQ8Okis4ZmStmwP7RE7Ne+hQu6Pkim/pOJlcHL1gQT49q72lr0pXgEImOVvcysGHM4He04paL8db4wyvuZS477Sgu7fss+3X4pOd+refJdySk3E1Ha6ECYRlBXBUO9A0G8Tpg3dnHRddGE75q72lrajyQF28BnSqxhTWD/+AanLMyJNCkzsI5TTycezfLej7Pjr7J9Kmwo28yy3o+zwXPHFnxeUNz94VlVdpThV6F7/Weyv/o/j6b+k4GYH1Ye2ya6mk4xPl4VHvamhofWnG6BYRCZbZytm7z4toL0cDn39vVwyZOZlN3IegtqHuUi8a0MrXrOrh6etWzEAZ860TY/Zyvp6rCfsZwUc/SgeBd1JCrsyCeNh2t0O2wNqIuV9XUxlpT24HcLchqX3gBsES9iSCKW2lBIYivzd3AeOlfKRfxrt2ORizuKUUV/rPvaD7Rc4nj41csag6xcaYqHlwDvQ5b8Y07yMbHI1TbQyuV1BMvl8PCiy7G8eQRXwp86vfMHqxZc9GY1sEgXlTNjZk7WuHOL1DO4p7v9Z7qGsTnHjHJeuNp5LbxSdcfqtuOGlPbgdwhyO7Tsax+80PhFdlvXsyTx15Gpw6OY1/c/Rk+8eSfBb7Gw88N5pFPFZcaGdXYmPlbJxbGw/3u+lLfQP6sZ1h14NOuT3HcNNkkm8fEvo2PR6u2h1b6b/X23Xspb9v3W3bqn3DVgcVs2n8CDUGL7A9ZEDGdyfxzz+LhY8B9vay77/lAvc6dewbH93fqZKa5BfOO1uhua8sYDwdg8myWjLuGx27d5vqUc07KUJnjWuIxsV9yfDzildVZV9s9coDmxczT6/jz/bdwcve1A8G2q6cQaCtSzIbZ+wqgHM4u1uZuYEHdo8OeNjQQV2Jq4+DdxFUHFg+sgBzl3osDXcdVR2vZQXzltBs8N1E+56QZfH3hsSE0zlSdy5xTH0q+d6777434vAzM76SlZlACWCDHPaBWHGgdsmHGSzcXjRn+xhwaiCux7LSjaMgV8shHZnwME8WS/Y5WuOs8/89v+Qycv4Xvb/m161MELIgnQUdrYbPg1Y2F//oNqC7DJzv7JrNi43b3oUSvBUTGFwvkuAfUigOtS89kqgyW52zI1Ve8qrNo4Zwmrlh0LE2NDeHV8PajmCPulJ0wUrEy4Znrybd30ucxFxr0i82EoPgFPbR3fNd5/oK5y5zTVQcWe9/h2gKiwCyQM7xnWxQo0Lr0TF6VyQjQ1NjAFYuODSUrY+GcJh5b/l5eWnsG4rU0/+4LA1+Luy8srNT0W/hq1ruGFbhadpv7uDgQ+IvNhODei0d/Qfd2+xuea14M868dtkBtec9nB+4WXe9wq5E9lnG1PdnZrxhQQ9sE9pRLR22QTK6Bw+f/My81nxFCi12cfiXkvwh9PaMfa7sRfv9i5aVfyy16NXn2sGvNW/8IPR5Vay3dMCHchuFKDc8NmawcUzeZC7r/ftRwn+sdl8vnxRYQ+WeBvN/COU3hBZK4aj0Xz++2NP6ln1aWwdLRWl4QH1FqNt/eyQuvvun69IZcnaUbptmIaoeHs4srczdAz+DcjecdbgJqo6edqGuqQ3RaWlq0ra2t6tetGasnejwohQ2H/epohTs/73MTCOcd6eeufWhgBaqTa6yeSnJcOcu5990wCS5+yfl3rj7Gcb3Cb5nCX7/1zeB3uGaAiGxV1ZaRx61HnkVS77E4R+Ebs+GrPtIGvzEb3viN/2t+8NuOvSivIA4BcvVN+JyG5+pyheNuXCYlD2c3L62NcCjRDAg02Ski60TkORHpEJE7RaSSXctM2I7/pPfjb/zGOwuhOKnpN4jXjXUN4ivz2z1/NWfT7cnSvBgWXtdfC0gK/114nfcwh1u1Q7fjJnRBe+T3AytU9YCIXAmsACJafZJe+fbO8CZS/ThzPfzs36HPIz3Qqch/Ryts/ALgc6k9FCY1z9/i+vDNT7jnjYPVGk8k27UndQIFclX9yZAfnwA+HKw51Rd1kM23d7Ji43a6egrBsXNPFyuCLv/3Y+G/eNcD3/tKodftOQxTQokgXoqNjaeAn6XzbgWxrFBW1YR5Y/tp4F63B0VkqYi0iUjbrl0umwZXWTHIdu7pQhkMsqEVzKKQ0lgM4kWBlv/71bwY3v6npZ9XURAXWHR9ySC+5PrHPR+3IJ5wfpfOWx547EoGchF5QER+7vDnrCHPuQQ4ANzidh5V3aCqLaraMmXKFLenVVU1gmzoy//L8dXn/AXzcjlkpoyUb+/0rKliUsDv0nmHFZ2WB15dJYdWVPVUr8dF5FzgTOAUjSOXMYBqBNmhmz+MPF4VX31u+O1xGfXCHS263tf46dc2dng+PveIhG0QbUbzu3Te8sBjF2iMXETeT2Fy812qui+cJlVPNYLsstOOGjZGDuHUWSnL0Mkrl5zfksocD9/ntYwTqzeeCm67W0nd6IVlSZ4grYESuUHHyL8FHATcLyLbROTbIbSpakKvseJgZGGrMOusVMTpNthL3Vhf4+FDlZpjsHrjKeH2XtHewkT66omFP5dPHTZunm/vZO7ah5i1fDNz1z4U6pxT2WqkRG7Nr+ysempghHy/loEeyivuWSsTK9+8+aiV97L/gHuP/GVbJJIexS38Sk2K9y8Iy/fOdbwDja3z4nYHOnE6fOXn1W9PQLay00WoNVZiVFaaY4S3wfn2Ts8gbr3xlGle3L+hdgnaCw+uYd3+a10TCGL5nNVIiVxbV5cRsaU5jlCqVK1tHJFCftMI9+6IN0vLSY2kRlogz4gkfIDy7Z2epWqrte+FCZnfeZWJ08LfpCWoGkmNtECeEUn4AF32o6c9H19iwyrp1L9hBOIRLqQeTrm0KgkEZSm2fWjtmPnXZi5rpebHyLMiCWmOf9jnsKHFEDaskmLFwHfXeaN3EKofC2f9CzQvZmH/oUQlECQ5NTIkFsgzIvRdjspkKYc1wMfCnyxlgaVJzacfmnB4bR4xtl74xeUfqHKLTLWNzJyCmFMPM8gt/dDGyE0ovDaPuOrD76xiS0xckpI5VYtsaCVj4ri1nbf+EdfHDhmfs95YjUhC5lStsh55hlSjLO9IK/PbPTdWXjX/6MiubZIlCZlTtcoCeYbEcWtbagcg643XjsSlHtYQG1qJSBxDHHZra+IUd+ZULbNAHoG4tneLvfb5CONtZ+WaU2ntIktbDMY+aRGIa/b+PbOdd15yOx61f17UHMt1TbrEMbeTNRbIIxDXEMfDzznvhep2PAxj650rqOTqbHzc+GNpi8FZII9AXLP31f4CWZnfTnfv6AVlAqw7+7hIrmmyx+Z2grNAHoG4Zu+r+QWSb+/kFpeMlYkNljtu/LO0xeBCCeQi8lURURGZHMb50i6u7URbHa0AAAnkSURBVN2q+QWy7r7nXbdx3tvlXTzLJFu1t2qztMXgAmetiMh0YB7gnVBcY+LYeaia6V9et73Wk0qvODKuLG0xuDDSD68GLgLuCuFcJqBqfYE0js85lq0VsJ5UinlNPEb5vsrKlotxCTS0IiILgE5VfcrHc5eKSJuItO3aFV0WhYneyvx219rjS06aYR/IFLOJx3Qq2SMXkQeAwx0eugT4GvA+PxdS1Q3ABiiUsS2jjSZB8u2drsvyGxtytnlEyrndaTWOz8XQGuNXyUCuqqc6HReRY4FZwFMiAjAN+JmInKCqvw21lSYxVm9y387NJjnTz217ghi2LTBlqHiMXFW3A4cWfxaRl4EWVd0dQrtMQu3xCNY2yZl+bl/G9iWdbFZrJaOiqF2xMr/d83Gb5Ey/pNXrMf6EtiBIVWdabzwZ8u2dLLvtqWG1K5bd9lSgfOCV+e2eJWsnjK23Sc4MsJzudLIeeQat3vQ0PX3DBzV7+pQVGzsqDrY/2PKK5+OXf9AmObOg0pxuq14YLwvkGeQ2jt3V00e+vbOiD1hvidku+9BmR7k53XGVbTaDrNZKjamkolypIZkmGz+taVa9MH4WyDPoEI+cX6/d7t2U+kDa+Glts0VE8bNAnkFeGx7Xi3P9cC+lgr/dPtc2q14YPwvkGeQVWEuNdTvxCv7lfy2YrLFMl/hZIM8or3HrctIQ8+2dnsF/yUkzymqXyZ64yjabQaIxrL1taWnRtra2ql+3luTbO7ng1m2OjzU25Ni2qnSJnJHZCCONz9XxzD+dHqidxhj/RGSrqraMPG498ozy6g15LbMf6rIfPe0axBty9ba5shlQ7c0ozHCWR24c5ds7XUvVAnbrbAZYHnn8rEdeo0r1mC663b3EfL2IfUDNAMsjj58F8hpV6kPW3es+d1JJ5ovJLssjj58F8gzzShv0+pCVqnJoKznNUJZHHj8L5Bn2sROnuz7m9SHzqnIItpLTDGd55PGzQJ5hX194LHOPmDTquNeHrFRv/MhDJ9j4uBnG8sjjZ3nkNaCcEqOzlm/G6x3x8tozommkMaYktzxySz+sAX7Lkq7Mb/cM4g05u4EzJonsk2mAQq+91Nj4FbYAyJhECtwjF5EvAecDB4DNqnpR4FaZqrvsR097Pp6rs8Udtch2/kmHQIFcRN4DnAU0q+p+ETk0nGaZKI38cI4fW+e5ihNg3dnHVal1JilsxWZ6BB1a+XtgraruB1DVV4M3yUSp+OEcujHzC6++6fk7trFybbIVm+kRNJD/BfC/RGSLiPxURP7K7YkislRE2kSkbdeuXQEvayrl9OEsxTZWrk22YjM9Sg6tiMgDwOEOD13S//uHACcBfwW0isifq0NOo6puADZAIf0wSKNN5cr9EJ5z0gzrjdeoqY0NjrtD2YrN5CkZyFX1VLfHROTvgY39gfu/RKQPmAxYlzuh3D6cTq75yHEWxGvYstOOGlWP3lZsJlPQoZU88F4AEfkLYCywO2ijTHScllM7mXvEJAviNc5WbKZH0PTDm4CbROTnQDdwrtOwikmO4ofwsh897ZqpMveISdzyub+uZrNMQvldTGbiFSiQq2o3cE5IbTFVUvxwWo6wMdlgS/RrmPW2jMkGW6JvjDEpZ4HcGGNSzgK5McaknI2RG2NKSvPEeJrb7pcFcmOMpzQXz0pz28thQyvGGE9pLp6V5raXwwK5McaTW32ezj1dzFq+mblrHyLf3lnlVvlTK4W/LJAbYzx5FckqlkJesXF7IoO5W9uzVvjLArkxxpOf+jxJHa5wansWC3/ZZKcxxlNxUvArrdvwqqSUxOGKYtsta8UYU/MWzmniglu3eT4nqcMVtVCKwoZWjDGhyNpwRZpYIDfG+HLI+JzrY1a/Pl4WyI0xvqyafzS5ehl13OrXx8/GyI0xvtTKxGEaWSA3xvhWCxOHaRRoaEVEjhORJ0Rkm4i0icgJYTXMGGOMP0HHyK8CLlPV44BL+382xhhTRUEDuQIH9/99IrAz4PmMMcaUKegY+QXAfSLyDQpfCn/j9kQRWQosBZgxY0bAyxpjjCkqGchF5AHgcIeHLgFOAb6iqneIyGLgRuBUp/Oo6gZgA0BLS4vHQl9jjDHlEPUqnlDql0X2Ao2qqiIiwF5VPdjH7+0CfuXxlMnA7ooblhz2OpIjC68B7HUkTbVfx5+p6pSRB4MOrewE3gU8ArwXeMHPLzk1ZCgRaVPVloBti529juTIwmsAex1Jk5TXETSQfw74poiMAd6ifwzcGGNM9QQK5Kr6KHB8SG0xxhhTgaTWWtkQdwNCYq8jObLwGsBeR9Ik4nUEmuw0xhgTv6T2yI0xxvhkgdwYY1IusYFcRP5JRDr6C3L9RESmxt2mconIOhF5rv913CkijXG3qRIicraIPC0ifSISe6pVuUTk/SLyvIi8KCLL425PJUTkJhF5VUR+HndbghCR6SLysIg82/+e+nLcbaqEiLxNRP5LRJ7qfx2XxdqepI6Ri8jBqvp6/9//AfhLVf1CzM0qi4i8D3hIVQ+IyJUAqnpxzM0qm4i8A+gD/g34qqq2xdwk30SkHvgFMA/YATwJfExVn4m1YWUSkb8F3gC+p6rHxN2eSonInwJ/qqo/E5GDgK3AwhT+ewgwQVXfEJEc8CjwZVV9Io72JLZHXgzi/SZQKNCVKqr6E1U90P/jE8C0ONtTKVV9VlWfj7sdFToBeFFVf6mq3cAPgbNiblPZVPU/gNfibkdQqvobVf1Z/9//CDwLpK7AuRa80f9jrv9PbDEqsYEcQEQuF5FXgCUUyuSm2aeBe+NuRA1qAl4Z8vMOUhg4skhEZgJzgC3xtqQyIlIvItuAV4H7VTW21xFrIBeRB0Tk5w5/zgJQ1UtUdTpwC3B+nG11U+o19D/nEuAAhdeRSH5eR0qN3mQyhXd3WSMibwfuAC4YcfedGqra278XwzTgBBGJbcgr1q3eVNWxUqKD7wObgVURNqcipV6DiJwLnAmcokmdkKCsf4u02QFMH/LzNKxufqz6x5TvAG5R1Y1xtycoVd0jIo8A7wdimYxO7NCKiBw55McFwHNxtaVSIvJ+4GJggarui7s9NepJ4EgRmSUiY4GPAptiblPN6p8kvBF4VlXXx92eSonIlGIWmog0UCjfHVuMSnLWyh3AURSyJX4FfEFVO+NtVXlE5EVgHPD7/kNPpC3zBkBEPgj8X2AKsAfYpqqnxdsq/0TkA8A1QD1wk6peHnOTyiYiPwDeTaFs6u+AVap6Y6yNqoCInAz8J7Cdwmcb4Guqek98rSqfiDQD36XwnqoDWlV1TWztSWogN8YY409ih1aMMcb4Y4HcGGNSzgK5McaknAVyY4xJOQvkxhiTchbIjTEm5SyQG2NMyv1/Nv5y6qgjxFcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight[0] shape:  torch.Size([5, 400])\n",
      "bias[0] shape:  torch.Size([5, 1])\n",
      "Relu Params[0]:  0.01\n",
      "BN gamma[0]:  1.0919343224478912\n",
      "BN beta[0]:  0.10000000149011642\n",
      "weight[1] shape:  torch.Size([5, 5])\n",
      "bias[1] shape:  torch.Size([5, 1])\n",
      "Relu Params[1]:  0.01\n",
      "BN gamma[1]:  1.0002533830882228\n",
      "BN beta[1]:  0.10000000149011642\n",
      "weight[2] shape:  torch.Size([400, 5])\n",
      "bias[2] shape:  torch.Size([400, 1])\n",
      "Relu Params[2]:  0.01\n",
      "BN gamma[2]:  1.0352160314177057\n",
      "BN beta[2]:  0.00872622385499952\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    layers = (5, 5, 400)\n",
    "    aFuncs = ('tanh', 'cos', 'linear')\n",
    "    batchSize = 10\n",
    "    nn1 = DNN(layers, aFuncs)\n",
    "    nn1.setBackProp(PRelu=True, Norm=True)\n",
    "    # 所有input全部变型成 m x 1 的形状\n",
    "    testInput = torch.randn((20, 20), dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "    testInput = testInput.view(-1, 1)\n",
    "    targetY = 5*torch.sin(testInput**3) + 2*torch.cos(2*testInput-1) - torch.tanh(testInput)\n",
    "    # 生成权重和偏移\n",
    "    nn1.genParam(testInput)\n",
    "    nn1.printShape()\n",
    "    # 训练\n",
    "    nn1.train(testInput, targetY, nanInvestigate=0, epoch=1000)\n",
    "    # 预测\n",
    "    estimateY = nn1.predict(testInput)\n",
    "    \n",
    "    plt.scatter(testInput.cpu().numpy().flatten(), targetY.cpu().numpy().flatten())\n",
    "    plt.scatter(testInput.cpu().numpy().flatten(), estimateY.cpu().numpy().flatten())\n",
    "    plt.show()\n",
    "    nn1.printShape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
