{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np, matplotlib.pyplot as plt, multiprocessing as mp\n",
    "from numpy import random\n",
    "import torch, cv2, time, random, os, threading, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = {\n",
    "    'boolean': torch.bool, 'ui8': torch.uint8, 'i8': torch.int8, 'i16': torch.int16, 'i32': torch.int32, 'i64': torch.int64, \n",
    "    'f16': torch.float16, 'f32': torch.float32, 'f64': torch.float64, 'f64Complex': torch.complex64, 'f128Complex': torch.complex128\n",
    "}\n",
    "DEVICE = {\n",
    "    'auto': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), \n",
    "    'cpu': torch.device('cpu'), \n",
    "    'cuda0': torch.device('cuda:0')\n",
    "}\n",
    "\n",
    "DEVICE_CHOICE = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch(x, area, startPoint):\n",
    "    rowImg, columnImg, depthImg = x.shape\n",
    "    rowVision, columnVision = area\n",
    "    startX, startY = startPoint\n",
    "    totalCanvas = np.zeros((rowImg+rowVision*2, columnImg+columnVision*2, depthImg), dtype=np.int)\n",
    "    totalCanvas[rowVision:rowVision+rowImg,columnVision:columnVision+columnImg] = x\n",
    "    seeing = totalCanvas[startX:startX+rowVision+1, startY:startY+columnVision+1]\n",
    "    return seeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNoise(x, volumn = 50):\n",
    "    row, column, depth = x.shape\n",
    "    noise = np.random.randint(low=-volumn, high=volumn, size=(row, column, depth))\n",
    "    imgWithNoise = x + noise\n",
    "    imgWithNoise[imgWithNoise > 255] = 255\n",
    "    imgWithNoise[imgWithNoise < 0] = 0\n",
    "    return imgWithNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takePhoto():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # 初始化摄像头，摄像头适应光源\n",
    "#     for i in range(20):\n",
    "#         cap.read()\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('camera not working')\n",
    "    cap.release()\n",
    "    print('Image shape: ', frame.shape)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, layers=(10, 20, 10), aFunc=('cos', 'PRelu', 'PRelu', 'sin')):\n",
    "        self.layerNum = len(layers)\n",
    "        assert self.layerNum >= 1\n",
    "        torch.manual_seed(0)\n",
    "        self.layerShapes = layers\n",
    "        self.aFuncChosen = aFunc\n",
    "        self.defaultLr = 1e-9\n",
    "        self.lr = {\n",
    "            'weight': self.defaultLr,\n",
    "            'bias': self.defaultLr,\n",
    "            'relu param': self.defaultLr, \n",
    "            'BN gamma': self.defaultLr, \n",
    "            'BN beta': self.defaultLr\n",
    "        }\n",
    "        self.inputs = None\n",
    "        self.targetY = None\n",
    "        self.weights = [None] * self.layerNum\n",
    "        self.biases = [None] * self.layerNum\n",
    "        self.reluParam = [None] * self.layerNum\n",
    "        self.layers = {\n",
    "            'Z': [None] * self.layerNum, \n",
    "            'N': [None] * self.layerNum, \n",
    "            'A': [None] * self.layerNum\n",
    "        }\n",
    "        self.BN = {\n",
    "            'epsilon': 1e-5, \n",
    "            'gamma': [1] * self.layerNum, \n",
    "            'beta': [0] * self.layerNum, \n",
    "            'cache': [None] * self.layerNum\n",
    "        }\n",
    "        self.activFunc = {\n",
    "            'PRelu': lambda x, i: torch.max(x, x * self.reluParam[i]), \n",
    "            'sigmoid': lambda x: 1/(1+torch.exp(-x)), \n",
    "            'softmax': lambda x: torch.exp(x - torch.max(x)) / torch.sum(torch.exp(x - torch.max(x))), \n",
    "            'tanh': lambda x: torch.tanh(x), \n",
    "            'sin': lambda x: torch.sin(x), \n",
    "            'cos': lambda x: torch.cos(x), \n",
    "            'linear': lambda x: x\n",
    "        }\n",
    "        self.activFuncDer = {\n",
    "            'PRelu': self._PReluDer,  \n",
    "            'sigmoid': lambda x: self.activFunc['sigmoid'](x) * (1 - self.activFunc['sigmoid'](x)), \n",
    "            'softmax': lambda x, a: self.activFunc['softmax'](x) * (a - self.activFunc['softmax'](x)), \n",
    "            'tanh': lambda x: 1 - torch.tanh(x) ** 2, \n",
    "            'sin': lambda x: torch.cos(x), \n",
    "            'cos': lambda x: -torch.sin(x), \n",
    "            'linear': lambda x: 1\n",
    "        }\n",
    "        self.lossFunc = {\n",
    "            'mse': lambda predictY, targetY: (targetY - predictY) ** 2, \n",
    "            'bce': lambda predictY, targetY: targetY * torch.log(predictY) + (1 - targetY) * torch.log(1 - predictY)\n",
    "        }\n",
    "        self.lossFuncDer = {\n",
    "            'mse': lambda predictY, targetY: 2 * (targetY - predictY), \n",
    "            'bce': lambda predictY, targetY: Y / predictY + (targetY - 1) / (1 - predictY)\n",
    "        }\n",
    "    \n",
    "    def _dA_dReluP(self, x, i):\n",
    "        data = x.clone()\n",
    "        data[data > 0] = 0\n",
    "        data[data <= 0] = torch.mean(data[data <= 0])\n",
    "        return data\n",
    "    \n",
    "    def _PReluDer(self, x, i): \n",
    "        data = x.clone()\n",
    "        data[data > 0] = 1\n",
    "        data[data <= 0] = self.reluParam[i]\n",
    "        return data\n",
    "    \n",
    "    def batchNorm(self, x, layerIter):\n",
    "        mean = torch.mean(x)\n",
    "        variance = torch.mean((x - mean) ** 2)\n",
    "        # normalize\n",
    "        fenzi = (x - mean) * 1.0\n",
    "        fenmu = torch.sqrt(variance + self.BN['epsilon'])\n",
    "        # * 1.0 是转换成float\n",
    "        xNorm = fenzi / fenmu\n",
    "        cache = {\n",
    "            'BNmean': mean, \n",
    "            'BNvariance': variance, \n",
    "            'BNfenzi': fenzi, \n",
    "            'BNfenmu': fenmu\n",
    "        }\n",
    "        self.BN['cache'][layerIter] = cache\n",
    "        return self.BN['gamma'][layerIter] * xNorm + self.BN['beta'][layerIter]\n",
    "    \n",
    "    # 生成 w、b, 以x的形状是 m x 1\n",
    "    def genParam(self, x):\n",
    "        self.inputs = x\n",
    "        column = x.shape[0]\n",
    "        for i, r in enumerate(self.layerShapes):\n",
    "#             print('row: ', row, '\\ncolumn: ', column)\n",
    "            self.weights[i] = torch.ones(r, column, dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "            self.biases[i] = torch.zeros(r, 1, dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "            self.reluParam[i] = 0.01\n",
    "            column = r\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "#         print('inputs.shape: ', inputs.shape)\n",
    "    #     layer['non linear'][0] = layer['linear'][0] = inputs\n",
    "        for i in range(self.layerNum):\n",
    "#             print(f'w[{i}].shape: ', self.weights[i].shape)\n",
    "#             print(f'b[{i}].shape: ', self.biases[i].shape)\n",
    "            self.layers['Z'][i] = self.weights[i] @ inputs + self.biases[i]\n",
    "            self.layers['N'][i] = self.batchNorm(self.layers['Z'][i], i)\n",
    "#             self.layers['N'][i] = self.batchNorm(self.layers['Z'][i], i)\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                self.layers['A'][i] = self.activFunc[self.aFuncChosen[i]](self.layers['N'][i], i)\n",
    "            else:\n",
    "                self.layers['A'][i] = self.activFunc[self.aFuncChosen[i]](self.layers['N'][i])\n",
    "            inputs = self.layers['A'][i]\n",
    "#             print(f'layer[{i}].shape: ', self.layers['A'][i].shape)\n",
    "\n",
    "    # 预测\n",
    "    def predict(self, x):\n",
    "        self.forward(x)\n",
    "        predictY = self.layers['A'][-1]\n",
    "        print('output: ', predictY.squeeze())\n",
    "        return predictY\n",
    "    \n",
    "    # 反向传播函数\n",
    "    # input = x, Z = W @ input + b, N = batchNormalize(Z), Y_preditc = activateFunc(N), L = lossFunc(Y_predict)\n",
    "    # 根据链式法则 dL / dW = (dL / dY_predict) * (dY_predict / dN) * (dN / dZ) * (dZ / dW)\n",
    "    # dL / dY_predict = lossFunc_Der, dY_predict / dN = activateFunc_Der, dN/dZ = gamma/sqrt(variance+epsilon), dZ/dW = input\n",
    "    # ==> dL / dW = lossFunc_Der(Y_predict) * activateFunc_Der(Z) * gamma/sqrt(variance+epsilon) * input\n",
    "    # 同理可证 dL / db = lossFunc_Der(Y_predict) * activateFunc_Der(Z) * gamma/sqrt(variance+epsilon) * 1\n",
    "    def backprop(self):\n",
    "        '''\n",
    "        尚未完成\n",
    "        '''\n",
    "        dW = [None] * self.layerNum\n",
    "        dB = [None] * self.layerNum\n",
    "        dReluP = [None] * self.layerNum\n",
    "        dGamma = [None] * self.layerNum\n",
    "        dBeta = [None] * self.layerNum\n",
    "        \n",
    "        dL_Div_dYtrain = self.lossFuncDer['mse'](self.layers['A'][-1], self.targetY)\n",
    "        dActivation = [None] * self.layerNum\n",
    "        for i in reversed(range(self.layerNum)):\n",
    "#             print('i: ', i)\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                dActivation[i] = self.activFuncDer[self.aFuncChosen[i]](self.layers['Z'][i], i)\n",
    "            else:\n",
    "                dActivation[i] = self.activFuncDer[self.aFuncChosen[i]](self.layers['Z'][i])\n",
    "#             print(f'weight[{i}] shape: {self.weights[i].shape} \\nbias[{i}] shape: {self.biases[i].shape} \\n')\n",
    "            if i == self.layerNum - 1:\n",
    "                dB[i] = dL_Div_dYtrain * dActivation[i] * (self.BN['gamma'][i] / self.BN['cache'][i]['BNfenmu'])\n",
    "                dW[i] = dB[i] @ torch.transpose(self.layers['A'][i-1], 0, 1)\n",
    "                dReluP[i] = torch.mean(dL_Div_dYtrain * self.layers['N'][i]).item()\n",
    "                dBeta[i] = dL_Div_dYtrain * dActivation[i]\n",
    "                dGamma[i] = dBeta[i] * (self.BN['cache'][i]['BNfenzi'] / self.BN['cache'][i]['BNfenmu'])\n",
    "            else:\n",
    "                dB[i] = (torch.transpose(self.weights[i+1], 0, 1) @ dB[i+1]) * dActivation[i]\n",
    "                dW[i] = dB[i] @ torch.transpose(self.layers['A'][i-1], 0, 1)\n",
    "                dReluP[i] = torch.mean(torch.transpose(self.weights[i], 0, 1) @ dB[i] * self.layers['N'][i-1]).item()\n",
    "                dBeta[i] = torch.transpose(self.weights[i+1], 0, 1) @ (dBeta[i+1] * (self.BN['gamma'][i+1] / self.BN['cache'][i+1]['BNfenmu']))\n",
    "                dGamma[i] = dBeta[i] * (self.BN['cache'][i]['BNfenzi'] / self.BN['cache'][i]['BNfenmu'])\n",
    "#             print(f'dReluP[{i}]: {dReluP[i]}')\n",
    "            self.weights[i] += dW[i] * self.lr['weight']\n",
    "            self.biases[i] += dB[i] * self.lr['bias']\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                self.reluParam[i] += dReluP[i] * self.lr['relu param']\n",
    "            self.BN['beta'][i] += torch.mean(dBeta[i]).item() * self.lr['BN gamma']\n",
    "            self.BN['gamma'][i] += torch.mean(dGamma[i]).item() * self.lr['BN beta']\n",
    "#             print(self.reluParam)\n",
    "            \n",
    "    def train(self, inputs, targetY, nanInvestigate=40, epoch = 1000):\n",
    "        self.targetY = targetY\n",
    "        for e in range(epoch):\n",
    "            self.forward(self.inputs)\n",
    "            if epoch % 100 == 0 and epoch != 0:\n",
    "                loss = torch.norm(self.layers['A'][-1] - self.targetY)\n",
    "                print(e, f': loss = {loss}')\n",
    "                if loss < 5.0:\n",
    "                    for k,v in self.lr.items():\n",
    "                        self.lr[k] = 0.0005\n",
    "                if loss < 2.0:\n",
    "                    for k,v in self.lr.items():\n",
    "                        self.lr[k] = 0.0001\n",
    "                if loss < 1.0 or torch.isnan(loss):\n",
    "                    return\n",
    "#                 print(f'{i} Loss: ', torch.mean(self.lossFunc['mse'](self.layers['A'][-1], self.targetY)).item())\n",
    "#                 print(epoch/100, ': \\n', torch.transpose(self.layers['A'][-1], 0, 1))\n",
    "            self.backprop()\n",
    "            if e > nanInvestigate and nanInvestigate > 0:\n",
    "                self.saveParams('d:\\\\nanInvest_'+str(e)+'.pt', True)\n",
    "                \n",
    "    def printShape(self):\n",
    "        for i in range(self.layerNum):\n",
    "            print(f'weight[{i}] shape: ', self.weights[i].shape)\n",
    "            print(f'bias[{i}] shape: ', self.biases[i].shape)\n",
    "            print(f'Relu Params[{i}]: ', self.reluParam[i])\n",
    "            print(f'BN gamma[{i}]: ', self.BN['gamma'][i])\n",
    "            print(f'BN beta[{i}]: ', self.BN['beta'][i])\n",
    "#             print(f'Z layer shape: ', self.layers['Z'][i].shape)\n",
    "#             print(f'N layer shape: ', self.layers['N'][i].shape)\n",
    "#             print(f'A layer shape: ', self.layers['A'][i].shape)\n",
    "    \n",
    "    def saveParams(self, PATH, layers=False):\n",
    "        params = {\n",
    "            'reluParam': self.reluParam, \n",
    "            'weight': self.weights, \n",
    "            'bias': self.biases, \n",
    "            'BN':self.BN\n",
    "        }\n",
    "        if layers:\n",
    "            params['layers'] = self.layers\n",
    "        torch.save(params, PATH)\n",
    "        \n",
    "    def readParams(self, PATH, layers=False):\n",
    "        params = torch.load(PATH)\n",
    "        self.reluParam = params['reluParam']\n",
    "        self.weights = params['weight']\n",
    "        self.biases = params['bias']\n",
    "        self.BN = params['BN']\n",
    "        if layers:\n",
    "            self.layers = params['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight[0] shape:  torch.Size([20, 400])\n",
      "bias[0] shape:  torch.Size([20, 1])\n",
      "Relu Params[0]:  0.01\n",
      "BN gamma[0]:  1\n",
      "BN beta[0]:  0\n",
      "weight[1] shape:  torch.Size([10, 20])\n",
      "bias[1] shape:  torch.Size([10, 1])\n",
      "Relu Params[1]:  0.01\n",
      "BN gamma[1]:  1\n",
      "BN beta[1]:  0\n",
      "weight[2] shape:  torch.Size([5, 10])\n",
      "bias[2] shape:  torch.Size([5, 1])\n",
      "Relu Params[2]:  0.01\n",
      "BN gamma[2]:  1\n",
      "BN beta[2]:  0\n",
      "weight[3] shape:  torch.Size([400, 5])\n",
      "bias[3] shape:  torch.Size([400, 1])\n",
      "Relu Params[3]:  0.01\n",
      "BN gamma[3]:  1\n",
      "BN beta[3]:  0\n",
      "0 : loss = 19.999989744074792\n",
      "1 : loss = 19.990340400243337\n",
      "2 : loss = 19.980980207954936\n",
      "3 : loss = 19.97163350800858\n",
      "4 : loss = 19.962300317646257\n",
      "5 : loss = 19.952980678540612\n",
      "6 : loss = 19.94367463200126\n",
      "7 : loss = 19.93438221896556\n",
      "8 : loss = 19.92510347999376\n",
      "9 : loss = 19.915838455296853\n",
      "10 : loss = 19.90658718470149\n",
      "11 : loss = 19.8973497076686\n",
      "12 : loss = 19.888126063290702\n",
      "13 : loss = 19.878916290293745\n",
      "14 : loss = 19.869720427021342\n",
      "15 : loss = 19.86053851146005\n",
      "16 : loss = 19.851370581219065\n",
      "17 : loss = 19.842216673537003\n",
      "18 : loss = 19.833076825284643\n",
      "19 : loss = 19.823951072956604\n",
      "20 : loss = 19.814839452680825\n",
      "21 : loss = 19.80574200021205\n",
      "22 : loss = 19.79665875093276\n",
      "23 : loss = 19.78758973985717\n",
      "24 : loss = 19.778535001621744\n",
      "25 : loss = 19.769494570500967\n",
      "26 : loss = 19.7604684803864\n",
      "27 : loss = 19.75145676480904\n",
      "28 : loss = 19.742459456923434\n",
      "29 : loss = 19.73347658951198\n",
      "30 : loss = 19.72450819498671\n",
      "31 : loss = 19.715554305392033\n",
      "32 : loss = 19.70661495239558\n",
      "33 : loss = 19.697690167298187\n",
      "34 : loss = 19.688779981029427\n",
      "35 : loss = 19.67988442415169\n",
      "36 : loss = 19.67100352685229\n",
      "37 : loss = 19.66213731894933\n",
      "38 : loss = 19.653285829895886\n",
      "39 : loss = 19.644449088771328\n",
      "40 : loss = 19.635627124288284\n",
      "41 : loss = 19.626819964792148\n",
      "42 : loss = 19.61802763825247\n",
      "43 : loss = 19.609250172285083\n",
      "44 : loss = 19.600487594124033\n",
      "45 : loss = 19.591739930647186\n",
      "46 : loss = 19.583007208358488\n",
      "47 : loss = 19.5742894533983\n",
      "48 : loss = 19.56558669154219\n",
      "49 : loss = 19.556898948202\n",
      "50 : loss = 19.54822624842207\n",
      "51 : loss = 19.539568616880903\n",
      "52 : loss = 19.530926077901317\n",
      "53 : loss = 19.522298655432756\n",
      "54 : loss = 19.51368637307413\n",
      "55 : loss = 19.505089254049665\n",
      "56 : loss = 19.496507321232517\n",
      "57 : loss = 19.48794059712919\n",
      "58 : loss = 19.479389103890288\n",
      "59 : loss = 19.47085286330567\n",
      "60 : loss = 19.462331896808163\n",
      "61 : loss = 19.453826225464383\n",
      "62 : loss = 19.445335869999006\n",
      "63 : loss = 19.436860850767697\n",
      "64 : loss = 19.428401187777123\n",
      "65 : loss = 19.419956900676535\n",
      "66 : loss = 19.411528018868257\n",
      "67 : loss = 19.4031145548095\n",
      "68 : loss = 19.39471652350651\n",
      "69 : loss = 19.386333943190984\n",
      "70 : loss = 19.377966831738263\n",
      "71 : loss = 19.36961520668326\n",
      "72 : loss = 19.361279085200955\n",
      "73 : loss = 19.352958484123047\n",
      "74 : loss = 19.34465341994285\n",
      "75 : loss = 19.336363908792062\n",
      "76 : loss = 19.328089966466592\n",
      "77 : loss = 19.31983160841795\n",
      "78 : loss = 19.311588849751857\n",
      "79 : loss = 19.303361705228763\n",
      "80 : loss = 19.2951501892754\n",
      "81 : loss = 19.28695431597146\n",
      "82 : loss = 19.278774099057266\n",
      "83 : loss = 19.270609551929947\n",
      "84 : loss = 19.262460687653878\n",
      "85 : loss = 19.254327518934097\n",
      "86 : loss = 19.24621005812255\n",
      "87 : loss = 19.238108317144732\n",
      "88 : loss = 19.230022307310872\n",
      "89 : loss = 19.22195203908337\n",
      "90 : loss = 19.21389753116787\n",
      "91 : loss = 19.20585878305845\n",
      "92 : loss = 19.197835809213235\n",
      "93 : loss = 19.189828631920495\n",
      "94 : loss = 19.18183723985777\n",
      "95 : loss = 19.173861654374395\n",
      "96 : loss = 19.165901882609074\n",
      "97 : loss = 19.157957933228413\n",
      "98 : loss = 19.150029814295888\n",
      "99 : loss = 19.142117532405756\n",
      "100 : loss = 19.134221098864273\n",
      "101 : loss = 19.126340529597844\n",
      "102 : loss = 19.11847581362821\n",
      "103 : loss = 19.110626966830424\n",
      "104 : loss = 19.10279399523172\n",
      "105 : loss = 19.094976905087616\n",
      "106 : loss = 19.08717570233031\n",
      "107 : loss = 19.07939039259514\n",
      "108 : loss = 19.07162098121004\n",
      "109 : loss = 19.063867480453467\n",
      "110 : loss = 19.056129917660524\n",
      "111 : loss = 19.04840826742956\n",
      "112 : loss = 19.040702533859395\n",
      "113 : loss = 19.03301272075012\n",
      "114 : loss = 19.025338831601374\n",
      "115 : loss = 19.01768086960548\n",
      "116 : loss = 19.010038846842026\n",
      "117 : loss = 19.0024127894277\n",
      "118 : loss = 18.9948026670737\n",
      "119 : loss = 18.987208481787686\n",
      "120 : loss = 18.979630235295478\n",
      "121 : loss = 18.972067929020852\n",
      "122 : loss = 18.964521564110754\n",
      "123 : loss = 18.9569911414176\n",
      "124 : loss = 18.949476661508186\n",
      "125 : loss = 18.941978124665816\n",
      "126 : loss = 18.934495530891223\n",
      "127 : loss = 18.927028879903773\n",
      "128 : loss = 18.91957817114242\n",
      "129 : loss = 18.912143403765434\n",
      "130 : loss = 18.90472457666026\n",
      "131 : loss = 18.897321688430598\n",
      "132 : loss = 18.88993473741332\n",
      "133 : loss = 18.88256372166789\n",
      "134 : loss = 18.87520863898364\n",
      "135 : loss = 18.867869486878853\n",
      "136 : loss = 18.86054626260958\n",
      "137 : loss = 18.853238963156922\n",
      "138 : loss = 18.84594758524648\n",
      "139 : loss = 18.838672125328\n",
      "140 : loss = 18.831412579600727\n",
      "141 : loss = 18.824168943999698\n",
      "142 : loss = 18.81694121419506\n",
      "143 : loss = 18.809729385609764\n",
      "144 : loss = 18.802533453405616\n",
      "145 : loss = 18.795353412483777\n",
      "146 : loss = 18.788189257506033\n",
      "147 : loss = 18.781040982868902\n",
      "148 : loss = 18.773908582731934\n",
      "149 : loss = 18.76679205099818\n",
      "150 : loss = 18.759691381327972\n",
      "151 : loss = 18.75260656712968\n",
      "152 : loss = 18.745537601578643\n",
      "153 : loss = 18.738484477605997\n",
      "154 : loss = 18.731447187890552\n",
      "155 : loss = 18.724425724890455\n",
      "156 : loss = 18.71742008081252\n",
      "157 : loss = 18.71043024763533\n",
      "158 : loss = 18.703456217099433\n",
      "159 : loss = 18.696497980716124\n",
      "160 : loss = 18.68955552976393\n",
      "161 : loss = 18.68262885529351\n",
      "162 : loss = 18.67571794812518\n",
      "163 : loss = 18.668822798852826\n",
      "164 : loss = 18.661943397852166\n",
      "165 : loss = 18.65507973526793\n",
      "166 : loss = 18.648231801024696\n",
      "167 : loss = 18.64139958483601\n",
      "168 : loss = 18.63458307618231\n",
      "169 : loss = 18.627782264341135\n",
      "170 : loss = 18.620997138367095\n",
      "171 : loss = 18.61422768710366\n",
      "172 : loss = 18.60747389917746\n",
      "173 : loss = 18.600735763018275\n",
      "174 : loss = 18.594013266830917\n",
      "175 : loss = 18.587306398623106\n",
      "176 : loss = 18.580615146192073\n",
      "177 : loss = 18.573939497133853\n",
      "178 : loss = 18.56727943884281\n",
      "179 : loss = 18.560634958508736\n",
      "180 : loss = 18.554006043123763\n",
      "181 : loss = 18.547392679484147\n",
      "182 : loss = 18.540794854186966\n",
      "183 : loss = 18.534212553638\n",
      "184 : loss = 18.5276457640435\n",
      "185 : loss = 18.521094471430267\n",
      "186 : loss = 18.51455866162185\n",
      "187 : loss = 18.508038320264976\n",
      "188 : loss = 18.501533432805825\n",
      "189 : loss = 18.495043984520617\n",
      "190 : loss = 18.488569960494498\n",
      "191 : loss = 18.482111345627143\n",
      "192 : loss = 18.475668124645036\n",
      "193 : loss = 18.46924028209117\n",
      "194 : loss = 18.462827802331258\n",
      "195 : loss = 18.45643066955953\n",
      "196 : loss = 18.45004886778676\n",
      "197 : loss = 18.443682380862278\n",
      "198 : loss = 18.437331192454298\n",
      "199 : loss = 18.430995286067166\n",
      "200 : loss = 18.42467464503663\n",
      "201 : loss = 18.418369252529782\n",
      "202 : loss = 18.41207909154891\n",
      "203 : loss = 18.405804243750392\n",
      "204 : loss = 18.399544611614683\n",
      "205 : loss = 18.393300159196556\n",
      "206 : loss = 18.38707086885324\n",
      "207 : loss = 18.380856722783538\n",
      "208 : loss = 18.374657703027687\n",
      "209 : loss = 18.368473791469363\n",
      "210 : loss = 18.36230496984463\n",
      "211 : loss = 18.356151219735175\n",
      "212 : loss = 18.350012522575458\n",
      "213 : loss = 18.343888859649777\n",
      "214 : loss = 18.33778021209096\n",
      "215 : loss = 18.331686560897165\n",
      "216 : loss = 18.325607886908884\n",
      "217 : loss = 18.31954417083321\n",
      "218 : loss = 18.313495393234636\n",
      "219 : loss = 18.307461534538792\n",
      "220 : loss = 18.301442575027867\n",
      "221 : loss = 18.295438494854466\n",
      "222 : loss = 18.289449274028314\n",
      "223 : loss = 18.28347489243346\n",
      "224 : loss = 18.27751532981591\n",
      "225 : loss = 18.271570565792874\n",
      "226 : loss = 18.26564057984935\n",
      "227 : loss = 18.259725351345963\n",
      "228 : loss = 18.253824859514467\n",
      "229 : loss = 18.247939083462736\n",
      "230 : loss = 18.242068002169724\n",
      "231 : loss = 18.23621159450235\n",
      "232 : loss = 18.230369839191034\n",
      "233 : loss = 18.224542714865656\n",
      "234 : loss = 18.2187302000235\n",
      "235 : loss = 18.212932273046817\n",
      "236 : loss = 18.207148912206623\n",
      "237 : loss = 18.201380095656805\n",
      "238 : loss = 18.195625801441082\n",
      "239 : loss = 18.189886007489147\n",
      "240 : loss = 18.184160691620114\n",
      "241 : loss = 18.17844983154958\n",
      "242 : loss = 18.172753404877476\n",
      "243 : loss = 18.16707138910557\n",
      "244 : loss = 18.161403761624914\n",
      "245 : loss = 18.155750499729162\n",
      "246 : loss = 18.150111580601884\n",
      "247 : loss = 18.14448698133311\n",
      "248 : loss = 18.13887667891377\n",
      "249 : loss = 18.133280650229786\n",
      "250 : loss = 18.127698872075136\n",
      "251 : loss = 18.122131321150412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 : loss = 18.116577974053502\n",
      "253 : loss = 18.111038807298232\n",
      "254 : loss = 18.105513797304994\n",
      "255 : loss = 18.10000292039702\n",
      "256 : loss = 18.09450615281897\n",
      "257 : loss = 18.08902347072006\n",
      "258 : loss = 18.083554850163388\n",
      "259 : loss = 18.078100267130218\n",
      "260 : loss = 18.07265969751696\n",
      "261 : loss = 18.067233117132705\n",
      "262 : loss = 18.061820501709544\n",
      "263 : loss = 18.056421826896926\n",
      "264 : loss = 18.051037068266506\n",
      "265 : loss = 18.045666201311\n",
      "266 : loss = 18.04030920144612\n",
      "267 : loss = 18.034966044012158\n",
      "268 : loss = 18.029636704273564\n",
      "269 : loss = 18.024321157426257\n",
      "270 : loss = 18.019019378587387\n",
      "271 : loss = 18.013731342812456\n",
      "272 : loss = 18.008457025073024\n",
      "273 : loss = 18.003196400285248\n",
      "274 : loss = 17.997949443290437\n",
      "275 : loss = 17.992716128869482\n",
      "276 : loss = 17.987496431733536\n",
      "277 : loss = 17.982290326528304\n",
      "278 : loss = 17.97709778784165\n",
      "279 : loss = 17.97191879019773\n",
      "280 : loss = 17.966753308055036\n",
      "281 : loss = 17.96160131582426\n",
      "282 : loss = 17.95646278784403\n",
      "283 : loss = 17.95133769840675\n",
      "284 : loss = 17.946226021738582\n",
      "285 : loss = 17.941127732019872\n",
      "286 : loss = 17.936042803371393\n",
      "287 : loss = 17.93097120986089\n",
      "288 : loss = 17.925912925506882\n",
      "289 : loss = 17.9208679242745\n",
      "290 : loss = 17.91583618008083\n",
      "291 : loss = 17.91081766678879\n",
      "292 : loss = 17.905812358223034\n",
      "293 : loss = 17.90082026496727\n",
      "294 : loss = 17.895841504637733\n",
      "295 : loss = 17.8908758704013\n",
      "296 : loss = 17.885923335894162\n",
      "297 : loss = 17.880983874715355\n",
      "298 : loss = 17.876057460413552\n",
      "299 : loss = 17.87114406650395\n",
      "300 : loss = 17.866243666458658\n",
      "301 : loss = 17.861356233710634\n",
      "302 : loss = 17.856481741657824\n",
      "303 : loss = 17.851620163656147\n",
      "304 : loss = 17.846771473031765\n",
      "305 : loss = 17.841935643074745\n",
      "306 : loss = 17.83711264703627\n",
      "307 : loss = 17.832302458139885\n",
      "308 : loss = 17.82750504957283\n",
      "309 : loss = 17.822720559218922\n",
      "310 : loss = 17.817948866243807\n",
      "311 : loss = 17.81318987316876\n",
      "312 : loss = 17.808443553063913\n",
      "313 : loss = 17.803709878972004\n",
      "314 : loss = 17.79898882390953\n",
      "315 : loss = 17.794280360862274\n",
      "316 : loss = 17.789584462796284\n",
      "317 : loss = 17.78490110265174\n",
      "318 : loss = 17.780230253352528\n",
      "319 : loss = 17.77557188778377\n",
      "320 : loss = 17.770925978828096\n",
      "321 : loss = 17.766292499336107\n",
      "322 : loss = 17.761671422140115\n",
      "323 : loss = 17.75706272005626\n",
      "324 : loss = 17.752466365881155\n",
      "325 : loss = 17.7478823323954\n",
      "326 : loss = 17.743310592362718\n",
      "327 : loss = 17.738751118526224\n",
      "328 : loss = 17.734203883623866\n",
      "329 : loss = 17.729668860369433\n",
      "330 : loss = 17.725146021473076\n",
      "331 : loss = 17.720635339626433\n",
      "332 : loss = 17.716136787508084\n",
      "333 : loss = 17.71165033779332\n",
      "334 : loss = 17.707175963137125\n",
      "335 : loss = 17.702713636197196\n",
      "336 : loss = 17.698263329609382\n",
      "337 : loss = 17.693825016012475\n",
      "338 : loss = 17.68939866803109\n",
      "339 : loss = 17.684984258289848\n",
      "340 : loss = 17.680581759399857\n",
      "341 : loss = 17.676191143974677\n",
      "342 : loss = 17.671812384619965\n",
      "343 : loss = 17.66744545393821\n",
      "344 : loss = 17.66309032452741\n",
      "345 : loss = 17.658746968989657\n",
      "346 : loss = 17.654415359918353\n",
      "347 : loss = 17.650095469911655\n",
      "348 : loss = 17.645787271564526\n",
      "349 : loss = 17.64149073746929\n",
      "350 : loss = 17.637205840229473\n",
      "351 : loss = 17.63293255244172\n",
      "352 : loss = 17.628670846709618\n",
      "353 : loss = 17.62442069563715\n",
      "354 : loss = 17.620182071835004\n",
      "355 : loss = 17.615954947916414\n",
      "356 : loss = 17.611739296502584\n",
      "357 : loss = 17.607535090217187\n",
      "358 : loss = 17.603342301692646\n",
      "359 : loss = 17.599160903565668\n",
      "360 : loss = 17.594990868485997\n",
      "361 : loss = 17.590832169107284\n",
      "362 : loss = 17.586684778090376\n",
      "363 : loss = 17.582548668113635\n",
      "364 : loss = 17.57842381185451\n",
      "365 : loss = 17.574310182008496\n",
      "366 : loss = 17.570207751284038\n",
      "367 : loss = 17.56611649239529\n",
      "368 : loss = 17.56203637807204\n",
      "369 : loss = 17.557967381057402\n",
      "370 : loss = 17.553909474105776\n",
      "371 : loss = 17.54986262998948\n",
      "372 : loss = 17.545826821488934\n",
      "373 : loss = 17.541802021407428\n",
      "374 : loss = 17.537788202561657\n",
      "375 : loss = 17.533785337779534\n",
      "376 : loss = 17.529793399910155\n",
      "377 : loss = 17.525812665732214\n",
      "378 : loss = 17.521842813277953\n",
      "379 : loss = 17.517883979397265\n",
      "380 : loss = 17.513936106905753\n",
      "381 : loss = 17.509999026136278\n",
      "382 : loss = 17.506072710045945\n",
      "383 : loss = 17.502157131617416\n",
      "384 : loss = 17.498252263849363\n",
      "385 : loss = 17.494358079765195\n",
      "386 : loss = 17.4904745524082\n",
      "387 : loss = 17.48660165484307\n",
      "388 : loss = 17.48273936016081\n",
      "389 : loss = 17.47888764147105\n",
      "390 : loss = 17.475046471910627\n",
      "391 : loss = 17.471215824639057\n",
      "392 : loss = 17.467395672838258\n",
      "393 : loss = 17.46358598971754\n",
      "394 : loss = 17.4597867485122\n",
      "395 : loss = 17.455997922483395\n",
      "396 : loss = 17.452219484915464\n",
      "397 : loss = 17.44845140912118\n",
      "398 : loss = 17.444693668441708\n",
      "399 : loss = 17.44094623624204\n",
      "400 : loss = 17.437209085917413\n",
      "401 : loss = 17.4334821908973\n",
      "402 : loss = 17.429765524628326\n",
      "403 : loss = 17.426059060591168\n",
      "404 : loss = 17.422362772301227\n",
      "405 : loss = 17.4186766332943\n",
      "406 : loss = 17.4150006171438\n",
      "407 : loss = 17.41133469744913\n",
      "408 : loss = 17.40767884784656\n",
      "409 : loss = 17.404033041996318\n",
      "410 : loss = 17.40039725359418\n",
      "411 : loss = 17.39677145636995\n",
      "412 : loss = 17.393155624080514\n",
      "413 : loss = 17.38954973052035\n",
      "414 : loss = 17.385953749514282\n",
      "415 : loss = 17.382367654922064\n",
      "416 : loss = 17.378791420637757\n",
      "417 : loss = 17.375225020585294\n",
      "418 : loss = 17.37166842872956\n",
      "419 : loss = 17.368121619065704\n",
      "420 : loss = 17.364584565622295\n",
      "421 : loss = 17.361057242470398\n",
      "422 : loss = 17.357539623710956\n",
      "423 : loss = 17.354031683481637\n",
      "424 : loss = 17.350533395957537\n",
      "425 : loss = 17.347044735350835\n",
      "426 : loss = 17.343565675906923\n",
      "427 : loss = 17.34009619191613\n",
      "428 : loss = 17.336636257700533\n",
      "429 : loss = 17.333185847618317\n",
      "430 : loss = 17.32974493606838\n",
      "431 : loss = 17.326313497489206\n",
      "432 : loss = 17.322891506359095\n",
      "433 : loss = 17.319478937188165\n",
      "434 : loss = 17.31607576453588\n",
      "435 : loss = 17.31268196298932\n",
      "436 : loss = 17.309297507185885\n",
      "437 : loss = 17.30592237179559\n",
      "438 : loss = 17.302556531534332\n",
      "439 : loss = 17.29919996115375\n",
      "440 : loss = 17.295852635449236\n",
      "441 : loss = 17.29251452925376\n",
      "442 : loss = 17.289185617445543\n",
      "443 : loss = 17.28586587493999\n",
      "444 : loss = 17.282555276698588\n",
      "445 : loss = 17.279253797721974\n",
      "446 : loss = 17.275961413050826\n",
      "447 : loss = 17.272678097773124\n",
      "448 : loss = 17.269403827015477\n",
      "449 : loss = 17.266138575946943\n",
      "450 : loss = 17.262882319785263\n",
      "451 : loss = 17.259635033782395\n",
      "452 : loss = 17.256396693241154\n",
      "453 : loss = 17.253167273501766\n",
      "454 : loss = 17.249946749954162\n",
      "455 : loss = 17.246735098027745\n",
      "456 : loss = 17.2435322931974\n",
      "457 : loss = 17.240338310982207\n",
      "458 : loss = 17.23715312694819\n",
      "459 : loss = 17.233976716701033\n",
      "460 : loss = 17.230809055894227\n",
      "461 : loss = 17.227650120227022\n",
      "462 : loss = 17.224499885443215\n",
      "463 : loss = 17.22135832732985\n",
      "464 : loss = 17.218225421721506\n",
      "465 : loss = 17.215101144498703\n",
      "466 : loss = 17.21198547158827\n",
      "467 : loss = 17.208878378957678\n",
      "468 : loss = 17.20577984262806\n",
      "469 : loss = 17.20268983866137\n",
      "470 : loss = 17.19960834317071\n",
      "471 : loss = 17.196535332309864\n",
      "472 : loss = 17.193470782284155\n",
      "473 : loss = 17.190414669343905\n",
      "474 : loss = 17.187366969788275\n",
      "475 : loss = 17.18432795922671\n",
      "476 : loss = 17.181297431322633\n",
      "477 : loss = 17.178275246027223\n",
      "478 : loss = 17.175261379828054\n",
      "479 : loss = 17.1722558092576\n",
      "480 : loss = 17.169258510899294\n",
      "481 : loss = 17.166269461383237\n",
      "482 : loss = 17.163288637388966\n",
      "483 : loss = 17.160316015641136\n",
      "484 : loss = 17.157351572914717\n",
      "485 : loss = 17.154395286033147\n",
      "486 : loss = 17.151447131867332\n",
      "487 : loss = 17.148507087337503\n",
      "488 : loss = 17.14557512940945\n",
      "489 : loss = 17.142651235103543\n",
      "490 : loss = 17.139735381484144\n",
      "491 : loss = 17.136827545665454\n",
      "492 : loss = 17.13392770481342\n",
      "493 : loss = 17.13103583613816\n",
      "494 : loss = 17.12815191690423\n",
      "495 : loss = 17.125275924420297\n",
      "496 : loss = 17.12240783604826\n",
      "497 : loss = 17.119547947088535\n",
      "498 : loss = 17.11669603569341\n",
      "499 : loss = 17.113851960811868\n",
      "500 : loss = 17.111015831139337\n",
      "501 : loss = 17.108187801617017\n",
      "502 : loss = 17.105367541455138\n",
      "503 : loss = 17.102555028366176\n",
      "504 : loss = 17.099750240104346\n",
      "505 : loss = 17.096953154480648\n",
      "506 : loss = 17.09416374935296\n",
      "507 : loss = 17.091382002631544\n",
      "508 : loss = 17.088607892271593\n",
      "509 : loss = 17.08584139628404\n",
      "510 : loss = 17.08308249272722\n",
      "511 : loss = 17.080331159711104\n",
      "512 : loss = 17.077587375395613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513 : loss = 17.07485111799029\n",
      "514 : loss = 17.0721223657526\n",
      "515 : loss = 17.069401096998455\n",
      "516 : loss = 17.06668729008685\n",
      "517 : loss = 17.06398092342978\n",
      "518 : loss = 17.061281975488093\n",
      "519 : loss = 17.058590424778927\n",
      "520 : loss = 17.055906249861454\n",
      "521 : loss = 17.05322942935204\n",
      "522 : loss = 17.050559941915026\n",
      "523 : loss = 17.04789776626688\n",
      "524 : loss = 17.045242881175344\n",
      "525 : loss = 17.04259526545404\n",
      "526 : loss = 17.039954897974805\n",
      "527 : loss = 17.037321757654098\n",
      "528 : loss = 17.034695823462016\n",
      "529 : loss = 17.03207707441916\n",
      "530 : loss = 17.029465489598042\n",
      "531 : loss = 17.026861048118462\n",
      "532 : loss = 17.02426372915488\n",
      "533 : loss = 17.021673511931954\n",
      "534 : loss = 17.01909037572252\n",
      "535 : loss = 17.016514299854766\n",
      "536 : loss = 17.013945263702084\n",
      "537 : loss = 17.01138324669606\n",
      "538 : loss = 17.00882822831098\n",
      "539 : loss = 17.006280188079224\n",
      "540 : loss = 17.00373910558067\n",
      "541 : loss = 17.00120496044503\n",
      "542 : loss = 16.998677732355237\n",
      "543 : loss = 16.996157401043867\n",
      "544 : loss = 16.99364394629653\n",
      "545 : loss = 16.9911373479466\n",
      "546 : loss = 16.988637585878816\n",
      "547 : loss = 16.98614464003106\n",
      "548 : loss = 16.983658490389868\n",
      "549 : loss = 16.981179116993978\n",
      "550 : loss = 16.978706499932958\n",
      "551 : loss = 16.9762406193446\n",
      "552 : loss = 16.973781455419754\n",
      "553 : loss = 16.971328988402476\n",
      "554 : loss = 16.968883198581988\n",
      "555 : loss = 16.966444066301882\n",
      "556 : loss = 16.964011571956323\n",
      "557 : loss = 16.961585695987836\n",
      "558 : loss = 16.959166418893354\n",
      "559 : loss = 16.95675372121661\n",
      "560 : loss = 16.954347583554654\n",
      "561 : loss = 16.951947986553524\n",
      "562 : loss = 16.94955491091064\n",
      "563 : loss = 16.947168337374407\n",
      "564 : loss = 16.94478824674092\n",
      "565 : loss = 16.94241461986088\n",
      "566 : loss = 16.94004743763224\n",
      "567 : loss = 16.937686681003022\n",
      "568 : loss = 16.93533233097677\n",
      "569 : loss = 16.93298436859904\n",
      "570 : loss = 16.93064277497226\n",
      "571 : loss = 16.92830753124816\n",
      "572 : loss = 16.925978618623002\n",
      "573 : loss = 16.923656018351338\n",
      "574 : loss = 16.92133971173341\n",
      "575 : loss = 16.919029680118697\n",
      "576 : loss = 16.916725904909544\n",
      "577 : loss = 16.914428367557026\n",
      "578 : loss = 16.91213704955983\n",
      "579 : loss = 16.909851932469547\n",
      "580 : loss = 16.907572997886586\n",
      "581 : loss = 16.90530022746296\n",
      "582 : loss = 16.903033602894812\n",
      "583 : loss = 16.90077310593523\n",
      "584 : loss = 16.898518718382434\n",
      "585 : loss = 16.896270422082466\n",
      "586 : loss = 16.894028198936855\n",
      "587 : loss = 16.891792030890926\n",
      "588 : loss = 16.88956189994371\n",
      "589 : loss = 16.887337788140005\n",
      "590 : loss = 16.885119677575293\n",
      "591 : loss = 16.882907550395444\n",
      "592 : loss = 16.880701388794076\n",
      "593 : loss = 16.878501175015337\n",
      "594 : loss = 16.87630689134693\n",
      "595 : loss = 16.874118520135884\n",
      "596 : loss = 16.871936043768027\n",
      "597 : loss = 16.86975944468389\n",
      "598 : loss = 16.867588705371293\n",
      "599 : loss = 16.86542380836476\n",
      "600 : loss = 16.86326473625254\n",
      "601 : loss = 16.86111147166421\n",
      "602 : loss = 16.858963997284185\n",
      "603 : loss = 16.856822295842257\n",
      "604 : loss = 16.854686350119522\n",
      "605 : loss = 16.8525561429382\n",
      "606 : loss = 16.850431657179058\n",
      "607 : loss = 16.848312875762915\n",
      "608 : loss = 16.846199781663167\n",
      "609 : loss = 16.84409235789677\n",
      "610 : loss = 16.841990587533815\n",
      "611 : loss = 16.839894453689254\n",
      "612 : loss = 16.83780393952722\n",
      "613 : loss = 16.835719028256502\n",
      "614 : loss = 16.8336397031392\n",
      "615 : loss = 16.83156594748065\n",
      "616 : loss = 16.829497744633603\n",
      "617 : loss = 16.8274350779995\n",
      "618 : loss = 16.825377931028026\n",
      "619 : loss = 16.823326287215256\n",
      "620 : loss = 16.821280130103897\n",
      "621 : loss = 16.819239443284847\n",
      "622 : loss = 16.8172042103944\n",
      "623 : loss = 16.815174415117493\n",
      "624 : loss = 16.813150041187342\n",
      "625 : loss = 16.81113107237742\n",
      "626 : loss = 16.809117492518286\n",
      "627 : loss = 16.80710928547585\n",
      "628 : loss = 16.805106435171712\n",
      "629 : loss = 16.8031089255708\n",
      "630 : loss = 16.801116740681127\n",
      "631 : loss = 16.799129864561312\n",
      "632 : loss = 16.797148281314225\n",
      "633 : loss = 16.795171975091815\n",
      "634 : loss = 16.793200930086112\n",
      "635 : loss = 16.791235130540993\n",
      "636 : loss = 16.789274560745532\n",
      "637 : loss = 16.78731920502978\n",
      "638 : loss = 16.78536904777403\n",
      "639 : loss = 16.783424073404433\n",
      "640 : loss = 16.781484266388897\n",
      "641 : loss = 16.77954961124499\n",
      "642 : loss = 16.777620092531215\n",
      "643 : loss = 16.775695694857127\n",
      "644 : loss = 16.773776402873995\n",
      "645 : loss = 16.77186220127647\n",
      "646 : loss = 16.7699530748077\n",
      "647 : loss = 16.76804900825417\n",
      "648 : loss = 16.766149986445228\n",
      "649 : loss = 16.764255994261017\n",
      "650 : loss = 16.76236701661882\n",
      "651 : loss = 16.760483038487223\n",
      "652 : loss = 16.75860404487374\n",
      "653 : loss = 16.75673002083345\n",
      "654 : loss = 16.75486095146558\n",
      "655 : loss = 16.752996821912404\n",
      "656 : loss = 16.751137617361525\n",
      "657 : loss = 16.74928332304257\n",
      "658 : loss = 16.747433924230574\n",
      "659 : loss = 16.74558940624611\n",
      "660 : loss = 16.743749754451592\n",
      "661 : loss = 16.741914954249978\n",
      "662 : loss = 16.74008499109473\n",
      "663 : loss = 16.738259850476165\n",
      "664 : loss = 16.736439517932276\n",
      "665 : loss = 16.73462397904271\n",
      "666 : loss = 16.732813219430515\n",
      "667 : loss = 16.731007224762145\n",
      "668 : loss = 16.729205980744943\n",
      "669 : loss = 16.727409473131598\n",
      "670 : loss = 16.72561768771822\n",
      "671 : loss = 16.723830610341828\n",
      "672 : loss = 16.72204822687996\n",
      "673 : loss = 16.720270523257234\n",
      "674 : loss = 16.71849748543955\n",
      "675 : loss = 16.71672909943289\n",
      "676 : loss = 16.714965351287645\n",
      "677 : loss = 16.71320622709482\n",
      "678 : loss = 16.711451712987465\n",
      "679 : loss = 16.709701795143054\n",
      "680 : loss = 16.707956459776295\n",
      "681 : loss = 16.70621569314941\n",
      "682 : loss = 16.704479485138148\n",
      "683 : loss = 16.70274838537842\n",
      "684 : loss = 16.701021813182532\n",
      "685 : loss = 16.69929975497554\n",
      "686 : loss = 16.697582197226957\n",
      "687 : loss = 16.695869126440655\n",
      "688 : loss = 16.694160529169988\n",
      "689 : loss = 16.692456392001812\n",
      "690 : loss = 16.690756701568418\n",
      "691 : loss = 16.68906144454159\n",
      "692 : loss = 16.68737060763316\n",
      "693 : loss = 16.68568417759604\n",
      "694 : loss = 16.684002141223793\n",
      "695 : loss = 16.682324485350833\n",
      "696 : loss = 16.680651196851368\n",
      "697 : loss = 16.678982262637554\n",
      "698 : loss = 16.677317669664234\n",
      "699 : loss = 16.67565740492753\n",
      "700 : loss = 16.674001455460083\n",
      "701 : loss = 16.672349808335632\n",
      "702 : loss = 16.670702450668948\n",
      "703 : loss = 16.669059369611485\n",
      "704 : loss = 16.667420552357694\n",
      "705 : loss = 16.665785986136473\n",
      "706 : loss = 16.664155658222473\n",
      "707 : loss = 16.662529555924905\n",
      "708 : loss = 16.660908051676067\n",
      "709 : loss = 16.659290945724653\n",
      "710 : loss = 16.657678027328583\n",
      "711 : loss = 16.656069283953865\n",
      "712 : loss = 16.654464703107088\n",
      "713 : loss = 16.652864272330373\n",
      "714 : loss = 16.651267979207447\n",
      "715 : loss = 16.64967581135915\n",
      "716 : loss = 16.648087756443086\n",
      "717 : loss = 16.646503802158477\n",
      "718 : loss = 16.644923936238552\n",
      "719 : loss = 16.643348146457644\n",
      "720 : loss = 16.641776420629057\n",
      "721 : loss = 16.64020874659598\n",
      "722 : loss = 16.638645112250593\n",
      "723 : loss = 16.637085505515575\n",
      "724 : loss = 16.635529914351224\n",
      "725 : loss = 16.633978326758164\n",
      "726 : loss = 16.63243073077303\n",
      "727 : loss = 16.630887114467182\n",
      "728 : loss = 16.629347465953508\n",
      "729 : loss = 16.627811773379378\n",
      "730 : loss = 16.626280024928818\n",
      "731 : loss = 16.624752208821427\n",
      "732 : loss = 16.623228313318542\n",
      "733 : loss = 16.62170832671263\n",
      "734 : loss = 16.62019223733674\n",
      "735 : loss = 16.618680033556302\n",
      "736 : loss = 16.617171703776034\n",
      "737 : loss = 16.615667236436707\n",
      "738 : loss = 16.614166620014547\n",
      "739 : loss = 16.612669843019116\n",
      "740 : loss = 16.611176894001783\n",
      "741 : loss = 16.609687761544002\n",
      "742 : loss = 16.608202434265902\n",
      "743 : loss = 16.60672090082421\n",
      "744 : loss = 16.60524314990798\n",
      "745 : loss = 16.60376917024392\n",
      "746 : loss = 16.602298950592537\n",
      "747 : loss = 16.600832479752892\n",
      "748 : loss = 16.599369746554054\n",
      "749 : loss = 16.597910739863863\n",
      "750 : loss = 16.596455448585317\n",
      "751 : loss = 16.59500386165376\n",
      "752 : loss = 16.59355596803933\n",
      "753 : loss = 16.592111756749432\n",
      "754 : loss = 16.590671216824816\n",
      "755 : loss = 16.5892343373398\n",
      "756 : loss = 16.587801107404186\n",
      "757 : loss = 16.586371516159353\n",
      "758 : loss = 16.58494555278532\n",
      "759 : loss = 16.583523206493762\n",
      "760 : loss = 16.582104466528808\n",
      "761 : loss = 16.580689322170574\n",
      "762 : loss = 16.579277762732698\n",
      "763 : loss = 16.57786977756186\n",
      "764 : loss = 16.576465356039236\n",
      "765 : loss = 16.5750644875772\n",
      "766 : loss = 16.573667161624808\n",
      "767 : loss = 16.572273367661108\n",
      "768 : loss = 16.570883095200422\n",
      "769 : loss = 16.569496333790998\n",
      "770 : loss = 16.568113073011663\n",
      "771 : loss = 16.56673330247531\n",
      "772 : loss = 16.56535701182652\n",
      "773 : loss = 16.56398419074615\n",
      "774 : loss = 16.562614828944046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775 : loss = 16.561248916161265\n",
      "776 : loss = 16.559886442177337\n",
      "777 : loss = 16.55852739679825\n",
      "778 : loss = 16.557171769865175\n",
      "779 : loss = 16.555819551251524\n",
      "780 : loss = 16.554470730859528\n",
      "781 : loss = 16.553125298628245\n",
      "782 : loss = 16.55178324452543\n",
      "783 : loss = 16.55044455855088\n",
      "784 : loss = 16.549109230736327\n",
      "785 : loss = 16.54777725114633\n",
      "786 : loss = 16.54644860987541\n",
      "787 : loss = 16.545123297049496\n",
      "788 : loss = 16.543801302825578\n",
      "789 : loss = 16.542482617394327\n",
      "790 : loss = 16.541167230974818\n",
      "791 : loss = 16.53985513381859\n",
      "792 : loss = 16.538546316206663\n",
      "793 : loss = 16.537240768452016\n",
      "794 : loss = 16.53593848089933\n",
      "795 : loss = 16.53463944392146\n",
      "796 : loss = 16.53334364792293\n",
      "797 : loss = 16.53205108334057\n",
      "798 : loss = 16.530761740638564\n",
      "799 : loss = 16.52947561031382\n",
      "800 : loss = 16.528192682892936\n",
      "801 : loss = 16.526912948928796\n",
      "802 : loss = 16.52563639901206\n",
      "803 : loss = 16.52436302375598\n",
      "804 : loss = 16.52309281380788\n",
      "805 : loss = 16.521825759842184\n",
      "806 : loss = 16.520561852565223\n",
      "807 : loss = 16.519301082711998\n",
      "808 : loss = 16.518043441046803\n",
      "809 : loss = 16.516788918363495\n",
      "810 : loss = 16.51553750548528\n",
      "811 : loss = 16.514289193263778\n",
      "812 : loss = 16.51304397258115\n",
      "813 : loss = 16.51180183434862\n",
      "814 : loss = 16.510562769503686\n",
      "815 : loss = 16.509326769013995\n",
      "816 : loss = 16.508093823880714\n",
      "817 : loss = 16.506863925126314\n",
      "818 : loss = 16.505637063805985\n",
      "819 : loss = 16.504413231001642\n",
      "820 : loss = 16.50319241782568\n",
      "821 : loss = 16.5019746154166\n",
      "822 : loss = 16.500759814943937\n",
      "823 : loss = 16.499548007601966\n",
      "824 : loss = 16.498339184615347\n",
      "825 : loss = 16.497133337235535\n",
      "826 : loss = 16.495930456743245\n",
      "827 : loss = 16.49473053444503\n",
      "828 : loss = 16.493533561677406\n",
      "829 : loss = 16.492339529804177\n",
      "830 : loss = 16.491148430212878\n",
      "831 : loss = 16.48996025432533\n",
      "832 : loss = 16.488774993583164\n",
      "833 : loss = 16.487592639461933\n",
      "834 : loss = 16.4864131834615\n",
      "835 : loss = 16.485236617106665\n",
      "836 : loss = 16.48406293195386\n",
      "837 : loss = 16.482892119583195\n",
      "838 : loss = 16.481724171602696\n",
      "839 : loss = 16.480559079647247\n",
      "840 : loss = 16.47939683537742\n",
      "841 : loss = 16.47823743048281\n",
      "842 : loss = 16.477080856676373\n",
      "843 : loss = 16.475927105699867\n",
      "844 : loss = 16.47477616932133\n",
      "845 : loss = 16.473628039333533\n",
      "846 : loss = 16.47248270755697\n",
      "847 : loss = 16.471340165836647\n",
      "848 : loss = 16.470200406045688\n",
      "849 : loss = 16.46906342008091\n",
      "850 : loss = 16.467929199867978\n",
      "851 : loss = 16.466797737355165\n",
      "852 : loss = 16.465669024518036\n",
      "853 : loss = 16.464543053357417\n",
      "854 : loss = 16.463419815901453\n",
      "855 : loss = 16.46229930420022\n",
      "856 : loss = 16.461181510332302\n",
      "857 : loss = 16.460066426399994\n",
      "858 : loss = 16.458954044531687\n",
      "859 : loss = 16.45784435687975\n",
      "860 : loss = 16.45673735562276\n",
      "861 : loss = 16.4556330329644\n",
      "862 : loss = 16.454531381132274\n",
      "863 : loss = 16.45343239237878\n",
      "864 : loss = 16.452336058982677\n",
      "865 : loss = 16.451242373245265\n",
      "866 : loss = 16.450151327494066\n",
      "867 : loss = 16.44906291408054\n",
      "868 : loss = 16.447977125380337\n",
      "869 : loss = 16.446893953793435\n",
      "870 : loss = 16.445813391745077\n",
      "871 : loss = 16.444735431684162\n",
      "872 : loss = 16.44366006608245\n",
      "873 : loss = 16.442587287437078\n",
      "874 : loss = 16.44151708826902\n",
      "875 : loss = 16.440449461124672\n",
      "876 : loss = 16.43938439856958\n",
      "877 : loss = 16.43832189319819\n",
      "878 : loss = 16.43726193762687\n",
      "879 : loss = 16.436204524493217\n",
      "880 : loss = 16.435149646461365\n",
      "881 : loss = 16.434097296217995\n",
      "882 : loss = 16.433047466472747\n",
      "883 : loss = 16.43200014995932\n",
      "884 : loss = 16.4309553394336\n",
      "885 : loss = 16.42991302767485\n",
      "886 : loss = 16.428873207485857\n",
      "887 : loss = 16.427835871692793\n",
      "888 : loss = 16.426801013143013\n",
      "889 : loss = 16.42576862470927\n",
      "890 : loss = 16.424738699284386\n",
      "891 : loss = 16.423711229784494\n",
      "892 : loss = 16.422686209151923\n",
      "893 : loss = 16.421663630345687\n",
      "894 : loss = 16.420643486351462\n",
      "895 : loss = 16.419625770176985\n",
      "896 : loss = 16.418610474848972\n",
      "897 : loss = 16.417597593421146\n",
      "898 : loss = 16.416587118964657\n",
      "899 : loss = 16.415579044578198\n",
      "900 : loss = 16.41457336337693\n",
      "901 : loss = 16.413570068500412\n",
      "902 : loss = 16.412569153113186\n",
      "903 : loss = 16.411570610396083\n",
      "904 : loss = 16.41057443355473\n",
      "905 : loss = 16.409580615816687\n",
      "906 : loss = 16.408589150429567\n",
      "907 : loss = 16.407600030663488\n",
      "908 : loss = 16.406613249809737\n",
      "909 : loss = 16.40562880118179\n",
      "910 : loss = 16.40464667811362\n",
      "911 : loss = 16.403666873960006\n",
      "912 : loss = 16.402689382099148\n",
      "913 : loss = 16.401714195926804\n",
      "914 : loss = 16.40074130886367\n",
      "915 : loss = 16.39977071434772\n",
      "916 : loss = 16.398802405841074\n",
      "917 : loss = 16.397836376824774\n",
      "918 : loss = 16.396872620800142\n",
      "919 : loss = 16.395911131292394\n",
      "920 : loss = 16.394951901843616\n",
      "921 : loss = 16.39399492601831\n",
      "922 : loss = 16.393040197401422\n",
      "923 : loss = 16.392087709598336\n",
      "924 : loss = 16.391137456233018\n",
      "925 : loss = 16.390189430953942\n",
      "926 : loss = 16.389243627426143\n",
      "927 : loss = 16.388300039334297\n",
      "928 : loss = 16.387358660387637\n",
      "929 : loss = 16.386419484310814\n",
      "930 : loss = 16.385482504849417\n",
      "931 : loss = 16.384547715771777\n",
      "932 : loss = 16.383615110862795\n",
      "933 : loss = 16.38268492985967\n",
      "934 : loss = 16.381757323603466\n",
      "935 : loss = 16.380831882596397\n",
      "936 : loss = 16.37990860070617\n",
      "937 : loss = 16.37898747181492\n",
      "938 : loss = 16.378068489827562\n",
      "939 : loss = 16.377151648668132\n",
      "940 : loss = 16.376236942280787\n",
      "941 : loss = 16.37532436462616\n",
      "942 : loss = 16.374414045012095\n",
      "943 : loss = 16.373506356933856\n",
      "944 : loss = 16.372600779192478\n",
      "945 : loss = 16.371697305827595\n",
      "946 : loss = 16.370795930896673\n",
      "947 : loss = 16.369896648477134\n",
      "948 : loss = 16.36899945266587\n",
      "949 : loss = 16.368104337576053\n",
      "950 : loss = 16.36721129734239\n",
      "951 : loss = 16.36632032611533\n",
      "952 : loss = 16.36543141806728\n",
      "953 : loss = 16.364544567385757\n",
      "954 : loss = 16.363659768279224\n",
      "955 : loss = 16.36277701497341\n",
      "956 : loss = 16.361896301711557\n",
      "957 : loss = 16.361017622757437\n",
      "958 : loss = 16.36014097239098\n",
      "959 : loss = 16.359266344911852\n",
      "960 : loss = 16.35839373463554\n",
      "961 : loss = 16.357523135898003\n",
      "962 : loss = 16.35665454305176\n",
      "963 : loss = 16.355787950467803\n",
      "964 : loss = 16.35492335253488\n",
      "965 : loss = 16.354060743658774\n",
      "966 : loss = 16.35320011826369\n",
      "967 : loss = 16.35234193889818\n",
      "968 : loss = 16.351485916523128\n",
      "969 : loss = 16.350631860601084\n",
      "970 : loss = 16.349779765625442\n",
      "971 : loss = 16.348929626109257\n",
      "972 : loss = 16.348081436583165\n",
      "973 : loss = 16.34723519159272\n",
      "974 : loss = 16.346390885702917\n",
      "975 : loss = 16.345548513495228\n",
      "976 : loss = 16.344708069569005\n",
      "977 : loss = 16.343869548538606\n",
      "978 : loss = 16.34303294503773\n",
      "979 : loss = 16.342198253715935\n",
      "980 : loss = 16.341366069396123\n",
      "981 : loss = 16.340535840293548\n",
      "982 : loss = 16.339707507010036\n",
      "983 : loss = 16.338881064263646\n",
      "984 : loss = 16.338056506788153\n",
      "985 : loss = 16.337233829334522\n",
      "986 : loss = 16.336413026669877\n",
      "987 : loss = 16.335594093579793\n",
      "988 : loss = 16.33477702486309\n",
      "989 : loss = 16.333961815337876\n",
      "990 : loss = 16.333148459835666\n",
      "991 : loss = 16.332337180626006\n",
      "992 : loss = 16.331528172782697\n",
      "993 : loss = 16.330721003146216\n",
      "994 : loss = 16.329915666617786\n",
      "995 : loss = 16.329112158110213\n",
      "996 : loss = 16.328310472555728\n",
      "997 : loss = 16.327510604899885\n",
      "998 : loss = 16.326712550105544\n",
      "999 : loss = 16.325916303151324\n",
      "output:  tensor([ 2.1463e-02,  1.8523e-01,  2.6110e-01,  9.8319e-01,  3.8341e-01,\n",
      "         4.1329e-01,  1.6722e-01,  7.5574e-01,  7.8681e-02, -1.7377e-03,\n",
      "         4.1703e-01,  5.9059e-01, -1.5043e-04,  1.8655e-01,  6.9418e-01,\n",
      "         2.7249e-01, -1.4922e-02,  1.9300e-01, -2.6361e-03, -1.2103e-02,\n",
      "         5.0765e-01,  6.4634e-04, -8.0217e-03, -2.3163e-03,  4.1069e-01,\n",
      "         2.1430e-01,  5.6449e-01,  5.4977e-01, -1.4901e-03, -6.2677e-03,\n",
      "        -6.5330e-03,  1.6093e-01,  7.7880e-02, -4.1361e-03,  1.8861e-01,\n",
      "         2.2013e-01, -6.5357e-03,  2.2362e-01,  5.9664e-01, -8.7042e-04,\n",
      "         5.5647e-01, -4.9201e-03,  8.2210e-01,  4.0636e-01,  2.6872e-01,\n",
      "         2.2465e-01, -1.1130e-03,  2.5175e-01,  5.6982e-02, -8.4491e-03,\n",
      "         4.2876e-01, -1.4778e-02,  8.2271e-01,  2.8464e-01,  4.0609e-03,\n",
      "        -1.0490e-02,  4.1211e-01,  1.0806e-01, -4.0157e-03, -1.4001e-02,\n",
      "        -2.0075e-03, -1.0256e-02, -9.0376e-03,  1.9662e-01,  2.4202e-01,\n",
      "         6.3311e-01, -5.7796e-03,  6.5652e-01,  3.4181e-01,  2.4018e-01,\n",
      "         1.2022e-01,  3.0136e-01,  3.0470e-01, -1.3416e-02, -8.1933e-03,\n",
      "        -1.1745e-02, -1.3297e-02,  1.6741e-01,  7.8676e-01,  5.9479e-01,\n",
      "         2.6995e-01,  2.5972e-01, -8.3033e-03,  8.2186e-01,  8.1923e-01,\n",
      "        -1.3136e-02, -2.1746e-03, -1.3241e-02,  2.6085e-01,  6.1390e-01,\n",
      "         8.0688e-01,  8.1146e-01,  1.1691e+00,  5.0946e-01, -3.4278e-03,\n",
      "         2.1204e-01,  1.1295e-01,  4.3047e-01, -5.5281e-03, -1.7725e-03,\n",
      "         2.5430e-01,  1.2638e-01,  2.3465e-02,  7.1730e-01,  6.8483e-01,\n",
      "        -8.7873e-03,  9.7089e-02, -6.6616e-03,  1.4506e-01,  7.4311e-01,\n",
      "         2.9085e-01,  1.9149e-01,  3.4496e-01, -1.5322e-02,  2.9458e-01,\n",
      "         5.2744e-01,  7.9576e-01, -6.7563e-03,  2.9447e-01, -3.1310e-03,\n",
      "         8.1364e-01, -6.3684e-03,  8.1677e-01,  6.8189e-02,  5.3864e-01,\n",
      "         2.2050e-01, -4.8941e-03,  2.4545e-01,  3.8373e-01,  1.2312e-01,\n",
      "         7.9432e-01,  7.0238e-01,  3.9006e-01, -1.5445e-02, -9.9093e-04,\n",
      "         2.1803e-01, -5.3020e-03,  3.7547e-01,  1.3433e-01,  3.0796e-01,\n",
      "        -1.8385e-02,  2.3073e-01,  7.8399e-01,  3.5077e-01,  3.6800e-01,\n",
      "        -1.1654e-02, -1.1180e-02,  4.2227e-01,  4.3200e-01, -1.0438e-04,\n",
      "         2.7536e-01, -1.8336e-02,  2.1893e-01,  3.8059e-01,  1.8775e-01,\n",
      "         1.6699e-01, -1.1381e-02,  3.4021e-01,  2.2163e-01, -3.5759e-03,\n",
      "        -1.2552e-02,  3.8780e-02,  2.4552e-01,  2.5413e-01,  7.6333e-01,\n",
      "         5.9387e-01,  5.1913e-01,  5.9770e-01,  2.1067e-01,  7.9816e-01,\n",
      "         5.3306e-01, -1.3737e-02,  1.1424e-01,  2.4964e-01,  3.0984e-01,\n",
      "         4.5060e-01,  2.3936e-01, -1.1340e-02, -1.3115e-02,  5.6211e-01,\n",
      "         9.7090e-02, -7.5059e-04,  8.3706e-02,  4.7088e-03,  5.8564e-02,\n",
      "         7.9326e-01, -1.4293e-02,  1.3464e-01, -5.3872e-03,  2.4268e-01,\n",
      "         3.9827e-01, -7.0530e-03,  2.9654e-01, -3.1908e-03,  4.9421e-01,\n",
      "         2.0761e-01,  4.6591e-01,  7.4010e-01,  6.7971e-01, -2.8735e-04,\n",
      "         2.6868e-01,  1.4709e-01, -1.1857e-02,  1.9082e-01,  2.9590e-01,\n",
      "         3.2629e-01,  3.1685e-01,  5.4091e-01,  7.7873e-01,  1.9700e-01,\n",
      "        -4.9562e-03, -1.2311e-03,  9.1640e-02,  6.9991e-01,  5.8998e-01,\n",
      "        -1.9104e-03,  3.1105e-01, -1.8491e-02,  4.2507e-01,  5.8996e-01,\n",
      "        -8.8940e-03,  2.4457e-01,  1.9767e-01,  8.2302e-01,  3.3373e-01,\n",
      "         3.3653e-01,  2.8685e-01, -1.1686e-02, -3.9502e-03,  1.9983e-01,\n",
      "         1.2469e+00,  8.2303e-01,  5.1674e-01, -9.2740e-03,  8.1919e-01,\n",
      "         7.5046e-01,  7.9326e-01, -1.4468e-02,  7.7672e-01,  2.0196e-01,\n",
      "         2.2832e-01,  4.7793e-01,  8.1751e-01,  2.9568e-01, -1.0504e-02,\n",
      "        -8.2824e-03,  1.0629e-01, -5.8070e-03, -1.1321e-02,  1.2535e-01,\n",
      "        -6.6530e-05, -1.4748e-03,  7.5765e-01, -1.3090e-02,  4.0835e-01,\n",
      "         4.6460e-01,  2.3466e-01, -8.6280e-03,  9.8373e-02,  2.2404e-01,\n",
      "         2.3417e-03,  2.7208e-01, -1.5218e-02,  6.9745e-01,  1.2621e-01,\n",
      "         3.0372e-01,  1.2897e+00,  5.0707e-01, -1.8053e-02,  9.5063e-02,\n",
      "        -1.3796e-02,  1.0289e-01, -7.3632e-03, -5.2801e-03,  2.4796e-01,\n",
      "         4.5411e-01, -4.7386e-03,  3.7787e-01, -9.3188e-04,  3.7912e-01,\n",
      "         3.5404e-01,  1.4732e-01,  2.0832e-01, -1.4676e-02,  5.8167e-01,\n",
      "        -1.5054e-02, -4.2602e-03,  2.2380e-01, -1.4211e-02,  4.4547e-01,\n",
      "         5.0023e-02, -2.3592e-03,  7.4792e-01,  3.2890e-01,  1.6759e-01,\n",
      "         1.8638e-01,  4.6399e-01,  2.7645e-01,  3.0816e-01,  2.7098e-01,\n",
      "        -2.9856e-03,  2.0568e-01,  2.3764e-01,  5.8234e-01, -1.2258e-02,\n",
      "        -3.9238e-03,  8.1235e-01,  5.0257e-02,  2.5208e-01,  3.5621e-01,\n",
      "         5.5032e-01, -1.5286e-02,  5.1898e-01, -1.9266e-03, -1.1271e-02,\n",
      "         2.2111e-01,  2.7493e-01, -1.8381e-02,  3.1217e-01,  1.0823e-01,\n",
      "         8.7489e-01,  4.0359e-01,  7.3781e-01,  2.0251e-01,  2.3788e-01,\n",
      "         2.7371e-01, -1.3220e-02,  6.1681e-01, -1.3387e-02,  1.4494e-03,\n",
      "        -7.7131e-03,  8.2079e-01, -4.7375e-03,  4.1059e-02, -9.5291e-03,\n",
      "         4.3420e-01,  4.7254e-01,  3.9085e-02,  9.1804e-02, -5.0760e-03,\n",
      "        -2.7211e-04,  1.9758e-01, -6.2174e-03,  1.4832e-01, -1.1750e-02,\n",
      "        -1.4181e-02,  2.5010e-01,  5.1362e-01,  1.9748e-01, -1.1713e-02,\n",
      "        -6.4467e-04,  3.2861e-01, -1.3350e-02,  3.6696e-01,  7.3142e-01,\n",
      "        -6.1538e-03,  3.4930e-01, -8.2326e-03, -4.0998e-04,  2.9968e-01,\n",
      "         1.2512e-01, -1.7481e-02,  2.4464e-01, -1.5285e-02,  3.9297e-01,\n",
      "        -3.1947e-03,  8.1052e-01,  1.0543e-01, -2.5477e-04,  7.4344e-01,\n",
      "         1.8305e-01, -1.2766e-02,  6.3091e-01, -2.7509e-04,  6.0738e-01,\n",
      "         2.1139e-01,  7.9751e-01, -6.8944e-03,  2.3633e-01,  8.1761e-01,\n",
      "         2.5440e-01, -1.7515e-02, -2.9405e-03, -1.7288e-04,  2.4689e-01,\n",
      "         4.7775e-01,  8.1190e-01,  5.5326e-01,  8.2197e-01,  5.5513e-01,\n",
      "         5.0644e-01, -8.1017e-03, -8.3592e-03, -9.8645e-03,  2.7390e-01,\n",
      "         1.2028e-01,  8.2195e-01, -1.0706e-02,  4.7827e-01, -1.2794e-02],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight[0] shape:  torch.Size([20, 400])\n",
      "bias[0] shape:  torch.Size([20, 1])\n",
      "Relu Params[0]:  0.009999973008629584\n",
      "BN gamma[0]:  1.5536595326946203\n",
      "BN beta[0]:  -7461.116251366032\n",
      "weight[1] shape:  torch.Size([10, 20])\n",
      "bias[1] shape:  torch.Size([10, 1])\n",
      "Relu Params[1]:  -9.9553530015232e-08\n",
      "BN gamma[1]:  -0.9910294075847725\n",
      "BN beta[1]:  -55.21421654115341\n",
      "weight[2] shape:  torch.Size([5, 10])\n",
      "bias[2] shape:  torch.Size([5, 1])\n",
      "Relu Params[2]:  0.01\n",
      "BN gamma[2]:  0.9999999988290565\n",
      "BN beta[2]:  -0.03492053859001166\n",
      "weight[3] shape:  torch.Size([400, 5])\n",
      "bias[3] shape:  torch.Size([400, 1])\n",
      "Relu Params[3]:  0.01000068816056092\n",
      "BN gamma[3]:  1.0000006881602725\n",
      "BN beta[3]:  -3.2648570178475096e-07\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    layers = (20, 10, 5, 400)\n",
    "    aFuncs = ('PRelu', 'PRelu', 'cos', 'PRelu')\n",
    "    batchSize = 10\n",
    "    nn1 = DNN(layers, aFuncs)\n",
    "    # 所有input全部变型成 m x 1 的形状\n",
    "    testInput = torch.randn((20, 20), dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "    testInput = testInput.view(-1, 1)\n",
    "    targetY = nn1.batchNorm(5*torch.sin(testInput**3) + 2*torch.cos(2*testInput-1) - torch.tanh(testInput), 0)\n",
    "    # 生成权重和偏移\n",
    "    nn1.genParam(testInput)\n",
    "    nn1.printShape()\n",
    "    # 训练\n",
    "    nn1.train(testInput, targetY, nanInvestigate=0, epoch=1000)\n",
    "    # 预测\n",
    "    estimateY = nn1.predict(testInput)\n",
    "    \n",
    "    plt.scatter(testInput.cpu().numpy().flatten(), targetY.cpu().numpy().flatten())\n",
    "    plt.scatter(testInput.cpu().numpy().flatten(), estimateY.cpu().numpy().flatten())\n",
    "    plt.show()\n",
    "    nn1.printShape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan]], device='cuda:0', dtype=torch.float64),\n",
       " tensor([[nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan]], device='cuda:0', dtype=torch.float64),\n",
       " tensor([[nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan]], device='cuda:0', dtype=torch.float64)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1.layers['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275]], device='cuda:0', dtype=torch.float64), tensor([[5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672]], device='cuda:0', dtype=torch.float64), tensor([[-1.2032e-02],\n",
      "        [-3.6203e-04],\n",
      "        [ 1.3286e-01],\n",
      "        [ 8.2065e-01],\n",
      "        [ 3.5820e-01],\n",
      "        [ 4.0628e-01],\n",
      "        [-1.7814e-03],\n",
      "        [ 9.0331e-01],\n",
      "        [-8.3471e-03],\n",
      "        [-2.0779e-02],\n",
      "        [ 4.1217e-01],\n",
      "        [ 7.1294e-01],\n",
      "        [-1.9788e-02],\n",
      "        [-2.5701e-04],\n",
      "        [ 8.4488e-01],\n",
      "        [ 3.0465e-01],\n",
      "        [-7.4479e-02],\n",
      "        [ 3.3996e-03],\n",
      "        [-2.8695e-02],\n",
      "        [-6.4443e-02],\n",
      "        [ 5.4960e-01],\n",
      "        [-1.3078e-02],\n",
      "        [-4.6815e-02],\n",
      "        [-2.3264e-02],\n",
      "        [ 5.0980e-01],\n",
      "        [ 4.2547e-02],\n",
      "        [ 2.9080e-01],\n",
      "        [ 6.6073e-01],\n",
      "        [-1.9707e-02],\n",
      "        [-3.9700e-02],\n",
      "        [-4.3438e-02],\n",
      "        [-2.2694e-03],\n",
      "        [-8.4038e-03],\n",
      "        [-3.0926e-02],\n",
      "        [-9.2543e-05],\n",
      "        [ 5.2865e-02],\n",
      "        [-4.0792e-02],\n",
      "        [ 6.0422e-02],\n",
      "        [ 7.2068e-01],\n",
      "        [-2.2535e-02],\n",
      "        [ 6.2077e-01],\n",
      "        [-3.4174e-02],\n",
      "        [ 1.0022e+00],\n",
      "        [ 4.7680e-01],\n",
      "        [ 1.4744e-01],\n",
      "        [ 6.2304e-02],\n",
      "        [-1.8062e-02],\n",
      "        [ 1.1498e-01],\n",
      "        [-9.8470e-03],\n",
      "        [-4.8539e-02],\n",
      "        [ 4.3053e-01],\n",
      "        [-7.3893e-02],\n",
      "        [ 1.0012e+00],\n",
      "        [ 1.8166e-01],\n",
      "        [-1.2919e-02],\n",
      "        [-5.6729e-02],\n",
      "        [ 4.0441e-01],\n",
      "        [-6.2334e-03],\n",
      "        [-3.0425e-02],\n",
      "        [-7.0760e-02],\n",
      "        [-2.6314e-02],\n",
      "        [-5.5792e-02],\n",
      "        [-5.2899e-02],\n",
      "        [ 9.5320e-03],\n",
      "        [ 9.6432e-02],\n",
      "        [ 7.3033e-01],\n",
      "        [-3.7706e-02],\n",
      "        [ 7.6347e-01],\n",
      "        [ 2.8762e-01],\n",
      "        [ 9.2942e-02],\n",
      "        [ 1.2172e-01],\n",
      "        [ 2.1371e-01],\n",
      "        [ 2.2000e-01],\n",
      "        [-6.8418e-02],\n",
      "        [-4.9712e-02],\n",
      "        [-6.1745e-02],\n",
      "        [-6.7944e-02],\n",
      "        [-1.7665e-03],\n",
      "        [ 9.4733e-01],\n",
      "        [ 6.7581e-01],\n",
      "        [ 1.4980e-01],\n",
      "        [ 2.8821e-01],\n",
      "        [-4.7952e-02],\n",
      "        [ 9.9958e-01],\n",
      "        [ 9.9947e-01],\n",
      "        [-5.0947e-02],\n",
      "        [-2.2658e-02],\n",
      "        [-6.7718e-02],\n",
      "        [ 3.0733e-01],\n",
      "        [ 7.0306e-01],\n",
      "        [ 9.7642e-01],\n",
      "        [ 9.9078e-01],\n",
      "        [ 1.0601e+00],\n",
      "        [ 2.0748e-01],\n",
      "        [-1.3902e-02],\n",
      "        [ 3.7673e-02],\n",
      "        [-5.8758e-03],\n",
      "        [ 4.3319e-01],\n",
      "        [-3.6675e-02],\n",
      "        [-2.0929e-02],\n",
      "        [ 1.1986e-01],\n",
      "        [-4.8850e-03],\n",
      "        [-4.8320e-04],\n",
      "        [ 8.7414e-01],\n",
      "        [ 8.0342e-01],\n",
      "        [-5.2763e-02],\n",
      "        [-7.0294e-03],\n",
      "        [-4.1305e-02],\n",
      "        [-3.4835e-03],\n",
      "        [ 8.8549e-01],\n",
      "        [ 1.9364e-01],\n",
      "        [ 1.2948e-03],\n",
      "        [ 2.9314e-01],\n",
      "        [-7.6138e-02],\n",
      "        [ 2.0079e-01],\n",
      "        [ 5.7861e-01],\n",
      "        [ 9.6025e-01],\n",
      "        [-4.1690e-02],\n",
      "        [-2.5483e-03],\n",
      "        [-3.1424e-02],\n",
      "        [ 9.8647e-01],\n",
      "        [-4.0111e-02],\n",
      "        [ 9.9681e-01],\n",
      "        [-9.0824e-03],\n",
      "        [ 5.9493e-01],\n",
      "        [ 5.3510e-02],\n",
      "        [-3.4066e-02],\n",
      "        [ 1.0296e-01],\n",
      "        [ 3.5873e-01],\n",
      "        [-5.1271e-03],\n",
      "        [ 9.5818e-01],\n",
      "        [ 8.2814e-01],\n",
      "        [ 3.6906e-01],\n",
      "        [-7.6743e-02],\n",
      "        [-1.7526e-02],\n",
      "        [ 4.9131e-02],\n",
      "        [-3.5746e-02],\n",
      "        [ 3.4509e-01],\n",
      "        [-4.2918e-03],\n",
      "        [ 2.2614e-01],\n",
      "        [-7.1035e-02],\n",
      "        [ 7.3448e-02],\n",
      "        [ 9.4337e-01],\n",
      "        [ 3.0325e-01],\n",
      "        [ 3.3262e-01],\n",
      "        [-4.5289e-02],\n",
      "        [-6.0973e-02],\n",
      "        [ 4.2041e-01],\n",
      "        [ 4.3556e-01],\n",
      "        [-1.3579e-02],\n",
      "        [ 1.6014e-01],\n",
      "        [-7.0890e-02],\n",
      "        [ 5.0722e-02],\n",
      "        [ 3.5357e-01],\n",
      "        [-1.6114e-04],\n",
      "        [-1.7995e-03],\n",
      "        [-6.1730e-02],\n",
      "        [-3.1650e-04],\n",
      "        [ 2.3917e-01],\n",
      "        [-2.8588e-02],\n",
      "        [-6.6129e-02],\n",
      "        [-1.1021e-02],\n",
      "        [ 1.0308e-01],\n",
      "        [ 2.9858e-01],\n",
      "        [ 9.3198e-01],\n",
      "        [ 6.7449e-01],\n",
      "        [ 5.6647e-01],\n",
      "        [ 7.2202e-01],\n",
      "        [ 3.5319e-02],\n",
      "        [ 9.7494e-01],\n",
      "        [ 5.8681e-01],\n",
      "        [-6.9702e-02],\n",
      "        [-5.7811e-03],\n",
      "        [ 1.1094e-01],\n",
      "        [ 2.2966e-01],\n",
      "        [ 4.6417e-01],\n",
      "        [-6.0916e-03],\n",
      "        [-6.0128e-02],\n",
      "        [-6.8235e-02],\n",
      "        [ 6.2891e-01],\n",
      "        [-7.0294e-03],\n",
      "        [-1.6467e-02],\n",
      "        [-7.9902e-03],\n",
      "        [-1.8529e-02],\n",
      "        [-9.7407e-03],\n",
      "        [ 9.6898e-01],\n",
      "        [-7.1934e-02],\n",
      "        [-4.2692e-03],\n",
      "        [-4.0029e-02],\n",
      "        [-6.0965e-03],\n",
      "        [ 3.8234e-01],\n",
      "        [-4.2895e-02],\n",
      "        [ 2.0454e-01],\n",
      "        [-2.6971e-02],\n",
      "        [ 5.2973e-01],\n",
      "        [ 2.9370e-02],\n",
      "        [ 4.8741e-01],\n",
      "        [ 8.8125e-01],\n",
      "        [ 7.9620e-01],\n",
      "        [-1.4403e-02],\n",
      "        [ 1.4737e-01],\n",
      "        [-3.3296e-03],\n",
      "        [-6.2192e-02],\n",
      "        [ 5.1252e-04],\n",
      "        [ 2.0331e-01],\n",
      "        [ 2.5991e-01],\n",
      "        [ 2.4266e-01],\n",
      "        [ 5.9823e-01],\n",
      "        [ 9.5112e-01],\n",
      "        [ 2.0745e-01],\n",
      "        [-3.4322e-02],\n",
      "        [-2.3375e-02],\n",
      "        [-7.4221e-03],\n",
      "        [ 8.5214e-01],\n",
      "        [ 6.6892e-01],\n",
      "        [-2.1524e-02],\n",
      "        [ 2.3191e-01],\n",
      "        [-7.1459e-02],\n",
      "        [ 4.2479e-01],\n",
      "        [ 6.6890e-01],\n",
      "        [-3.4546e-02],\n",
      "        [ 1.0127e-01],\n",
      "        [ 1.1329e-02],\n",
      "        [ 1.0020e+00],\n",
      "        [ 2.7330e-01],\n",
      "        [ 2.7829e-01],\n",
      "        [ 1.8594e-01],\n",
      "        [-6.2875e-02],\n",
      "        [-3.0152e-02],\n",
      "        [ 1.5270e-02],\n",
      "        [ 1.1633e+00],\n",
      "        [ 1.0026e+00],\n",
      "        [ 5.6295e-01],\n",
      "        [-5.1857e-02],\n",
      "        [ 6.1640e-01],\n",
      "        [ 8.9585e-01],\n",
      "        [ 9.5666e-01],\n",
      "        [-7.2638e-02],\n",
      "        [ 9.3303e-01],\n",
      "        [ 1.9023e-02],\n",
      "        [ 6.9027e-02],\n",
      "        [ 5.0548e-01],\n",
      "        [ 9.9241e-01],\n",
      "        [ 2.0290e-01],\n",
      "        [-4.1255e-02],\n",
      "        [-4.7867e-02],\n",
      "        [-6.3621e-03],\n",
      "        [-3.7818e-02],\n",
      "        [-6.0053e-02],\n",
      "        [-4.9616e-03],\n",
      "        [-1.3408e-02],\n",
      "        [-1.9641e-02],\n",
      "        [ 9.0600e-01],\n",
      "        [-6.8140e-02],\n",
      "        [ 4.7935e-01],\n",
      "        [ 4.8543e-01],\n",
      "        [ 8.0687e-02],\n",
      "        [-4.9259e-02],\n",
      "        [-6.9367e-03],\n",
      "        [ 6.1188e-02],\n",
      "        [-1.3000e-02],\n",
      "        [ 1.5387e-01],\n",
      "        [-7.5996e-02],\n",
      "        [ 8.2119e-01],\n",
      "        [-4.8978e-03],\n",
      "        [ 2.1816e-01],\n",
      "        [ 1.2153e+00],\n",
      "        [ 6.0604e-01],\n",
      "        [-6.9753e-02],\n",
      "        [-7.1756e-03],\n",
      "        [-6.9939e-02],\n",
      "        [-6.6097e-03],\n",
      "        [-4.4153e-02],\n",
      "        [-3.5656e-02],\n",
      "        [ 1.0774e-01],\n",
      "        [ 4.6952e-01],\n",
      "        [-3.3424e-02],\n",
      "        [ 3.4907e-01],\n",
      "        [-1.7267e-02],\n",
      "        [ 3.5114e-01],\n",
      "        [ 3.0889e-01],\n",
      "        [-3.3120e-03],\n",
      "        [ 3.1316e-02],\n",
      "        [-7.3479e-02],\n",
      "        [ 6.5703e-01],\n",
      "        [-5.8495e-02],\n",
      "        [-3.1442e-02],\n",
      "        [ 6.0751e-02],\n",
      "        [-7.1602e-02],\n",
      "        [ 4.5632e-01],\n",
      "        [-1.0307e-02],\n",
      "        [-2.3447e-02],\n",
      "        [ 8.9227e-01],\n",
      "        [ 2.6463e-01],\n",
      "        [-1.7529e-03],\n",
      "        [-2.7028e-04],\n",
      "        [ 4.8451e-01],\n",
      "        [ 1.6221e-01],\n",
      "        [ 3.5055e-01],\n",
      "        [ 1.5177e-01],\n",
      "        [-2.6106e-02],\n",
      "        [ 2.6163e-02],\n",
      "        [ 8.8138e-02],\n",
      "        [ 6.5799e-01],\n",
      "        [-6.3795e-02],\n",
      "        [-3.0042e-02],\n",
      "        [ 9.8454e-01],\n",
      "        [-1.0292e-02],\n",
      "        [ 1.1560e-01],\n",
      "        [ 3.1261e-01],\n",
      "        [ 6.1187e-01],\n",
      "        [-7.5987e-02],\n",
      "        [ 5.6625e-01],\n",
      "        [-2.1593e-02],\n",
      "        [-5.9852e-02],\n",
      "        [ 5.5856e-02],\n",
      "        [ 3.3374e-01],\n",
      "        [-7.1020e-02],\n",
      "        [ 2.3400e-01],\n",
      "        [-6.2207e-03],\n",
      "        [ 6.9091e-01],\n",
      "        [ 3.9087e-01],\n",
      "        [ 8.7802e-01],\n",
      "        [ 1.9886e-02],\n",
      "        [ 8.8578e-02],\n",
      "        [ 1.5699e-01],\n",
      "        [-6.7634e-02],\n",
      "        [ 3.5835e-01],\n",
      "        [-6.9252e-02],\n",
      "        [-1.3041e-02],\n",
      "        [-3.0252e-02],\n",
      "        [ 9.9770e-01],\n",
      "        [-3.3420e-02],\n",
      "        [-1.0880e-02],\n",
      "        [-5.2880e-02],\n",
      "        [ 4.3898e-01],\n",
      "        [ 4.9739e-01],\n",
      "        [-1.1003e-02],\n",
      "        [-7.4103e-03],\n",
      "        [-3.8581e-02],\n",
      "        [-1.4334e-02],\n",
      "        [ 1.1189e-02],\n",
      "        [-2.4545e-02],\n",
      "        [-3.2362e-03],\n",
      "        [-6.1765e-02],\n",
      "        [-7.1485e-02],\n",
      "        [ 1.1181e-01],\n",
      "        [ 5.5838e-01],\n",
      "        [-8.0634e-03],\n",
      "        [-6.1618e-02],\n",
      "        [-1.5997e-02],\n",
      "        [ 2.6411e-01],\n",
      "        [-6.8154e-02],\n",
      "        [ 3.2660e-02],\n",
      "        [ 8.6902e-01],\n",
      "        [-3.9235e-02],\n",
      "        [ 3.0071e-01],\n",
      "        [-4.7666e-02],\n",
      "        [-1.4952e-02],\n",
      "        [ 2.1052e-01],\n",
      "        [-4.9787e-03],\n",
      "        [-6.7083e-02],\n",
      "        [ 1.0142e-01],\n",
      "        [-7.5979e-02],\n",
      "        [ 3.7378e-01],\n",
      "        [-2.6988e-02],\n",
      "        [ 6.0508e-01],\n",
      "        [-6.4246e-03],\n",
      "        [-1.4257e-02],\n",
      "        [ 5.2173e-01],\n",
      "        [-5.3597e-04],\n",
      "        [-6.6930e-02],\n",
      "        [ 7.2722e-01],\n",
      "        [-1.7564e-03],\n",
      "        [ 6.9376e-01],\n",
      "        [ 3.6566e-02],\n",
      "        [ 9.6277e-01],\n",
      "        [-4.2251e-02],\n",
      "        [ 8.5647e-02],\n",
      "        [ 9.9256e-01],\n",
      "        [ 1.2003e-01],\n",
      "        [-6.7803e-02],\n",
      "        [-3.0433e-02],\n",
      "        [-1.3888e-02],\n",
      "        [ 1.0569e-01],\n",
      "        [ 5.9586e-01],\n",
      "        [ 9.8387e-01],\n",
      "        [ 6.1612e-01],\n",
      "        [ 9.9977e-01],\n",
      "        [ 6.1882e-01],\n",
      "        [ 5.4781e-01],\n",
      "        [-4.7138e-02],\n",
      "        [-4.8177e-02],\n",
      "        [-3.8833e-02],\n",
      "        [ 1.5736e-01],\n",
      "        [-5.3365e-03],\n",
      "        [ 1.0021e+00],\n",
      "        [-5.7596e-02],\n",
      "        [ 5.0598e-01],\n",
      "        [-6.5934e-02]], device='cuda:0', dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "nn2 = DNN(layers, aFuncs)\n",
    "nn2.readParams('d:\\\\nanInvest_41.pt')\n",
    "print(nn2.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
