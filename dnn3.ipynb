{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np, matplotlib.pyplot as plt, multiprocessing as mp\n",
    "from numpy import random\n",
    "import torch, cv2, time, random, os, threading, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = {\n",
    "    'boolean': torch.bool, 'ui8': torch.uint8, 'i8': torch.int8, 'i16': torch.int16, 'i32': torch.int32, 'i64': torch.int64, \n",
    "    'f16': torch.float16, 'f32': torch.float32, 'f64': torch.float64, 'f64Complex': torch.complex64, 'f128Complex': torch.complex128\n",
    "}\n",
    "DEVICE = {\n",
    "    'auto': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), \n",
    "    'cpu': torch.device('cpu'), \n",
    "    'cuda0': torch.device('cuda:0')\n",
    "}\n",
    "\n",
    "DEVICE_CHOICE = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch(x, area, startPoint):\n",
    "    rowImg, columnImg, depthImg = x.shape\n",
    "    rowVision, columnVision = area\n",
    "    startX, startY = startPoint\n",
    "    totalCanvas = np.zeros((rowImg+rowVision*2, columnImg+columnVision*2, depthImg), dtype=np.int)\n",
    "    totalCanvas[rowVision:rowVision+rowImg,columnVision:columnVision+columnImg] = x\n",
    "    seeing = totalCanvas[startX:startX+rowVision+1, startY:startY+columnVision+1]\n",
    "    return seeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNoise(x, volumn = 50):\n",
    "    row, column, depth = x.shape\n",
    "    noise = np.random.randint(low=-volumn, high=volumn, size=(row, column, depth))\n",
    "    imgWithNoise = x + noise\n",
    "    imgWithNoise[imgWithNoise > 255] = 255\n",
    "    imgWithNoise[imgWithNoise < 0] = 0\n",
    "    return imgWithNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takePhoto():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # 初始化摄像头，摄像头适应光源\n",
    "#     for i in range(20):\n",
    "#         cap.read()\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('camera not working')\n",
    "    cap.release()\n",
    "    print('Image shape: ', frame.shape)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, layers=(10, 20, 10), aFunc=('cos', 'PRelu', 'PRelu', 'sin')):\n",
    "        self.layerNum = len(layers)\n",
    "        assert self.layerNum >= 1\n",
    "        torch.manual_seed(0)\n",
    "        self.layerShapes = layers\n",
    "        self.aFuncChosen = aFunc\n",
    "        self.lr = {\n",
    "            'weight': 0.01,\n",
    "            'bias': 0.01,\n",
    "            'relu param': 0.01\n",
    "        }\n",
    "        self.inputs = None\n",
    "        self.targetY = None\n",
    "        self.weights = [None] * self.layerNum\n",
    "        self.biases = [None] * self.layerNum\n",
    "        self.reluParam = [None] * self.layerNum\n",
    "        self.layers = {\n",
    "            'Z': [None] * self.layerNum, \n",
    "            'N': [None] * self.layerNum, \n",
    "            'A': [None] * self.layerNum\n",
    "        }\n",
    "        self.BN = {\n",
    "            'epsilon': 1e-5, \n",
    "            'gamma': [1] * self.layerNum, \n",
    "            'beta': [0] * self.layerNum\n",
    "        }\n",
    "        self.activFunc = {\n",
    "            'PRelu': lambda x, i: torch.max(x, x * self.reluParam[i]), \n",
    "            'sigmoid': lambda x: 1/(1+torch.exp(-x)), \n",
    "            'softmax': lambda x: torch.exp(x - torch.max(x)) / torch.sum(torch.exp(x - torch.max(x))), \n",
    "            'tanh': lambda x: torch.tanh(x), \n",
    "            'sin': lambda x: torch.sin(x), \n",
    "            'cos': lambda x: torch.cos(x), \n",
    "            'linear': lambda x: x\n",
    "        }\n",
    "        self.activFuncDer = {\n",
    "            'PRelu': self._PReluDer,  \n",
    "            'sigmoid': lambda x: self.activFunc['sigmoid'](x) * (1 - self.activFunc['sigmoid'](x)), \n",
    "            'softmax': lambda x, a: self.activFunc['softmax'](x) * (a - self.activFunc['softmax'](x)), \n",
    "            'tanh': lambda x: 1 - torch.tanh(x) ** 2, \n",
    "            'sin': lambda x: torch.cos(x), \n",
    "            'cos': lambda x: -torch.sin(x), \n",
    "            'linear': lambda x: 1\n",
    "        }\n",
    "        self.lossFunc = {\n",
    "            'mse': lambda predictY, targetY: (targetY - predictY) ** 2, \n",
    "            'bce': lambda predictY, targetY: targetY * torch.log(predictY) + (1 - targetY) * torch.log(1 - predictY)\n",
    "        }\n",
    "        self.lossFuncDer = {\n",
    "            'mse': lambda predictY, targetY: 2 * (targetY - predictY), \n",
    "            'bce': lambda predictY, targetY: Y / predictY + (targetY - 1) / (1 - predictY)\n",
    "        }\n",
    "    \n",
    "    def _dA_dReluP(self, x, i):\n",
    "        data = x.clone()\n",
    "        data[data > 0] = 0\n",
    "        data[data <= 0] = torch.mean(data[data <= 0])\n",
    "        return data\n",
    "    \n",
    "    def _PReluDer(self, x, i): \n",
    "        data = x.clone()\n",
    "        data[data > 0] = 1\n",
    "        data[data <= 0] = self.reluParam[i]\n",
    "        return data\n",
    "    \n",
    "    def batchNorm(self, x, layerIter):\n",
    "        mean = torch.mean(x)\n",
    "        variance = torch.mean((x - mean) ** 2)\n",
    "        # normalize\n",
    "        xNorm = (x - mean) * 1.0 / torch.sqrt(variance + self.BN['epsilon'])\n",
    "        return self.BN['gamma'][layerIter] * xNorm + self.BN['beta'][layerIter]\n",
    "    \n",
    "    # 生成 w、b, 以x的形状是 m x 1\n",
    "    def genParam(self, x):\n",
    "        self.inputs = x\n",
    "        column = x.shape[0]\n",
    "        for i, r in enumerate(self.layerShapes):\n",
    "#             print('row: ', row, '\\ncolumn: ', column)\n",
    "            self.weights[i] = torch.ones(r, column, dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "            self.biases[i] = torch.zeros(r, 1, dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "            self.reluParam[i] = 0.01\n",
    "            column = r\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "#         print('inputs.shape: ', inputs.shape)\n",
    "    #     layer['non linear'][0] = layer['linear'][0] = inputs\n",
    "        for i in range(self.layerNum):\n",
    "#             print(f'w[{i}].shape: ', self.weights[i].shape)\n",
    "#             print(f'b[{i}].shape: ', self.biases[i].shape)\n",
    "            inputs = self.batchNorm(inputs, i)\n",
    "            self.layers['Z'][i] = self.weights[i] @ inputs + self.biases[i]\n",
    "#             self.layers['N'][i] = self.batchNorm(self.layers['Z'][i], i)\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                self.layers['A'][i] = self.activFunc[self.aFuncChosen[i]](self.layers['Z'][i], i)\n",
    "            else:\n",
    "                self.layers['A'][i] = self.activFunc[self.aFuncChosen[i]](self.layers['Z'][i])\n",
    "            inputs = self.layers['A'][i]\n",
    "#             print(f'layer[{i}].shape: ', self.layers['A'][i].shape)\n",
    "\n",
    "    # 预测\n",
    "    def predict(self, x):\n",
    "        self.forward(x)\n",
    "        predictY = self.layers['A'][-1]\n",
    "        print('output: ', predictY.squeeze())\n",
    "        return predictY\n",
    "    \n",
    "    # 反向传播函数\n",
    "    # input = x, Z = W @ input + b, Y_preditc = activateFunc(Z), L = lossFunc(Y_predict)\n",
    "    # 根据链式法则 dL / dW = (dL / dY_predict) * (dY_predict / dZ) + (dZ / dW)\n",
    "    # dL / dY_predict = lossFunc_Der, dY_predict / dZ = activateFunc_Der, dZ/dW = input\n",
    "    # ==> dL / dW = lossFunc_Der(Y_predict) * activateFunc_Der(Z) * input\n",
    "    # 同理可证 dL / db = lossFunc_Der(Y_predict) * activateFunc_Der(Z) * 1\n",
    "    def backprop(self):\n",
    "        '''\n",
    "        尚未完成\n",
    "        '''\n",
    "        dW = [None] * self.layerNum\n",
    "        dB = [None] * self.layerNum\n",
    "        dReluP = [None] * self.layerNum\n",
    "        dL_Div_dYtrain = self.lossFuncDer['mse'](self.layers['A'][-1], self.targetY)\n",
    "        dActivation = [None] * self.layerNum\n",
    "        for i in reversed(range(self.layerNum)):\n",
    "#             print('i: ', i)\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                dActivation[i] = self.activFuncDer[self.aFuncChosen[i]](self.layers['Z'][i], i)\n",
    "            else:\n",
    "                dActivation[i] = self.activFuncDer[self.aFuncChosen[i]](self.layers['Z'][i])\n",
    "#             print(f'weight[{i}] shape: {self.weights[i].shape} \\nbias[{i}] shape: {self.biases[i].shape} \\n')\n",
    "            if i == self.layerNum - 1:\n",
    "                dB[i] = dL_Div_dYtrain * dActivation[i]\n",
    "                dW[i] = dB[i] @ torch.transpose(self.layers['A'][i-1], 0, 1)\n",
    "                dReluP[i] = torch.mean(dL_Div_dYtrain * self.layers['Z'][i]).item()\n",
    "            else:\n",
    "                dB[i] = (torch.transpose(self.weights[i+1], 0, 1) @ dB[i+1]) * dActivation[i]\n",
    "                dW[i] = dB[i] @ torch.transpose(self.layers['A'][i-1], 0, 1)\n",
    "                dReluP[i] = torch.mean(torch.transpose(self.weights[i], 0, 1) @ dB[i] * self.layers['Z'][i-1]).item()\n",
    "#             print(f'dReluP[{i}]: {dReluP[i]}')\n",
    "            self.weights[i] += dW[i] * self.lr['weight']\n",
    "            self.biases[i] += dB[i] * self.lr['bias']\n",
    "            if self.aFuncChosen[i] == 'PRelu':\n",
    "                self.reluParam[i] += dReluP[i] * self.lr['relu param']\n",
    "#             print(self.reluParam)\n",
    "            \n",
    "    def train(self, inputs, targetY, nanInvestigate=40, epoch = 1000):\n",
    "        self.targetY = targetY\n",
    "        for e in range(epoch):\n",
    "            self.forward(self.inputs)\n",
    "            if epoch % 100 == 0 and epoch != 0:\n",
    "                loss = torch.norm(self.layers['A'][-1] - self.targetY)\n",
    "                print(e, f': loss = {loss}')\n",
    "                if loss < 5.0:\n",
    "                    for k,v in self.lr.items():\n",
    "                        self.lr[k] = 0.0005\n",
    "                if loss < 2.0:\n",
    "                    for k,v in self.lr.items():\n",
    "                        self.lr[k] = 0.0001\n",
    "                if loss < 1.0 or torch.isnan(loss):\n",
    "                    return\n",
    "#                 print(f'{i} Loss: ', torch.mean(self.lossFunc['mse'](self.layers['A'][-1], self.targetY)).item())\n",
    "#                 print(epoch/100, ': \\n', torch.transpose(self.layers['A'][-1], 0, 1))\n",
    "            self.backprop()\n",
    "            if e > nanInvestigate and nanInvestigate > 0:\n",
    "                self.saveParams('d:\\\\nanInvest_'+str(e)+'.pt', True)\n",
    "                \n",
    "    def printShape(self):\n",
    "        for i in range(self.layerNum):\n",
    "            print(f'weight[{i}] shape: ', self.weights[i].shape)\n",
    "            print(f'bias[{i}] shape: ', self.biases[i].shape)\n",
    "            print(f'Relu Params[{i}]: ', self.reluParam[i])\n",
    "#             print(f'Z layer shape: ', self.layers['Z'][i].shape)\n",
    "#             print(f'N layer shape: ', self.layers['N'][i].shape)\n",
    "#             print(f'A layer shape: ', self.layers['A'][i].shape)\n",
    "    \n",
    "    def saveParams(self, PATH, layers=False):\n",
    "        params = {\n",
    "            'reluParam': self.reluParam, \n",
    "            'weight': self.weights, \n",
    "            'bias': self.biases, \n",
    "            'BN':self.BN\n",
    "        }\n",
    "        if layers:\n",
    "            params['layers'] = self.layers\n",
    "        torch.save(params, PATH)\n",
    "        \n",
    "    def readParams(self, PATH, layers=False):\n",
    "        params = torch.load(PATH)\n",
    "        self.reluParam = params['reluParam']\n",
    "        self.weights = params['weight']\n",
    "        self.biases = params['bias']\n",
    "        self.BN = params['BN']\n",
    "        if layers:\n",
    "            self.layers = params['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight[0] shape:  torch.Size([10, 400])\n",
      "bias[0] shape:  torch.Size([10, 1])\n",
      "Relu Params[0]:  0.01\n",
      "weight[1] shape:  torch.Size([10, 10])\n",
      "bias[1] shape:  torch.Size([10, 1])\n",
      "Relu Params[1]:  0.01\n",
      "weight[2] shape:  torch.Size([400, 10])\n",
      "bias[2] shape:  torch.Size([400, 1])\n",
      "Relu Params[2]:  0.01\n",
      "0 : loss = 19.999989731589288\n",
      "1 : loss = 19.99879034267986\n",
      "2 : loss = 19.847077759894596\n",
      "3 : loss = 19.699216003030802\n",
      "4 : loss = 19.556094039422604\n",
      "5 : loss = 19.41761987962163\n",
      "6 : loss = 19.283679841003167\n",
      "7 : loss = 19.154149481238804\n",
      "8 : loss = 19.028907532971033\n",
      "9 : loss = 18.907828242010755\n",
      "10 : loss = 18.790793091210553\n",
      "11 : loss = 18.67768366851004\n",
      "12 : loss = 18.568383899027225\n",
      "13 : loss = 18.462779296985047\n",
      "14 : loss = 18.3607574693035\n",
      "15 : loss = 18.26220811391937\n",
      "16 : loss = 18.16702285172393\n",
      "17 : loss = 18.075095753786904\n",
      "18 : loss = 17.98632257933954\n",
      "19 : loss = 17.90060165671102\n",
      "20 : loss = 17.817833170770093\n",
      "21 : loss = 17.737919402113814\n",
      "22 : loss = 17.660764723139824\n",
      "23 : loss = 17.586275548723048\n",
      "24 : loss = 17.514360468666354\n",
      "25 : loss = 17.44493009711021\n",
      "26 : loss = 17.37789711009848\n",
      "27 : loss = 17.313176233084803\n",
      "28 : loss = 17.250684226328307\n",
      "29 : loss = 17.19033986831777\n",
      "30 : loss = 17.13206393709375\n",
      "31 : loss = 17.075779189605385\n",
      "32 : loss = 17.02141033916476\n",
      "33 : loss = 16.9688840309568\n",
      "34 : loss = 16.9181288158313\n",
      "35 : loss = 16.869075122317735\n",
      "36 : loss = 16.8216552270913\n",
      "37 : loss = 16.77580322384958\n",
      "38 : loss = 16.731454990816797\n",
      "39 : loss = 16.688548156861156\n",
      "40 : loss = 16.64702206645073\n",
      "41 : loss = 16.606817743480857\n",
      "42 : loss = 16.567877854151615\n",
      "43 : loss = 16.530146669023527\n",
      "44 : loss = 16.493570024366985\n",
      "45 : loss = 16.458095282845516\n",
      "46 : loss = 16.42367129370095\n",
      "47 : loss = 16.390248352435094\n",
      "48 : loss = 16.35777816012128\n",
      "49 : loss = 16.326213782462638\n",
      "50 : loss = 16.295509608613244\n",
      "51 : loss = 16.265621310012957\n",
      "52 : loss = 16.236505799157708\n",
      "53 : loss = 16.208121188482945\n",
      "54 : loss = 16.180426749391113\n",
      "55 : loss = 16.153382871474705\n",
      "56 : loss = 16.12695102200427\n",
      "57 : loss = 16.101093705753698\n",
      "58 : loss = 16.075774425147845\n",
      "59 : loss = 16.05095764090196\n",
      "60 : loss = 16.026608733056587\n",
      "61 : loss = 16.002693962554748\n",
      "62 : loss = 15.979180433325137\n",
      "63 : loss = 15.956036054955732\n",
      "64 : loss = 15.933229505970793\n",
      "65 : loss = 15.910730197698998\n",
      "66 : loss = 15.88850823881204\n",
      "67 : loss = 15.866534400517754\n",
      "68 : loss = 15.844780082436142\n",
      "69 : loss = 15.823217279171896\n",
      "70 : loss = 15.801818547596486\n",
      "71 : loss = 15.780556974850974\n",
      "72 : loss = 15.759406147079162\n",
      "73 : loss = 15.73834011889924\n",
      "74 : loss = 15.717333383620911\n",
      "75 : loss = 15.696360844213912\n",
      "76 : loss = 15.67539778503304\n",
      "77 : loss = 15.654419844304138\n",
      "78 : loss = 15.633402987374986\n",
      "79 : loss = 15.612323480734688\n",
      "80 : loss = 15.59115786680509\n",
      "81 : loss = 15.569882939507588\n",
      "82 : loss = 15.548475720608941\n",
      "83 : loss = 15.526913436849927\n",
      "84 : loss = 15.505173497861033\n",
      "85 : loss = 15.483233474869953\n",
      "86 : loss = 15.461071080206265\n",
      "87 : loss = 15.43866414760941\n",
      "88 : loss = 15.415990613346937\n",
      "89 : loss = 15.393028498150926\n",
      "90 : loss = 15.369755889981555\n",
      "91 : loss = 15.346150927627848\n",
      "92 : loss = 15.322191785156859\n",
      "93 : loss = 15.297856657223827\n",
      "94 : loss = 15.273123745257102\n",
      "95 : loss = 15.247971244533074\n",
      "96 : loss = 15.2223773321577\n",
      "97 : loss = 15.196320155972739\n",
      "98 : loss = 15.169777824406252\n",
      "99 : loss = 15.142728397288414\n",
      "100 : loss = 15.115149877655256\n",
      "101 : loss = 15.087020204564432\n",
      "102 : loss = 15.058317246948596\n",
      "103 : loss = 15.029018798533507\n",
      "104 : loss = 14.999102573838611\n",
      "105 : loss = 14.96854620529293\n",
      "106 : loss = 14.93732724164727\n",
      "107 : loss = 14.905423147303257\n",
      "108 : loss = 14.872811303063825\n",
      "109 : loss = 14.839469008085949\n",
      "110 : loss = 14.80537348312482\n",
      "111 : loss = 14.770501875106206\n",
      "112 : loss = 14.734831263064237\n",
      "113 : loss = 14.698338665482451\n",
      "114 : loss = 14.661001049076221\n",
      "115 : loss = 14.622795339054768\n",
      "116 : loss = 14.583698430900789\n",
      "117 : loss = 14.543687203705375\n",
      "118 : loss = 14.502738535095197\n",
      "119 : loss = 14.460829317787875\n",
      "120 : loss = 14.417936477810226\n",
      "121 : loss = 14.374036994412336\n",
      "122 : loss = 14.329107921708374\n",
      "123 : loss = 14.28312641207261\n",
      "124 : loss = 14.236069741316213\n",
      "125 : loss = 14.18791533566707\n",
      "126 : loss = 14.138640800571089\n",
      "127 : loss = 14.088223951329095\n",
      "128 : loss = 14.03664284557868\n",
      "129 : loss = 13.983875817625012\n",
      "130 : loss = 13.929901514618669\n",
      "131 : loss = 13.874698934572164\n",
      "132 : loss = 13.81824746619979\n",
      "133 : loss = 13.760526930557774\n",
      "134 : loss = 13.701517624453603\n",
      "135 : loss = 13.641200365584508\n",
      "136 : loss = 13.579556539355883\n",
      "137 : loss = 13.516568147320342\n",
      "138 : loss = 13.452217857167785\n",
      "139 : loss = 13.386489054185766\n",
      "140 : loss = 13.319365894098013\n",
      "141 : loss = 13.250833357176983\n",
      "142 : loss = 13.180877303513986\n",
      "143 : loss = 13.109484529317719\n",
      "144 : loss = 13.03664282409899\n",
      "145 : loss = 12.962341028586227\n",
      "146 : loss = 12.886569093202926\n",
      "147 : loss = 12.809318136924844\n",
      "148 : loss = 12.730580506321205\n",
      "149 : loss = 12.650349834571044\n",
      "150 : loss = 12.568621100232773\n",
      "151 : loss = 12.485390685532424\n",
      "152 : loss = 12.400656433923917\n",
      "153 : loss = 12.31441770666332\n",
      "154 : loss = 12.226675438128257\n",
      "155 : loss = 12.137432189604061\n",
      "156 : loss = 12.04669220124949\n",
      "157 : loss = 11.954461441947531\n",
      "158 : loss = 11.860747656740738\n",
      "159 : loss = 11.765560411546138\n",
      "160 : loss = 11.66891113484195\n",
      "161 : loss = 11.570813156017461\n",
      "162 : loss = 11.471281740078382\n",
      "163 : loss = 11.370334118403216\n",
      "164 : loss = 11.267989515251458\n",
      "165 : loss = 11.164269169732123\n",
      "166 : loss = 11.059196352951082\n",
      "167 : loss = 10.952796380068264\n",
      "168 : loss = 10.845096617010622\n",
      "169 : loss = 10.736126481604344\n",
      "170 : loss = 10.625917438909696\n",
      "171 : loss = 10.514502990564297\n",
      "172 : loss = 10.401918657965536\n",
      "173 : loss = 10.288201959149836\n",
      "174 : loss = 10.173392379255889\n",
      "175 : loss = 10.057531334490209\n",
      "176 : loss = 9.9406621295466\n",
      "177 : loss = 9.82282990846587\n",
      "178 : loss = 9.704081598958298\n",
      "179 : loss = 9.584465850248701\n",
      "180 : loss = 9.464032964542008\n",
      "181 : loss = 9.342834822245932\n",
      "182 : loss = 9.220924801126115\n",
      "183 : loss = 9.098357689607795\n",
      "184 : loss = 8.97518959447618\n",
      "185 : loss = 8.85147784326495\n",
      "186 : loss = 8.727280881658352\n",
      "187 : loss = 8.602658166266746\n",
      "188 : loss = 8.477670053167916\n",
      "189 : loss = 8.352377682636607\n",
      "190 : loss = 8.226842860512312\n",
      "191 : loss = 8.101127936679962\n",
      "192 : loss = 7.975295681159608\n",
      "193 : loss = 7.84940915831919\n",
      "194 : loss = 7.723531599738955\n",
      "195 : loss = 7.597726276266598\n",
      "196 : loss = 7.472056369808978\n",
      "197 : loss = 7.346584845408902\n",
      "198 : loss = 7.221374324154139\n",
      "199 : loss = 7.096486957460496\n",
      "200 : loss = 6.971984303261424\n",
      "201 : loss = 6.847927204623416\n",
      "202 : loss = 6.724375671289566\n",
      "203 : loss = 6.6013887646331195\n",
      "204 : loss = 6.479024486479076\n",
      "205 : loss = 6.357339672225004\n",
      "206 : loss = 6.236389888662558\n",
      "207 : loss = 6.1162293368690275\n",
      "208 : loss = 5.996910760503964\n",
      "209 : loss = 5.87848535980986\n",
      "210 : loss = 5.76100271157828\n",
      "211 : loss = 5.644510695304362\n",
      "212 : loss = 5.529055425713245\n",
      "213 : loss = 5.414681191802473\n",
      "214 : loss = 5.301430402504826\n",
      "215 : loss = 5.189343539036921\n",
      "216 : loss = 5.07845911396045\n",
      "217 : loss = 4.968813636945579\n",
      "218 : loss = 4.963407933666741\n",
      "219 : loss = 4.95800540988485\n",
      "220 : loss = 4.952606069567854\n",
      "221 : loss = 4.947209916671103\n",
      "222 : loss = 4.941816955137341\n",
      "223 : loss = 4.9364271888966975\n",
      "224 : loss = 4.9310406218666945\n",
      "225 : loss = 4.925657257952231\n",
      "226 : loss = 4.920277101045589\n",
      "227 : loss = 4.914900155026427\n",
      "228 : loss = 4.909526423761776\n",
      "229 : loss = 4.904155911106036\n",
      "230 : loss = 4.898788620900977\n",
      "231 : loss = 4.89342455697573\n",
      "232 : loss = 4.88806372314679\n",
      "233 : loss = 4.882706123218009\n",
      "234 : loss = 4.877351760980594\n",
      "235 : loss = 4.872000640213109\n",
      "236 : loss = 4.8666527646814695\n",
      "237 : loss = 4.861308138138936\n",
      "238 : loss = 4.855966764326118\n",
      "239 : loss = 4.850628646970971\n",
      "240 : loss = 4.84529378978879\n",
      "241 : loss = 4.839962196482212\n",
      "242 : loss = 4.834633870741213\n",
      "243 : loss = 4.829308816243105\n",
      "244 : loss = 4.823987036652537\n",
      "245 : loss = 4.818668535621491\n",
      "246 : loss = 4.8133533167892795\n",
      "247 : loss = 4.808041383782549\n",
      "248 : loss = 4.802732740215274\n",
      "249 : loss = 4.797427389688755\n",
      "250 : loss = 4.792125335791624\n",
      "251 : loss = 4.786826582099837\n",
      "252 : loss = 4.781531132176674\n",
      "253 : loss = 4.776238989572741\n",
      "254 : loss = 4.77095015782597\n",
      "255 : loss = 4.765664640461611\n",
      "256 : loss = 4.760382440992241\n",
      "257 : loss = 4.755103562917756\n",
      "258 : loss = 4.749828009725375\n",
      "259 : loss = 4.744555784889639\n",
      "260 : loss = 4.739286891872409\n",
      "261 : loss = 4.734021334122864\n",
      "262 : loss = 4.72875911507751\n",
      "263 : loss = 4.723500238160172\n",
      "264 : loss = 4.718244706781993\n",
      "265 : loss = 4.712992524341443\n",
      "266 : loss = 4.707743694224307\n",
      "267 : loss = 4.702498219803698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 : loss = 4.69725610444005\n",
      "269 : loss = 4.692017351481118\n",
      "270 : loss = 4.686781964261985\n",
      "271 : loss = 4.681549946105055\n",
      "272 : loss = 4.676321300320062\n",
      "273 : loss = 4.671096030204063\n",
      "274 : loss = 4.665874139041443\n",
      "275 : loss = 4.660655630103916\n",
      "276 : loss = 4.655440506650529\n",
      "277 : loss = 4.650228771927656\n",
      "278 : loss = 4.645020429169007\n",
      "279 : loss = 4.639815481595623\n",
      "280 : loss = 4.634613932415886\n",
      "281 : loss = 4.629415784825509\n",
      "282 : loss = 4.62422104200755\n",
      "283 : loss = 4.619029707132403\n",
      "284 : loss = 4.613841783357808\n",
      "285 : loss = 4.608657273828851\n",
      "286 : loss = 4.603476181677961\n",
      "287 : loss = 4.598298510024922\n",
      "288 : loss = 4.593124261976863\n",
      "289 : loss = 4.58795344062827\n",
      "290 : loss = 4.582786049060987\n",
      "291 : loss = 4.577622090344214\n",
      "292 : loss = 4.5724615675345115\n",
      "293 : loss = 4.567304483675808\n",
      "294 : loss = 4.562150841799397\n",
      "295 : loss = 4.55700064492394\n",
      "296 : loss = 4.551853896055474\n",
      "297 : loss = 4.546710598187407\n",
      "298 : loss = 4.5415707543005315\n",
      "299 : loss = 4.536434367363018\n",
      "300 : loss = 4.531301440330426\n",
      "301 : loss = 4.5261719761457\n",
      "302 : loss = 4.521045977739178\n",
      "303 : loss = 4.515923448028591\n",
      "304 : loss = 4.510804389919073\n",
      "305 : loss = 4.5056888063031595\n",
      "306 : loss = 4.500576700060791\n",
      "307 : loss = 4.495468074059322\n",
      "308 : loss = 4.490362931153517\n",
      "309 : loss = 4.485261274185562\n",
      "310 : loss = 4.480163105985065\n",
      "311 : loss = 4.47506842936906\n",
      "312 : loss = 4.469977247142013\n",
      "313 : loss = 4.464889562095824\n",
      "314 : loss = 4.459805377009834\n",
      "315 : loss = 4.454724694650827\n",
      "316 : loss = 4.449647517773037\n",
      "317 : loss = 4.444573849118151\n",
      "318 : loss = 4.439503691415312\n",
      "319 : loss = 4.434437047381131\n",
      "320 : loss = 4.429373919719683\n",
      "321 : loss = 4.424314311122517\n",
      "322 : loss = 4.41925822426866\n",
      "323 : loss = 4.414205661824622\n",
      "324 : loss = 4.409156626444402\n",
      "325 : loss = 4.4041111207694925\n",
      "326 : loss = 4.399069147428885\n",
      "327 : loss = 4.394030709039074\n",
      "328 : loss = 4.388995808204065\n",
      "329 : loss = 4.383964447515382\n",
      "330 : loss = 4.378936629552066\n",
      "331 : loss = 4.373912356880686\n",
      "332 : loss = 4.368891632055347\n",
      "333 : loss = 4.36387445761769\n",
      "334 : loss = 4.358860836096901\n",
      "335 : loss = 4.353850770009717\n",
      "336 : loss = 4.348844261860433\n",
      "337 : loss = 4.343841314140906\n",
      "338 : loss = 4.338841929330564\n",
      "339 : loss = 4.333846109896409\n",
      "340 : loss = 4.328853858293028\n",
      "341 : loss = 4.323865176962597\n",
      "342 : loss = 4.3188800683348845\n",
      "343 : loss = 4.313898534827263\n",
      "344 : loss = 4.308920578844714\n",
      "345 : loss = 4.3039462027798345\n",
      "346 : loss = 4.298975409012845\n",
      "347 : loss = 4.294008199911597\n",
      "348 : loss = 4.289044577831573\n",
      "349 : loss = 4.284084545115906\n",
      "350 : loss = 4.279128104095375\n",
      "351 : loss = 4.274175257088421\n",
      "352 : loss = 4.26922600640115\n",
      "353 : loss = 4.264280354327338\n",
      "354 : loss = 4.259338303148447\n",
      "355 : loss = 4.254399855133621\n",
      "356 : loss = 4.249465012539704\n",
      "357 : loss = 4.244533777611242\n",
      "358 : loss = 4.239606152580492\n",
      "359 : loss = 4.23468213966743\n",
      "360 : loss = 4.22976174107976\n",
      "361 : loss = 4.22484495901292\n",
      "362 : loss = 4.2199317956500915\n",
      "363 : loss = 4.215022253162208\n",
      "364 : loss = 4.2101163337079575\n",
      "365 : loss = 4.205214039433804\n",
      "366 : loss = 4.2003153724739795\n",
      "367 : loss = 4.195420334950504\n",
      "368 : loss = 4.190528928973192\n",
      "369 : loss = 4.185641156639657\n",
      "370 : loss = 4.180757020035322\n",
      "371 : loss = 4.175876521233429\n",
      "372 : loss = 4.170999662295047\n",
      "373 : loss = 4.166126445269085\n",
      "374 : loss = 4.161256872192293\n",
      "375 : loss = 4.156390945089276\n",
      "376 : loss = 4.151528665972503\n",
      "377 : loss = 4.1466700368423135\n",
      "378 : loss = 4.14181505968693\n",
      "379 : loss = 4.136963736482467\n",
      "380 : loss = 4.132116069192934\n",
      "381 : loss = 4.127272059770253\n",
      "382 : loss = 4.122431710154267\n",
      "383 : loss = 4.1175950222727415\n",
      "384 : loss = 4.112761998041384\n",
      "385 : loss = 4.107932639363845\n",
      "386 : loss = 4.10310694813174\n",
      "387 : loss = 4.098284926224643\n",
      "388 : loss = 4.093466575510107\n",
      "389 : loss = 4.088651897843674\n",
      "390 : loss = 4.08384089506888\n",
      "391 : loss = 4.079033569017266\n",
      "392 : loss = 4.074229921508392\n",
      "393 : loss = 4.0694299543498476\n",
      "394 : loss = 4.0646336693372485\n",
      "395 : loss = 4.0598410682542685\n",
      "396 : loss = 4.055052152872636\n",
      "397 : loss = 4.050266924952144\n",
      "398 : loss = 4.045485386240664\n",
      "399 : loss = 4.04070753847416\n",
      "400 : loss = 4.035933383376691\n",
      "401 : loss = 4.03116292266043\n",
      "402 : loss = 4.026396158025668\n",
      "403 : loss = 4.021633091160828\n",
      "404 : loss = 4.016873723742473\n",
      "405 : loss = 4.012118057435325\n",
      "406 : loss = 4.007366093892264\n",
      "407 : loss = 4.002617834754346\n",
      "408 : loss = 3.9978732816508176\n",
      "409 : loss = 3.993132436199117\n",
      "410 : loss = 3.988395300004893\n",
      "411 : loss = 3.9836618746620163\n",
      "412 : loss = 3.9789321617525846\n",
      "413 : loss = 3.9742061628469396\n",
      "414 : loss = 3.9694838795036778\n",
      "415 : loss = 3.9647653132696585\n",
      "416 : loss = 3.9600504656800197\n",
      "417 : loss = 3.955339338258187\n",
      "418 : loss = 3.950631932515886\n",
      "419 : loss = 3.9459282499531545\n",
      "420 : loss = 3.941228292058352\n",
      "421 : loss = 3.936532060308176\n",
      "422 : loss = 3.9318395561676676\n",
      "423 : loss = 3.9271507810902295\n",
      "424 : loss = 3.9224657365176356\n",
      "425 : loss = 3.9177844238800414\n",
      "426 : loss = 3.9131068445959976\n",
      "427 : loss = 3.908433000072464\n",
      "428 : loss = 3.9037628917048175\n",
      "429 : loss = 3.8990965208768693\n",
      "430 : loss = 3.8944338889608727\n",
      "431 : loss = 3.88977499731754\n",
      "432 : loss = 3.885119847296049\n",
      "433 : loss = 3.8804684402340612\n",
      "434 : loss = 3.8758207774577333\n",
      "435 : loss = 3.8711768602817256\n",
      "436 : loss = 3.8665366900092186\n",
      "437 : loss = 3.861900267931927\n",
      "438 : loss = 3.8572675953301063\n",
      "439 : loss = 3.8526386734725717\n",
      "440 : loss = 3.8480135036167105\n",
      "441 : loss = 3.8433920870084908\n",
      "442 : loss = 3.8387744248824744\n",
      "443 : loss = 3.834160518461839\n",
      "444 : loss = 3.829550368958379\n",
      "445 : loss = 3.8249439775725267\n",
      "446 : loss = 3.820341345493363\n",
      "447 : loss = 3.81574247389863\n",
      "448 : loss = 3.8111473639547437\n",
      "449 : loss = 3.8065560168168107\n",
      "450 : loss = 3.8019684336286397\n",
      "451 : loss = 3.797384615522752\n",
      "452 : loss = 3.792804563620398\n",
      "453 : loss = 3.7882282790315718\n",
      "454 : loss = 3.7836557628550245\n",
      "455 : loss = 3.7790870161782713\n",
      "456 : loss = 3.7745220400776156\n",
      "457 : loss = 3.769960835618156\n",
      "458 : loss = 3.765403403853801\n",
      "459 : loss = 3.7608497458272843\n",
      "460 : loss = 3.756299862570176\n",
      "461 : loss = 3.7517537551029\n",
      "462 : loss = 3.7472114244347465\n",
      "463 : loss = 3.7426728715638853\n",
      "464 : loss = 3.738138097477378\n",
      "465 : loss = 3.7336071031511997\n",
      "466 : loss = 3.7290798895502415\n",
      "467 : loss = 3.724556457628334\n",
      "468 : loss = 3.7200368083282602\n",
      "469 : loss = 3.715520942581765\n",
      "470 : loss = 3.7110088613095757\n",
      "471 : loss = 3.7065005654214103\n",
      "472 : loss = 3.701996055815997\n",
      "473 : loss = 3.697495333381085\n",
      "474 : loss = 3.692998398993462\n",
      "475 : loss = 3.6885052535189673\n",
      "476 : loss = 3.6840158978125044\n",
      "477 : loss = 3.6795303327180604\n",
      "478 : loss = 3.675048559068715\n",
      "479 : loss = 3.6705705776866626\n",
      "480 : loss = 3.666096389383217\n",
      "481 : loss = 3.6616259949588366\n",
      "482 : loss = 3.6571593952031316\n",
      "483 : loss = 3.652696590894883\n",
      "484 : loss = 3.6482375828020537\n",
      "485 : loss = 3.643782371681811\n",
      "486 : loss = 3.6393309582805324\n",
      "487 : loss = 3.634883343333827\n",
      "488 : loss = 3.6304395275665455\n",
      "489 : loss = 3.6259995116928034\n",
      "490 : loss = 3.6215632964159883\n",
      "491 : loss = 3.6171308824287767\n",
      "492 : loss = 3.6127022704131533\n",
      "493 : loss = 3.608277461040421\n",
      "494 : loss = 3.603856454971221\n",
      "495 : loss = 3.5994392528555434\n",
      "496 : loss = 3.5950258553327474\n",
      "497 : loss = 3.5906162630315746\n",
      "498 : loss = 3.586210476570163\n",
      "499 : loss = 3.581808496556063\n",
      "500 : loss = 3.5774103235862573\n",
      "501 : loss = 3.573015958247169\n",
      "502 : loss = 3.5686254011146854\n",
      "503 : loss = 3.564238652754166\n",
      "504 : loss = 3.5598557137204643\n",
      "505 : loss = 3.5554765845579404\n",
      "506 : loss = 3.5511012658004777\n",
      "507 : loss = 3.5467297579714976\n",
      "508 : loss = 3.5423620615839777\n",
      "509 : loss = 3.5379981771404663\n",
      "510 : loss = 3.5336381051330967\n",
      "511 : loss = 3.529281846043606\n",
      "512 : loss = 3.524929400343351\n",
      "513 : loss = 3.520580768493321\n",
      "514 : loss = 3.5162359509441563\n",
      "515 : loss = 3.5118949481361668\n",
      "516 : loss = 3.5075577604993433\n",
      "517 : loss = 3.503224388453374\n",
      "518 : loss = 3.498894832407665\n",
      "519 : loss = 3.4945690927613557\n",
      "520 : loss = 3.49024716990333\n",
      "521 : loss = 3.4859290642122396\n",
      "522 : loss = 3.4816147760565133\n",
      "523 : loss = 3.4773043057943798\n",
      "524 : loss = 3.4729976537738794\n",
      "525 : loss = 3.468694820332882\n",
      "526 : loss = 3.4643958057991053\n",
      "527 : loss = 3.4601006104901293\n",
      "528 : loss = 3.455809234713415\n",
      "529 : loss = 3.451521678766317\n",
      "530 : loss = 3.4472379429361024\n",
      "531 : loss = 3.4429580274999707\n",
      "532 : loss = 3.4386819327250637\n",
      "533 : loss = 3.4344096588684905\n",
      "534 : loss = 3.4301412061773346\n",
      "535 : loss = 3.425876574888678\n",
      "536 : loss = 3.421615765229615\n",
      "537 : loss = 3.4173587774172702\n",
      "538 : loss = 3.413105611658815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539 : loss = 3.4088562681514847\n",
      "540 : loss = 3.4046107470825926\n",
      "541 : loss = 3.400369048629551\n",
      "542 : loss = 3.3961311729598878\n",
      "543 : loss = 3.3918971202312607\n",
      "544 : loss = 3.387666890591476\n",
      "545 : loss = 3.383440484178505\n",
      "546 : loss = 3.3792179011205015\n",
      "547 : loss = 3.374999141535818\n",
      "548 : loss = 3.370784205533027\n",
      "549 : loss = 3.366573093210932\n",
      "550 : loss = 3.362365804658588\n",
      "551 : loss = 3.3581623399553178\n",
      "552 : loss = 3.353962699170731\n",
      "553 : loss = 3.349766882364738\n",
      "554 : loss = 3.3455748895875717\n",
      "555 : loss = 3.341386720879801\n",
      "556 : loss = 3.3372023762723497\n",
      "557 : loss = 3.3330218557865137\n",
      "558 : loss = 3.328845159433978\n",
      "559 : loss = 3.324672287216835\n",
      "560 : loss = 3.320503239127601\n",
      "561 : loss = 3.316338015149233\n",
      "562 : loss = 3.312176615255151\n",
      "563 : loss = 3.308019039409246\n",
      "564 : loss = 3.3038652875659085\n",
      "565 : loss = 3.29971535967004\n",
      "566 : loss = 3.2955692556570684\n",
      "567 : loss = 3.291426975452972\n",
      "568 : loss = 3.2872885189742918\n",
      "569 : loss = 3.283153886128151\n",
      "570 : loss = 3.2790230768122743\n",
      "571 : loss = 3.274896090915003\n",
      "572 : loss = 3.270772928315315\n",
      "573 : loss = 3.2666535888828405\n",
      "574 : loss = 3.2625380724778803\n",
      "575 : loss = 3.258426378951425\n",
      "576 : loss = 3.2543185081451718\n",
      "577 : loss = 3.250214459891539\n",
      "578 : loss = 3.2461142340136915\n",
      "579 : loss = 3.242017830325552\n",
      "580 : loss = 3.2379252486318224\n",
      "581 : loss = 3.2338364887279973\n",
      "582 : loss = 3.2297515504003895\n",
      "583 : loss = 3.225670433426139\n",
      "584 : loss = 3.2215931375732407\n",
      "585 : loss = 3.217519662600551\n",
      "586 : loss = 3.213450008257817\n",
      "587 : loss = 3.2093841742856877\n",
      "588 : loss = 3.2053221604157334\n",
      "589 : loss = 3.201263966370464\n",
      "590 : loss = 3.197209591863347\n",
      "591 : loss = 3.193159036598828\n",
      "592 : loss = 3.189112300272346\n",
      "593 : loss = 3.185069382570348\n",
      "594 : loss = 3.1810302831703186\n",
      "595 : loss = 3.1769950017407855\n",
      "596 : loss = 3.1729635379413437\n",
      "597 : loss = 3.168935891422677\n",
      "598 : loss = 3.164912061826565\n",
      "599 : loss = 3.160892048785914\n",
      "600 : loss = 3.15687585192477\n",
      "601 : loss = 3.152863470858334\n",
      "602 : loss = 3.1488549051929864\n",
      "603 : loss = 3.1448501545262983\n",
      "604 : loss = 3.140849218447056\n",
      "605 : loss = 3.1368520965352746\n",
      "606 : loss = 3.132858788362222\n",
      "607 : loss = 3.128869293490431\n",
      "608 : loss = 3.1248836114737197\n",
      "609 : loss = 3.1209017418572147\n",
      "610 : loss = 3.116923684177361\n",
      "611 : loss = 3.112949437961947\n",
      "612 : loss = 3.1089790027301203\n",
      "613 : loss = 3.1050123779924057\n",
      "614 : loss = 3.1010495632507276\n",
      "615 : loss = 3.097090557998421\n",
      "616 : loss = 3.093135361720259\n",
      "617 : loss = 3.0891839738924625\n",
      "618 : loss = 3.0852363939827243\n",
      "619 : loss = 3.0812926214502263\n",
      "620 : loss = 3.077352655745658\n",
      "621 : loss = 3.073416496311236\n",
      "622 : loss = 3.0694841425807198\n",
      "623 : loss = 3.0655555939794317\n",
      "624 : loss = 3.061630849924277\n",
      "625 : loss = 3.057709909823761\n",
      "626 : loss = 3.053792773078007\n",
      "627 : loss = 3.0498794390787762\n",
      "628 : loss = 3.045969907209487\n",
      "629 : loss = 3.04206417684523\n",
      "630 : loss = 3.038162247352791\n",
      "631 : loss = 3.0342641180906678\n",
      "632 : loss = 3.0303697884090863\n",
      "633 : loss = 3.026479257650024\n",
      "634 : loss = 3.0225925251472274\n",
      "635 : loss = 3.0187095902262273\n",
      "636 : loss = 3.0148304522043587\n",
      "637 : loss = 3.0109551103907837\n",
      "638 : loss = 3.007083564086506\n",
      "639 : loss = 3.0032158125843913\n",
      "640 : loss = 2.999351855169182\n",
      "641 : loss = 2.995491691117522\n",
      "642 : loss = 2.9916353196979735\n",
      "643 : loss = 2.9877827401710353\n",
      "644 : loss = 2.9839339517891577\n",
      "645 : loss = 2.980088953796768\n",
      "646 : loss = 2.9762477454302863\n",
      "647 : loss = 2.9724103259181422\n",
      "648 : loss = 2.9685766944807965\n",
      "649 : loss = 2.964746850330759\n",
      "650 : loss = 2.9609207926726064\n",
      "651 : loss = 2.9570985207030023\n",
      "652 : loss = 2.953280033610715\n",
      "653 : loss = 2.9494653305766376\n",
      "654 : loss = 2.9456544107738076\n",
      "655 : loss = 2.9418472733674226\n",
      "656 : loss = 2.93804391751486\n",
      "657 : loss = 2.9342443423656976\n",
      "658 : loss = 2.9304485470617334\n",
      "659 : loss = 2.9266565307369987\n",
      "660 : loss = 2.9228682925177827\n",
      "661 : loss = 2.91908383152265\n",
      "662 : loss = 2.9153031468624575\n",
      "663 : loss = 2.911526237640374\n",
      "664 : loss = 2.9077531029519026\n",
      "665 : loss = 2.9039837418848937\n",
      "666 : loss = 2.9002181535195692\n",
      "667 : loss = 2.896456336928538\n",
      "668 : loss = 2.892698291176814\n",
      "669 : loss = 2.8889440153218398\n",
      "670 : loss = 2.8851935084135\n",
      "671 : loss = 2.881446769494143\n",
      "672 : loss = 2.8777037975986013\n",
      "673 : loss = 2.8739645917542083\n",
      "674 : loss = 2.8702291509808164\n",
      "675 : loss = 2.8664974742908162\n",
      "676 : loss = 2.8627695606891597\n",
      "677 : loss = 2.859045409173372\n",
      "678 : loss = 2.855325018733577\n",
      "679 : loss = 2.8516083883525094\n",
      "680 : loss = 2.847895517005542\n",
      "681 : loss = 2.8441864036606965\n",
      "682 : loss = 2.8404810472786672\n",
      "683 : loss = 2.8367794468128373\n",
      "684 : loss = 2.8330816012093023\n",
      "685 : loss = 2.829387509406881\n",
      "686 : loss = 2.8256971703371447\n",
      "687 : loss = 2.8220105829244253\n",
      "688 : loss = 2.818327746085842\n",
      "689 : loss = 2.8146486587313166\n",
      "690 : loss = 2.8109733197635944\n",
      "691 : loss = 2.8073017280782633\n",
      "692 : loss = 2.803633882563768\n",
      "693 : loss = 2.799969782101436\n",
      "694 : loss = 2.79630942556549\n",
      "695 : loss = 2.7926528118230713\n",
      "696 : loss = 2.788999939734255\n",
      "697 : loss = 2.7853508081520735\n",
      "698 : loss = 2.7817054159225325\n",
      "699 : loss = 2.77806376188463\n",
      "700 : loss = 2.7744258448703754\n",
      "701 : loss = 2.770791663704806\n",
      "702 : loss = 2.767161217206011\n",
      "703 : loss = 2.763534504185149\n",
      "704 : loss = 2.759911523446461\n",
      "705 : loss = 2.756292273787299\n",
      "706 : loss = 2.752676753998133\n",
      "707 : loss = 2.7490649628625845\n",
      "708 : loss = 2.7454568991574324\n",
      "709 : loss = 2.741852561652638\n",
      "710 : loss = 2.7382519491113615\n",
      "711 : loss = 2.7346550602899837\n",
      "712 : loss = 2.731061893938123\n",
      "713 : loss = 2.7274724487986557\n",
      "714 : loss = 2.7238867236077304\n",
      "715 : loss = 2.720304717094791\n",
      "716 : loss = 2.7167264279825973\n",
      "717 : loss = 2.7131518549872364\n",
      "718 : loss = 2.709580996818152\n",
      "719 : loss = 2.706013852178151\n",
      "720 : loss = 2.702450419763436\n",
      "721 : loss = 2.6988906982636087\n",
      "722 : loss = 2.6953346863617025\n",
      "723 : loss = 2.691782382734195\n",
      "724 : loss = 2.6882337860510246\n",
      "725 : loss = 2.6846888949756154\n",
      "726 : loss = 2.6811477081648913\n",
      "727 : loss = 2.6776102242692947\n",
      "728 : loss = 2.6740764419328085\n",
      "729 : loss = 2.670546359792974\n",
      "730 : loss = 2.667019976480904\n",
      "731 : loss = 2.6634972906213124\n",
      "732 : loss = 2.659978300832522\n",
      "733 : loss = 2.6564630057264917\n",
      "734 : loss = 2.6529514039088276\n",
      "735 : loss = 2.6494434939788074\n",
      "736 : loss = 2.6459392745294\n",
      "737 : loss = 2.6424387441472774\n",
      "738 : loss = 2.63894190141284\n",
      "739 : loss = 2.6354487449002306\n",
      "740 : loss = 2.6319592731773587\n",
      "741 : loss = 2.628473484805912\n",
      "742 : loss = 2.624991378341381\n",
      "743 : loss = 2.6215129523330747\n",
      "744 : loss = 2.6180382053241407\n",
      "745 : loss = 2.6145671358515834\n",
      "746 : loss = 2.611099742446281\n",
      "747 : loss = 2.607636023633006\n",
      "748 : loss = 2.6041759779304434\n",
      "749 : loss = 2.6007196038512097\n",
      "750 : loss = 2.5972668999018693\n",
      "751 : loss = 2.5938178645829573\n",
      "752 : loss = 2.590372496388993\n",
      "753 : loss = 2.586930793808502\n",
      "754 : loss = 2.583492755324034\n",
      "755 : loss = 2.580058379412182\n",
      "756 : loss = 2.5766276645435964\n",
      "757 : loss = 2.5732006091830106\n",
      "758 : loss = 2.569777211789255\n",
      "759 : loss = 2.5663574708152734\n",
      "760 : loss = 2.5629413847081515\n",
      "761 : loss = 2.55952895190912\n",
      "762 : loss = 2.556120170853586\n",
      "763 : loss = 2.5527150399711465\n",
      "764 : loss = 2.549313557685608\n",
      "765 : loss = 2.5459157224150015\n",
      "766 : loss = 2.5425215325716044\n",
      "767 : loss = 2.5391309865619607\n",
      "768 : loss = 2.535744082786893\n",
      "769 : loss = 2.5323608196415286\n",
      "770 : loss = 2.5289811955153114\n",
      "771 : loss = 2.525605208792022\n",
      "772 : loss = 2.5222328578497986\n",
      "773 : loss = 2.518864141061153\n",
      "774 : loss = 2.5154990567929882\n",
      "775 : loss = 2.512137603406621\n",
      "776 : loss = 2.5087797792577953\n",
      "777 : loss = 2.505425582696704\n",
      "778 : loss = 2.502075012068002\n",
      "779 : loss = 2.4987280657108313\n",
      "780 : loss = 2.495384741958836\n",
      "781 : loss = 2.492045039140181\n",
      "782 : loss = 2.4887089555775668\n",
      "783 : loss = 2.4853764895882535\n",
      "784 : loss = 2.4820476394840747\n",
      "785 : loss = 2.4787224035714592\n",
      "786 : loss = 2.4754007801514457\n",
      "787 : loss = 2.4720827675197032\n",
      "788 : loss = 2.468768363966547\n",
      "789 : loss = 2.46545756777696\n",
      "790 : loss = 2.4621503772306066\n",
      "791 : loss = 2.458846790601857\n",
      "792 : loss = 2.4555468061597967\n",
      "793 : loss = 2.452250422168255\n",
      "794 : loss = 2.4489576368858126\n",
      "795 : loss = 2.4456684485658253\n",
      "796 : loss = 2.4423828554564437\n",
      "797 : loss = 2.439100855800627\n",
      "798 : loss = 2.435822447836163\n",
      "799 : loss = 2.432547629795684\n",
      "800 : loss = 2.42927639990669\n",
      "801 : loss = 2.4260087563915604\n",
      "802 : loss = 2.422744697467577\n",
      "803 : loss = 2.419484221346936\n",
      "804 : loss = 2.4162273262367746\n",
      "805 : loss = 2.4129740103391786\n",
      "806 : loss = 2.4097242718512097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807 : loss = 2.406478108964916\n",
      "808 : loss = 2.403235519867354\n",
      "809 : loss = 2.399996502740608\n",
      "810 : loss = 2.396761055761803\n",
      "811 : loss = 2.3935291771031224\n",
      "812 : loss = 2.3903008649318327\n",
      "813 : loss = 2.3870761174102935\n",
      "814 : loss = 2.383854932695982\n",
      "815 : loss = 2.3806373089415027\n",
      "816 : loss = 2.377423244294612\n",
      "817 : loss = 2.3742127368982335\n",
      "818 : loss = 2.371005784890478\n",
      "819 : loss = 2.3678023864046547\n",
      "820 : loss = 2.3646025395692942\n",
      "821 : loss = 2.361406242508165\n",
      "822 : loss = 2.358213493340293\n",
      "823 : loss = 2.355024290179974\n",
      "824 : loss = 2.3518386311367965\n",
      "825 : loss = 2.348656514315655\n",
      "826 : loss = 2.3454779378167725\n",
      "827 : loss = 2.3423028997357136\n",
      "828 : loss = 2.3391313981634028\n",
      "829 : loss = 2.3359634311861455\n",
      "830 : loss = 2.3327989968856384\n",
      "831 : loss = 2.3296380933389953\n",
      "832 : loss = 2.326480718618758\n",
      "833 : loss = 2.3233268707929176\n",
      "834 : loss = 2.32017654792493\n",
      "835 : loss = 2.317029748073733\n",
      "836 : loss = 2.3138864692937635\n",
      "837 : loss = 2.3107467096349796\n",
      "838 : loss = 2.3076104671428697\n",
      "839 : loss = 2.304477739858478\n",
      "840 : loss = 2.3013485258184154\n",
      "841 : loss = 2.2982228230548785\n",
      "842 : loss = 2.29510062959567\n",
      "843 : loss = 2.291981943464211\n",
      "844 : loss = 2.288866762679564\n",
      "845 : loss = 2.2857550852564446\n",
      "846 : loss = 2.2826469092052397\n",
      "847 : loss = 2.279542232532028\n",
      "848 : loss = 2.2764410532385955\n",
      "849 : loss = 2.273343369322449\n",
      "850 : loss = 2.2702491787768384\n",
      "851 : loss = 2.267158479590773\n",
      "852 : loss = 2.2640712697490324\n",
      "853 : loss = 2.2609875472321916\n",
      "854 : loss = 2.2579073100166367\n",
      "855 : loss = 2.254830556074574\n",
      "856 : loss = 2.251757283374058\n",
      "857 : loss = 2.248687489879\n",
      "858 : loss = 2.2456211735491904\n",
      "859 : loss = 2.24255833234031\n",
      "860 : loss = 2.2394989642039547\n",
      "861 : loss = 2.2364430670876443\n",
      "862 : loss = 2.2333906389348437\n",
      "863 : loss = 2.2303416776849816\n",
      "864 : loss = 2.2272961812734624\n",
      "865 : loss = 2.224254147631683\n",
      "866 : loss = 2.221215574687055\n",
      "867 : loss = 2.2181804603630177\n",
      "868 : loss = 2.215148802579056\n",
      "869 : loss = 2.2121205992507136\n",
      "870 : loss = 2.209095848289614\n",
      "871 : loss = 2.2060745476034773\n",
      "872 : loss = 2.203056695096134\n",
      "873 : loss = 2.2000422886675413\n",
      "874 : loss = 2.1970313262138017\n",
      "875 : loss = 2.1940238056271815\n",
      "876 : loss = 2.191019724796122\n",
      "877 : loss = 2.188019081605261\n",
      "878 : loss = 2.1850218739354434\n",
      "879 : loss = 2.1820280996637478\n",
      "880 : loss = 2.1790377566634898\n",
      "881 : loss = 2.17605084280425\n",
      "882 : loss = 2.173067355951884\n",
      "883 : loss = 2.1700872939685394\n",
      "884 : loss = 2.167110654712675\n",
      "885 : loss = 2.1641374360390757\n",
      "886 : loss = 2.161167635798866\n",
      "887 : loss = 2.15820125183953\n",
      "888 : loss = 2.1552382820049276\n",
      "889 : loss = 2.1522787241353085\n",
      "890 : loss = 2.1493225760673305\n",
      "891 : loss = 2.1463698356340735\n",
      "892 : loss = 2.143420500665057\n",
      "893 : loss = 2.140474568986257\n",
      "894 : loss = 2.13753203842012\n",
      "895 : loss = 2.1345929067855822\n",
      "896 : loss = 2.1316571718980826\n",
      "897 : loss = 2.1287248315695813\n",
      "898 : loss = 2.1257958836085744\n",
      "899 : loss = 2.1228703258201094\n",
      "900 : loss = 2.119948156005803\n",
      "901 : loss = 2.117029371963854\n",
      "902 : loss = 2.114113971489065\n",
      "903 : loss = 2.1112019523728502\n",
      "904 : loss = 2.1082933124032603\n",
      "905 : loss = 2.1053880493649926\n",
      "906 : loss = 2.102486161039404\n",
      "907 : loss = 2.099587645204538\n",
      "908 : loss = 2.096692499635127\n",
      "909 : loss = 2.0938007221026194\n",
      "910 : loss = 2.0909123103751863\n",
      "911 : loss = 2.088027262217746\n",
      "912 : loss = 2.0851455753919725\n",
      "913 : loss = 2.082267247656314\n",
      "914 : loss = 2.07939227676601\n",
      "915 : loss = 2.076520660473102\n",
      "916 : loss = 2.0736523965264584\n",
      "917 : loss = 2.070787482671778\n",
      "918 : loss = 2.067925916651616\n",
      "919 : loss = 2.065067696205395\n",
      "920 : loss = 2.0622128190694182\n",
      "921 : loss = 2.0593612829768917\n",
      "922 : loss = 2.056513085657931\n",
      "923 : loss = 2.053668224839587\n",
      "924 : loss = 2.050826698245851\n",
      "925 : loss = 2.0479885035976797\n",
      "926 : loss = 2.0451536386130016\n",
      "927 : loss = 2.042322101006737\n",
      "928 : loss = 2.0394938884908154\n",
      "929 : loss = 2.036668998774186\n",
      "930 : loss = 2.033847429562836\n",
      "931 : loss = 2.031029178559807\n",
      "932 : loss = 2.028214243465204\n",
      "933 : loss = 2.0254026219762213\n",
      "934 : loss = 2.0225943117871443\n",
      "935 : loss = 2.019789310589379\n",
      "936 : loss = 2.016987616071454\n",
      "937 : loss = 2.014189225919045\n",
      "938 : loss = 2.011394137814985\n",
      "939 : loss = 2.0086023494392817\n",
      "940 : loss = 2.0058138584691294\n",
      "941 : loss = 2.0030286625789295\n",
      "942 : loss = 2.0002467594402997\n",
      "943 : loss = 1.9974681467220934\n",
      "944 : loss = 1.9969131042023214\n",
      "945 : loss = 1.9963581930710086\n",
      "946 : loss = 1.995803413309478\n",
      "947 : loss = 1.9952487648990485\n",
      "948 : loss = 1.9946942478210328\n",
      "949 : loss = 1.9941398620567408\n",
      "950 : loss = 1.9935856075874765\n",
      "951 : loss = 1.9930314843945418\n",
      "952 : loss = 1.9924774924592334\n",
      "953 : loss = 1.9919236317628428\n",
      "954 : loss = 1.9913699022866584\n",
      "955 : loss = 1.9908163040119629\n",
      "956 : loss = 1.990262836920036\n",
      "957 : loss = 1.9897095009921513\n",
      "958 : loss = 1.9891562962095806\n",
      "959 : loss = 1.9886032225535892\n",
      "960 : loss = 1.98805028000544\n",
      "961 : loss = 1.9874974685463915\n",
      "962 : loss = 1.9869447881576967\n",
      "963 : loss = 1.9863922388206023\n",
      "964 : loss = 1.9858398205163557\n",
      "965 : loss = 1.985287533226197\n",
      "966 : loss = 1.9847353769313627\n",
      "967 : loss = 1.9841833516130853\n",
      "968 : loss = 1.9836314572525915\n",
      "969 : loss = 1.983079693831106\n",
      "970 : loss = 1.9825280613298466\n",
      "971 : loss = 1.9819765597300298\n",
      "972 : loss = 1.9814251890128651\n",
      "973 : loss = 1.9808739491595606\n",
      "974 : loss = 1.9803228401513189\n",
      "975 : loss = 1.9797718619693367\n",
      "976 : loss = 1.97922101459481\n",
      "977 : loss = 1.9786702980089261\n",
      "978 : loss = 1.9781197121928724\n",
      "979 : loss = 1.977569257127828\n",
      "980 : loss = 1.9770189327949734\n",
      "981 : loss = 1.9764687391754805\n",
      "982 : loss = 1.975918676250516\n",
      "983 : loss = 1.9753687440012477\n",
      "984 : loss = 1.9748189424088343\n",
      "985 : loss = 1.9742692714544305\n",
      "986 : loss = 1.9737197311191914\n",
      "987 : loss = 1.9731703213842626\n",
      "988 : loss = 1.9726210422307913\n",
      "989 : loss = 1.9720718936399142\n",
      "990 : loss = 1.971522875592766\n",
      "991 : loss = 1.9709739880704804\n",
      "992 : loss = 1.9704252310541837\n",
      "993 : loss = 1.9698766045249985\n",
      "994 : loss = 1.9693281084640448\n",
      "995 : loss = 1.968779742852436\n",
      "996 : loss = 1.9682315076712846\n",
      "997 : loss = 1.9676834029016956\n",
      "998 : loss = 1.9671354285247722\n",
      "999 : loss = 1.9665875845216134\n",
      "output:  tensor([-1.1060e-01,  1.7868e-01,  3.2362e-01,  1.7030e+00,  5.5728e-01,\n",
      "         6.1436e-01,  1.4427e-01,  1.2685e+00, -2.1847e-02, -3.3177e-01,\n",
      "         6.2149e-01,  9.5304e-01, -1.5392e-01,  1.8119e-01,  1.1509e+00,\n",
      "         3.4539e-01, -1.8091e+00,  1.9350e-01, -4.3244e-01, -1.4932e+00,\n",
      "         7.9461e-01, -1.3634e-01, -1.0359e+00, -3.9661e-01,  6.0939e-01,\n",
      "         2.3422e-01,  9.0319e-01,  8.7507e-01, -3.0403e-01, -8.3935e-01,\n",
      "        -8.6907e-01,  1.3226e-01, -2.3186e-02, -6.0051e-01,  1.8512e-01,\n",
      "         2.4537e-01, -8.6938e-01,  2.5202e-01,  9.6461e-01, -2.3460e-01,\n",
      "         8.8788e-01, -6.8836e-01,  1.3953e+00,  6.0112e-01,  3.3818e-01,\n",
      "         2.5399e-01, -2.6178e-01,  3.0577e-01, -5.7588e-02, -1.0838e+00,\n",
      "         6.4391e-01, -1.7929e+00,  1.3965e+00,  3.6859e-01, -1.3243e-01,\n",
      "        -1.3124e+00,  6.1210e-01,  3.1242e-02, -5.8701e-01, -1.7058e+00,\n",
      "        -3.6200e-01, -1.2862e+00, -1.1497e+00,  2.0043e-01,  2.8718e-01,\n",
      "         1.0343e+00, -7.8466e-01,  1.0790e+00,  4.7781e-01,  2.8367e-01,\n",
      "         5.4468e-02,  4.0054e-01,  4.0691e-01, -1.6403e+00, -1.0551e+00,\n",
      "        -1.4531e+00, -1.6270e+00,  1.4464e-01,  1.3278e+00,  9.6108e-01,\n",
      "         3.4053e-01,  3.2099e-01, -1.0674e+00,  1.3949e+00,  1.3898e+00,\n",
      "        -1.6089e+00, -3.8073e-01, -1.6207e+00,  3.2315e-01,  9.9759e-01,\n",
      "         1.3662e+00,  1.3750e+00,  2.0583e+00,  7.9806e-01, -5.2114e-01,\n",
      "         2.2988e-01,  4.0590e-02,  6.4717e-01, -7.5648e-01, -3.3567e-01,\n",
      "         3.1065e-01,  6.6247e-02, -1.0789e-01,  1.1951e+00,  1.1331e+00,\n",
      "        -1.1217e+00,  1.0295e-02, -8.8348e-01,  1.0194e-01,  1.2444e+00,\n",
      "         3.8045e-01,  1.9063e-01,  4.8382e-01, -1.8538e+00,  3.8758e-01,\n",
      "         8.3241e-01,  1.3450e+00, -8.9409e-01,  3.8738e-01, -4.8789e-01,\n",
      "         1.3791e+00, -8.5063e-01,  1.3851e+00, -3.9298e-02,  8.5381e-01,\n",
      "         2.4606e-01, -6.8544e-01,  2.9374e-01,  5.5790e-01,  6.0014e-02,\n",
      "         1.3422e+00,  1.1666e+00,  5.6997e-01, -1.8676e+00, -2.4810e-01,\n",
      "         2.4135e-01, -7.3115e-01,  5.4210e-01,  8.1448e-02,  4.1315e-01,\n",
      "        -2.1970e+00,  2.6561e-01,  1.3225e+00,  4.9492e-01,  5.2784e-01,\n",
      "        -1.4428e+00, -1.3898e+00,  6.3152e-01,  6.5009e-01, -1.4876e-01,\n",
      "         3.5087e-01, -2.1915e+00,  2.4306e-01,  5.5189e-01,  1.8348e-01,\n",
      "         1.4383e-01, -1.4123e+00,  4.7475e-01,  2.4824e-01, -5.3774e-01,\n",
      "        -1.5435e+00, -8.5950e-02,  2.9386e-01,  3.1032e-01,  1.2830e+00,\n",
      "         9.5931e-01,  8.1655e-01,  9.6662e-01,  2.2725e-01,  1.3496e+00,\n",
      "         8.4314e-01, -1.6763e+00,  4.3055e-02,  3.0173e-01,  4.1674e-01,\n",
      "         6.8563e-01,  2.8210e-01, -1.4077e+00, -1.6066e+00,  8.9865e-01,\n",
      "         1.0296e-02, -2.2117e-01, -1.3426e-02, -1.3168e-01, -5.5035e-02,\n",
      "         1.3402e+00, -1.7386e+00,  8.2022e-02, -7.4069e-01,  2.8843e-01,\n",
      "         5.8567e-01, -9.2734e-01,  3.9133e-01, -4.9459e-01,  7.6894e-01,\n",
      "         2.2141e-01,  7.1488e-01,  1.2387e+00,  1.1233e+00, -1.6927e-01,\n",
      "         3.3810e-01,  1.0581e-01, -1.4656e+00,  1.8935e-01,  3.9010e-01,\n",
      "         4.4815e-01,  4.3012e-01,  8.5814e-01,  1.3125e+00,  2.0116e-01,\n",
      "        -6.9239e-01, -2.7501e-01, -9.3575e-05,  1.1619e+00,  9.5188e-01,\n",
      "        -3.5113e-01,  4.1905e-01, -2.2090e+00,  6.3687e-01,  9.5184e-01,\n",
      "        -1.1336e+00,  2.9204e-01,  2.0244e-01,  1.3971e+00,  4.6238e-01,\n",
      "         4.6773e-01,  3.7282e-01, -1.4464e+00, -5.7967e-01,  2.0657e-01,\n",
      "         2.2068e+00,  1.3971e+00,  8.1197e-01, -1.1762e+00,  1.3897e+00,\n",
      "         1.2585e+00,  1.3402e+00, -1.7582e+00,  1.3086e+00,  2.1064e-01,\n",
      "         2.6101e-01,  7.3784e-01,  1.3865e+00,  3.8969e-01, -1.3140e+00,\n",
      "        -1.0651e+00,  2.7871e-02, -7.8773e-01, -1.4056e+00,  6.4277e-02,\n",
      "        -1.4452e-01, -3.0231e-01,  1.2722e+00, -1.6037e+00,  6.0491e-01,\n",
      "         7.1237e-01,  2.7312e-01, -1.1038e+00,  1.2746e-02,  2.5282e-01,\n",
      "        -1.3442e-01,  3.4460e-01, -1.8422e+00,  1.1572e+00,  6.5918e-02,\n",
      "         4.0505e-01,  2.2885e+00,  7.9350e-01, -2.1598e+00,  6.4280e-03,\n",
      "        -1.6829e+00,  2.1366e-02, -9.6210e-01, -7.2869e-01,  2.9853e-01,\n",
      "         6.9233e-01, -6.6801e-01,  5.4669e-01, -2.4148e-01,  5.4909e-01,\n",
      "         5.0117e-01,  1.0625e-01,  2.2278e-01, -1.7815e+00,  9.3601e-01,\n",
      "        -1.8238e+00, -6.1441e-01,  2.5237e-01, -1.7293e+00,  6.7582e-01,\n",
      "        -6.8668e-02, -4.0141e-01,  1.2536e+00,  4.5314e-01,  1.4497e-01,\n",
      "         1.8088e-01,  7.1121e-01,  3.5294e-01,  4.1353e-01,  3.4250e-01,\n",
      "        -4.7159e-01,  2.1773e-01,  2.7882e-01,  9.3730e-01, -1.5106e+00,\n",
      "        -5.7673e-01,  1.3767e+00, -6.8299e-02,  3.0640e-01,  5.0531e-01,\n",
      "         8.7612e-01, -1.8499e+00,  8.1626e-01, -3.5293e-01, -1.4000e+00,\n",
      "         2.4723e-01,  3.5004e-01, -2.1966e+00,  4.2119e-01,  3.1574e-02,\n",
      "         1.4961e+00,  5.9584e-01,  1.2343e+00,  2.1169e-01,  2.7926e-01,\n",
      "         3.4772e-01, -1.6183e+00,  1.0031e+00, -1.6370e+00, -1.3543e-01,\n",
      "        -1.0013e+00,  1.3928e+00, -6.6790e-01, -8.2516e-02, -1.2048e+00,\n",
      "         6.5430e-01,  7.2754e-01, -8.5492e-02,  2.1277e-04, -7.0582e-01,\n",
      "        -1.6756e-01,  2.0225e-01, -8.3371e-01,  1.0815e-01, -1.4536e+00,\n",
      "        -1.7261e+00,  3.0261e-01,  8.0602e-01,  2.0207e-01, -1.4495e+00,\n",
      "        -2.0930e-01,  4.5260e-01, -1.6329e+00,  5.2585e-01,  1.2221e+00,\n",
      "        -8.2659e-01,  4.9212e-01, -1.0595e+00, -1.8301e-01,  3.9734e-01,\n",
      "         6.3838e-02, -2.0958e+00,  2.9219e-01, -1.8497e+00,  5.7554e-01,\n",
      "        -4.9503e-01,  1.3732e+00,  2.6230e-02, -1.6561e-01,  1.2450e+00,\n",
      "         1.7451e-01, -1.5675e+00,  1.0301e+00, -1.6789e-01,  9.8512e-01,\n",
      "         2.2864e-01,  1.3483e+00, -9.0956e-01,  2.7631e-01,  1.3867e+00,\n",
      "         3.1082e-01, -2.0996e+00, -4.6655e-01, -1.5644e-01,  2.9648e-01,\n",
      "         7.3750e-01,  1.3758e+00,  8.8173e-01,  1.3951e+00,  8.8530e-01,\n",
      "         7.9229e-01, -1.0448e+00, -1.0737e+00, -1.2424e+00,  3.4809e-01,\n",
      "         5.4596e-02,  1.3950e+00, -1.3367e+00,  7.3849e-01, -1.5706e+00],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight[0] shape:  torch.Size([10, 400])\n",
      "bias[0] shape:  torch.Size([10, 1])\n",
      "Relu Params[0]:  0.004816332705627686\n",
      "weight[1] shape:  torch.Size([10, 10])\n",
      "bias[1] shape:  torch.Size([10, 1])\n",
      "Relu Params[1]:  0.01\n",
      "weight[2] shape:  torch.Size([400, 10])\n",
      "bias[2] shape:  torch.Size([400, 1])\n",
      "Relu Params[2]:  0.8479886406695659\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    layers = (10, 10, 400)\n",
    "    aFuncs = ('PRelu', 'cos', 'PRelu')\n",
    "    batchSize = 10\n",
    "    nn1 = DNN(layers, aFuncs)\n",
    "    # 所有input全部变型成 m x 1 的形状\n",
    "    testInput = torch.randn((20, 20), dtype=DTYPE['f64'], device=DEVICE[DEVICE_CHOICE])\n",
    "    testInput = testInput.view(-1, 1)\n",
    "    targetY = nn1.batchNorm(5*torch.sin(testInput**3) + 2*torch.cos(2*testInput-1) - torch.tanh(testInput), 0)\n",
    "    # 生成权重和偏移\n",
    "    nn1.genParam(testInput)\n",
    "    nn1.printShape()\n",
    "    # 训练\n",
    "    nn1.train(testInput, targetY, nanInvestigate=0, epoch=1000)\n",
    "    # 预测\n",
    "    estimateY = nn1.predict(testInput)\n",
    "    \n",
    "    plt.scatter(testInput.cpu().numpy().flatten(), targetY.cpu().numpy().flatten())\n",
    "    plt.scatter(testInput.cpu().numpy().flatten(), estimateY.cpu().numpy().flatten())\n",
    "    plt.show()\n",
    "    nn1.printShape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan]], device='cuda:0', dtype=torch.float64),\n",
       " tensor([[nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan]], device='cuda:0', dtype=torch.float64),\n",
       " tensor([[nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan]], device='cuda:0', dtype=torch.float64)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1.layers['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275],\n",
      "        [-5.0529e+275]], device='cuda:0', dtype=torch.float64), tensor([[5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672],\n",
      "        [5.3672]], device='cuda:0', dtype=torch.float64), tensor([[-1.2032e-02],\n",
      "        [-3.6203e-04],\n",
      "        [ 1.3286e-01],\n",
      "        [ 8.2065e-01],\n",
      "        [ 3.5820e-01],\n",
      "        [ 4.0628e-01],\n",
      "        [-1.7814e-03],\n",
      "        [ 9.0331e-01],\n",
      "        [-8.3471e-03],\n",
      "        [-2.0779e-02],\n",
      "        [ 4.1217e-01],\n",
      "        [ 7.1294e-01],\n",
      "        [-1.9788e-02],\n",
      "        [-2.5701e-04],\n",
      "        [ 8.4488e-01],\n",
      "        [ 3.0465e-01],\n",
      "        [-7.4479e-02],\n",
      "        [ 3.3996e-03],\n",
      "        [-2.8695e-02],\n",
      "        [-6.4443e-02],\n",
      "        [ 5.4960e-01],\n",
      "        [-1.3078e-02],\n",
      "        [-4.6815e-02],\n",
      "        [-2.3264e-02],\n",
      "        [ 5.0980e-01],\n",
      "        [ 4.2547e-02],\n",
      "        [ 2.9080e-01],\n",
      "        [ 6.6073e-01],\n",
      "        [-1.9707e-02],\n",
      "        [-3.9700e-02],\n",
      "        [-4.3438e-02],\n",
      "        [-2.2694e-03],\n",
      "        [-8.4038e-03],\n",
      "        [-3.0926e-02],\n",
      "        [-9.2543e-05],\n",
      "        [ 5.2865e-02],\n",
      "        [-4.0792e-02],\n",
      "        [ 6.0422e-02],\n",
      "        [ 7.2068e-01],\n",
      "        [-2.2535e-02],\n",
      "        [ 6.2077e-01],\n",
      "        [-3.4174e-02],\n",
      "        [ 1.0022e+00],\n",
      "        [ 4.7680e-01],\n",
      "        [ 1.4744e-01],\n",
      "        [ 6.2304e-02],\n",
      "        [-1.8062e-02],\n",
      "        [ 1.1498e-01],\n",
      "        [-9.8470e-03],\n",
      "        [-4.8539e-02],\n",
      "        [ 4.3053e-01],\n",
      "        [-7.3893e-02],\n",
      "        [ 1.0012e+00],\n",
      "        [ 1.8166e-01],\n",
      "        [-1.2919e-02],\n",
      "        [-5.6729e-02],\n",
      "        [ 4.0441e-01],\n",
      "        [-6.2334e-03],\n",
      "        [-3.0425e-02],\n",
      "        [-7.0760e-02],\n",
      "        [-2.6314e-02],\n",
      "        [-5.5792e-02],\n",
      "        [-5.2899e-02],\n",
      "        [ 9.5320e-03],\n",
      "        [ 9.6432e-02],\n",
      "        [ 7.3033e-01],\n",
      "        [-3.7706e-02],\n",
      "        [ 7.6347e-01],\n",
      "        [ 2.8762e-01],\n",
      "        [ 9.2942e-02],\n",
      "        [ 1.2172e-01],\n",
      "        [ 2.1371e-01],\n",
      "        [ 2.2000e-01],\n",
      "        [-6.8418e-02],\n",
      "        [-4.9712e-02],\n",
      "        [-6.1745e-02],\n",
      "        [-6.7944e-02],\n",
      "        [-1.7665e-03],\n",
      "        [ 9.4733e-01],\n",
      "        [ 6.7581e-01],\n",
      "        [ 1.4980e-01],\n",
      "        [ 2.8821e-01],\n",
      "        [-4.7952e-02],\n",
      "        [ 9.9958e-01],\n",
      "        [ 9.9947e-01],\n",
      "        [-5.0947e-02],\n",
      "        [-2.2658e-02],\n",
      "        [-6.7718e-02],\n",
      "        [ 3.0733e-01],\n",
      "        [ 7.0306e-01],\n",
      "        [ 9.7642e-01],\n",
      "        [ 9.9078e-01],\n",
      "        [ 1.0601e+00],\n",
      "        [ 2.0748e-01],\n",
      "        [-1.3902e-02],\n",
      "        [ 3.7673e-02],\n",
      "        [-5.8758e-03],\n",
      "        [ 4.3319e-01],\n",
      "        [-3.6675e-02],\n",
      "        [-2.0929e-02],\n",
      "        [ 1.1986e-01],\n",
      "        [-4.8850e-03],\n",
      "        [-4.8320e-04],\n",
      "        [ 8.7414e-01],\n",
      "        [ 8.0342e-01],\n",
      "        [-5.2763e-02],\n",
      "        [-7.0294e-03],\n",
      "        [-4.1305e-02],\n",
      "        [-3.4835e-03],\n",
      "        [ 8.8549e-01],\n",
      "        [ 1.9364e-01],\n",
      "        [ 1.2948e-03],\n",
      "        [ 2.9314e-01],\n",
      "        [-7.6138e-02],\n",
      "        [ 2.0079e-01],\n",
      "        [ 5.7861e-01],\n",
      "        [ 9.6025e-01],\n",
      "        [-4.1690e-02],\n",
      "        [-2.5483e-03],\n",
      "        [-3.1424e-02],\n",
      "        [ 9.8647e-01],\n",
      "        [-4.0111e-02],\n",
      "        [ 9.9681e-01],\n",
      "        [-9.0824e-03],\n",
      "        [ 5.9493e-01],\n",
      "        [ 5.3510e-02],\n",
      "        [-3.4066e-02],\n",
      "        [ 1.0296e-01],\n",
      "        [ 3.5873e-01],\n",
      "        [-5.1271e-03],\n",
      "        [ 9.5818e-01],\n",
      "        [ 8.2814e-01],\n",
      "        [ 3.6906e-01],\n",
      "        [-7.6743e-02],\n",
      "        [-1.7526e-02],\n",
      "        [ 4.9131e-02],\n",
      "        [-3.5746e-02],\n",
      "        [ 3.4509e-01],\n",
      "        [-4.2918e-03],\n",
      "        [ 2.2614e-01],\n",
      "        [-7.1035e-02],\n",
      "        [ 7.3448e-02],\n",
      "        [ 9.4337e-01],\n",
      "        [ 3.0325e-01],\n",
      "        [ 3.3262e-01],\n",
      "        [-4.5289e-02],\n",
      "        [-6.0973e-02],\n",
      "        [ 4.2041e-01],\n",
      "        [ 4.3556e-01],\n",
      "        [-1.3579e-02],\n",
      "        [ 1.6014e-01],\n",
      "        [-7.0890e-02],\n",
      "        [ 5.0722e-02],\n",
      "        [ 3.5357e-01],\n",
      "        [-1.6114e-04],\n",
      "        [-1.7995e-03],\n",
      "        [-6.1730e-02],\n",
      "        [-3.1650e-04],\n",
      "        [ 2.3917e-01],\n",
      "        [-2.8588e-02],\n",
      "        [-6.6129e-02],\n",
      "        [-1.1021e-02],\n",
      "        [ 1.0308e-01],\n",
      "        [ 2.9858e-01],\n",
      "        [ 9.3198e-01],\n",
      "        [ 6.7449e-01],\n",
      "        [ 5.6647e-01],\n",
      "        [ 7.2202e-01],\n",
      "        [ 3.5319e-02],\n",
      "        [ 9.7494e-01],\n",
      "        [ 5.8681e-01],\n",
      "        [-6.9702e-02],\n",
      "        [-5.7811e-03],\n",
      "        [ 1.1094e-01],\n",
      "        [ 2.2966e-01],\n",
      "        [ 4.6417e-01],\n",
      "        [-6.0916e-03],\n",
      "        [-6.0128e-02],\n",
      "        [-6.8235e-02],\n",
      "        [ 6.2891e-01],\n",
      "        [-7.0294e-03],\n",
      "        [-1.6467e-02],\n",
      "        [-7.9902e-03],\n",
      "        [-1.8529e-02],\n",
      "        [-9.7407e-03],\n",
      "        [ 9.6898e-01],\n",
      "        [-7.1934e-02],\n",
      "        [-4.2692e-03],\n",
      "        [-4.0029e-02],\n",
      "        [-6.0965e-03],\n",
      "        [ 3.8234e-01],\n",
      "        [-4.2895e-02],\n",
      "        [ 2.0454e-01],\n",
      "        [-2.6971e-02],\n",
      "        [ 5.2973e-01],\n",
      "        [ 2.9370e-02],\n",
      "        [ 4.8741e-01],\n",
      "        [ 8.8125e-01],\n",
      "        [ 7.9620e-01],\n",
      "        [-1.4403e-02],\n",
      "        [ 1.4737e-01],\n",
      "        [-3.3296e-03],\n",
      "        [-6.2192e-02],\n",
      "        [ 5.1252e-04],\n",
      "        [ 2.0331e-01],\n",
      "        [ 2.5991e-01],\n",
      "        [ 2.4266e-01],\n",
      "        [ 5.9823e-01],\n",
      "        [ 9.5112e-01],\n",
      "        [ 2.0745e-01],\n",
      "        [-3.4322e-02],\n",
      "        [-2.3375e-02],\n",
      "        [-7.4221e-03],\n",
      "        [ 8.5214e-01],\n",
      "        [ 6.6892e-01],\n",
      "        [-2.1524e-02],\n",
      "        [ 2.3191e-01],\n",
      "        [-7.1459e-02],\n",
      "        [ 4.2479e-01],\n",
      "        [ 6.6890e-01],\n",
      "        [-3.4546e-02],\n",
      "        [ 1.0127e-01],\n",
      "        [ 1.1329e-02],\n",
      "        [ 1.0020e+00],\n",
      "        [ 2.7330e-01],\n",
      "        [ 2.7829e-01],\n",
      "        [ 1.8594e-01],\n",
      "        [-6.2875e-02],\n",
      "        [-3.0152e-02],\n",
      "        [ 1.5270e-02],\n",
      "        [ 1.1633e+00],\n",
      "        [ 1.0026e+00],\n",
      "        [ 5.6295e-01],\n",
      "        [-5.1857e-02],\n",
      "        [ 6.1640e-01],\n",
      "        [ 8.9585e-01],\n",
      "        [ 9.5666e-01],\n",
      "        [-7.2638e-02],\n",
      "        [ 9.3303e-01],\n",
      "        [ 1.9023e-02],\n",
      "        [ 6.9027e-02],\n",
      "        [ 5.0548e-01],\n",
      "        [ 9.9241e-01],\n",
      "        [ 2.0290e-01],\n",
      "        [-4.1255e-02],\n",
      "        [-4.7867e-02],\n",
      "        [-6.3621e-03],\n",
      "        [-3.7818e-02],\n",
      "        [-6.0053e-02],\n",
      "        [-4.9616e-03],\n",
      "        [-1.3408e-02],\n",
      "        [-1.9641e-02],\n",
      "        [ 9.0600e-01],\n",
      "        [-6.8140e-02],\n",
      "        [ 4.7935e-01],\n",
      "        [ 4.8543e-01],\n",
      "        [ 8.0687e-02],\n",
      "        [-4.9259e-02],\n",
      "        [-6.9367e-03],\n",
      "        [ 6.1188e-02],\n",
      "        [-1.3000e-02],\n",
      "        [ 1.5387e-01],\n",
      "        [-7.5996e-02],\n",
      "        [ 8.2119e-01],\n",
      "        [-4.8978e-03],\n",
      "        [ 2.1816e-01],\n",
      "        [ 1.2153e+00],\n",
      "        [ 6.0604e-01],\n",
      "        [-6.9753e-02],\n",
      "        [-7.1756e-03],\n",
      "        [-6.9939e-02],\n",
      "        [-6.6097e-03],\n",
      "        [-4.4153e-02],\n",
      "        [-3.5656e-02],\n",
      "        [ 1.0774e-01],\n",
      "        [ 4.6952e-01],\n",
      "        [-3.3424e-02],\n",
      "        [ 3.4907e-01],\n",
      "        [-1.7267e-02],\n",
      "        [ 3.5114e-01],\n",
      "        [ 3.0889e-01],\n",
      "        [-3.3120e-03],\n",
      "        [ 3.1316e-02],\n",
      "        [-7.3479e-02],\n",
      "        [ 6.5703e-01],\n",
      "        [-5.8495e-02],\n",
      "        [-3.1442e-02],\n",
      "        [ 6.0751e-02],\n",
      "        [-7.1602e-02],\n",
      "        [ 4.5632e-01],\n",
      "        [-1.0307e-02],\n",
      "        [-2.3447e-02],\n",
      "        [ 8.9227e-01],\n",
      "        [ 2.6463e-01],\n",
      "        [-1.7529e-03],\n",
      "        [-2.7028e-04],\n",
      "        [ 4.8451e-01],\n",
      "        [ 1.6221e-01],\n",
      "        [ 3.5055e-01],\n",
      "        [ 1.5177e-01],\n",
      "        [-2.6106e-02],\n",
      "        [ 2.6163e-02],\n",
      "        [ 8.8138e-02],\n",
      "        [ 6.5799e-01],\n",
      "        [-6.3795e-02],\n",
      "        [-3.0042e-02],\n",
      "        [ 9.8454e-01],\n",
      "        [-1.0292e-02],\n",
      "        [ 1.1560e-01],\n",
      "        [ 3.1261e-01],\n",
      "        [ 6.1187e-01],\n",
      "        [-7.5987e-02],\n",
      "        [ 5.6625e-01],\n",
      "        [-2.1593e-02],\n",
      "        [-5.9852e-02],\n",
      "        [ 5.5856e-02],\n",
      "        [ 3.3374e-01],\n",
      "        [-7.1020e-02],\n",
      "        [ 2.3400e-01],\n",
      "        [-6.2207e-03],\n",
      "        [ 6.9091e-01],\n",
      "        [ 3.9087e-01],\n",
      "        [ 8.7802e-01],\n",
      "        [ 1.9886e-02],\n",
      "        [ 8.8578e-02],\n",
      "        [ 1.5699e-01],\n",
      "        [-6.7634e-02],\n",
      "        [ 3.5835e-01],\n",
      "        [-6.9252e-02],\n",
      "        [-1.3041e-02],\n",
      "        [-3.0252e-02],\n",
      "        [ 9.9770e-01],\n",
      "        [-3.3420e-02],\n",
      "        [-1.0880e-02],\n",
      "        [-5.2880e-02],\n",
      "        [ 4.3898e-01],\n",
      "        [ 4.9739e-01],\n",
      "        [-1.1003e-02],\n",
      "        [-7.4103e-03],\n",
      "        [-3.8581e-02],\n",
      "        [-1.4334e-02],\n",
      "        [ 1.1189e-02],\n",
      "        [-2.4545e-02],\n",
      "        [-3.2362e-03],\n",
      "        [-6.1765e-02],\n",
      "        [-7.1485e-02],\n",
      "        [ 1.1181e-01],\n",
      "        [ 5.5838e-01],\n",
      "        [-8.0634e-03],\n",
      "        [-6.1618e-02],\n",
      "        [-1.5997e-02],\n",
      "        [ 2.6411e-01],\n",
      "        [-6.8154e-02],\n",
      "        [ 3.2660e-02],\n",
      "        [ 8.6902e-01],\n",
      "        [-3.9235e-02],\n",
      "        [ 3.0071e-01],\n",
      "        [-4.7666e-02],\n",
      "        [-1.4952e-02],\n",
      "        [ 2.1052e-01],\n",
      "        [-4.9787e-03],\n",
      "        [-6.7083e-02],\n",
      "        [ 1.0142e-01],\n",
      "        [-7.5979e-02],\n",
      "        [ 3.7378e-01],\n",
      "        [-2.6988e-02],\n",
      "        [ 6.0508e-01],\n",
      "        [-6.4246e-03],\n",
      "        [-1.4257e-02],\n",
      "        [ 5.2173e-01],\n",
      "        [-5.3597e-04],\n",
      "        [-6.6930e-02],\n",
      "        [ 7.2722e-01],\n",
      "        [-1.7564e-03],\n",
      "        [ 6.9376e-01],\n",
      "        [ 3.6566e-02],\n",
      "        [ 9.6277e-01],\n",
      "        [-4.2251e-02],\n",
      "        [ 8.5647e-02],\n",
      "        [ 9.9256e-01],\n",
      "        [ 1.2003e-01],\n",
      "        [-6.7803e-02],\n",
      "        [-3.0433e-02],\n",
      "        [-1.3888e-02],\n",
      "        [ 1.0569e-01],\n",
      "        [ 5.9586e-01],\n",
      "        [ 9.8387e-01],\n",
      "        [ 6.1612e-01],\n",
      "        [ 9.9977e-01],\n",
      "        [ 6.1882e-01],\n",
      "        [ 5.4781e-01],\n",
      "        [-4.7138e-02],\n",
      "        [-4.8177e-02],\n",
      "        [-3.8833e-02],\n",
      "        [ 1.5736e-01],\n",
      "        [-5.3365e-03],\n",
      "        [ 1.0021e+00],\n",
      "        [-5.7596e-02],\n",
      "        [ 5.0598e-01],\n",
      "        [-6.5934e-02]], device='cuda:0', dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "nn2 = DNN(layers, aFuncs)\n",
    "nn2.readParams('d:\\\\nanInvest_41.pt')\n",
    "print(nn2.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
